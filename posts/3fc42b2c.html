<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>cs231n 学习笔记（9）循环神经网络 | Kuhne</title><meta name="keywords" content="计算机视觉,深度学习"><meta name="author" content="Kuhne"><meta name="copyright" content="Kuhne"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="cs231n 学习笔记（9）循环神经网络"><meta name="application-name" content="cs231n 学习笔记（9）循环神经网络"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="cs231n 学习笔记（9）循环神经网络"><meta property="og:url" content="https://kuhne.gitee.io/kuhne.gitee.io/posts/3fc42b2c.html"><meta property="og:site_name" content="Kuhne"><meta property="og:description" content="循环神经网络 &amp;#x2F; MSTL &amp;#x2F; RNN 应用"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg"><meta property="article:author" content="Kuhne"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg"><meta name="description" content="循环神经网络 &amp;#x2F; MSTL &amp;#x2F; RNN 应用"><link rel="shortcut icon" href="/img/head2.png"><link rel="canonical" href="https://kuhne.gitee.io/kuhne.gitee.io/posts/3fc42b2c"><link rel="preconnect" href="//npm.elemecdn.com"/><link rel="preconnect" href="//npm.onmicrosoft.cn"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/swiper/swiper.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: {"enable":true,"img":"https://upload-bbs.miyoushe.com/upload/2023/09/03/125766904/ee23df8517f3c3e3efc4145658269c06_5714860933110284659.png"},
  postHeadAiDescription: undefined,
  diytitle: undefined,
  LA51: {"enable":true,"ck":"3Fpd8lKaw5wGj1Fj","LingQueMonitorID":null},
  greetingBox: {"enable":true,"default":"晚上好👋","list":[{"greeting":"晚安😴","startTime":0,"endTime":5},{"greeting":"早上好鸭👋, 祝你一天好心情！","startTime":6,"endTime":9},{"greeting":"上午好👋, 状态很好，鼓励一下～","startTime":10,"endTime":10},{"greeting":"11点多啦, 在坚持一下就吃饭啦～","startTime":11,"endTime":11},{"greeting":"午安👋，中午要好好休息一下～","startTime":12,"endTime":14},{"greeting":"🌈充实的一天辛苦啦！","startTime":14,"endTime":18},{"greeting":"19点喽, 奖励一顿丰盛的大餐吧🍔。","startTime":19,"endTime":19},{"greeting":"晚上好👋, 在属于自己的时间好好放松😌~","startTime":20,"endTime":24}]},
  twikooEnvId: 'https://twikoo-api-pearl.vercel.app',
  commentBarrageConfig:{"enable":true,"maxBarrage":1,"barrageTime":4000,"accessToken":"","mailMd5":""},
  root: '/',
  preloader: {"source":3},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: {"mode":"both","api":"https://api.kuhne.icu/api?img=","cover_change":true},
  authorStatus: {"skills":["🐟 上班摸鱼佼佼者"]},
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: Kuhne","link":"链接: ","source":"来源: Kuhne","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: {"enable":true,"delay":100,"shiftDelay":200},
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'Kuhne',
  title: 'cs231n 学习笔记（9）循环神经网络',
  postAI: '',
  pageFillDescription: 'cs231n 学习笔记（9）循环神经网络, 一、 循环神经网络介绍, 1.1 RNN 与 NN, 1.2 RNN 结构, 1.3 Vanilla RNN, 1.4 RNN 计算图, 1.5 Sequence to Sequence 模型, 二、RNN 应用, 2.1 语言建模, 2.2 沿时间的截断反向传播法, 2.3 RNN 的其他应用, 2.4 Image Caption 模型, 2.5 注意力模型（Attention）, 2.6 视觉问答（Visual Question Answering）, 三、长短时记忆神经网络（LSTM）, 3.1 LSTM 概要, 3.2 LSTM 的细节, 3.3 LSTM 反向传播, 总结学习笔记循环神经网络一循环神经网络介绍与所有的神经网络架构都有类似于神经网络如下图所示的基础架构输入是固定尺寸的对象比如一幅图片或一个向量它在经过一些隐藏层后会给出单一的输出结果比如一个分类但是在机器学习中有时候我们希望计算机有更高的灵活性能处理不同数据类型当谈到循环神经网络时处理各种类型的输入和输出数据就有很大的发挥空间一旦使用了就可以实现对多的模型输入是固定尺寸的对象如一幅图片输出是可变长度的序列比如一段文字描述也可也实现对的模型但输入的尺寸是可变的比如输入一段文字得到该文字的感情属性积极消极或是对视频进行分类可以实现多对多的模型比如机器翻译输入是英文句子输出中文句子长度都可变最后一种是多对多的另一形式对输入的每一单元做出处理比如输入一段视频要求对视频的每一帧进行分类决策就是用于处理大小可变的有序数据的一类模型使我们可以比较自然地理解模型中这些不同的架构因此非常重要且对固定输入和输出问题也同样有效如果我们想要对输入进行序列化处理比如收到了一个固定大小的图片希望做出分类决策比如判断图像中的数字是多少但不是做单一的前向传播随后马上做出决策而是观察一幅图片的不同部分完成一组观察后再最终做出判断还可以使用来生成图像让模型来合成一些类似训练集图片的图像一次大概能生成一张但是可以让模型一直工作每次计算出图像的一部分结构每个网络都有一个小小的循环核心单元他把作为输入将其传入而有一个内部隐藏态这一隐藏态会在每次读取新的输入时更新在下一次读取输入时隐藏态会将结果反馈至模型通常来说我们希望在每一步都能给出输出因此就有了这样的模式读取数据更新隐藏态生成输出这一循环过程的计算公式如下在一个模块中我们对某种循环关系用函数进行计算该函数依赖权重他接收隐藏态和输入数据随后可以输出更新后的隐藏态在读取下一个输入时更新后的隐藏态将再作为输入传入同一个函数如果我们希望在网络的每一步都生成一些输出就可以增加全连接层每一步都将作为输入根据每一步的隐藏态来做出决策需要注意每一步计算使用的是同样的函数和权重最简单的被称为该模型仅使用了非线性函数在此架构中如果希望每一步都生成就需要另外一个矩阵来接受这一隐藏态将其转化为某个计算图提到时可以从两方面来思考他有一种隐藏态可以将隐藏态循环反馈给自身的计算图如下有输入初始隐藏态通常情况下等于输入函数可以计算出下一个隐藏态该计算图可能让人感到疑惑将其展开更多的步时有助于更好地理解该计算步骤会一直持续直到用完输入项多对多情况现在可以将权重矩阵标注在计算流程图上可以看到每次计算过程中都在重复使用相同的权重矩阵如果将反向传播的原理应用到这个模型中就能得到每一步计算出的梯度最终的梯度是在所有过程中独立算出的梯度和同样的我们也可以直接把画在计算图中这样每个下输出的会重新作为输入计算出对应的如果每一个都有一个对应的真实这样就能计算出对应的这里的通常是损失计算这样的损失需要序列在每个时步下都有对应的真实损失值是整个训练中单独损失值的总和如果是多对的情况通常会根据网络最终的隐藏层做出决策并计算损失因为最终隐藏态整合了序列中包含的所有情况如果是对多情况则会接受固定长度的输入项然后输出不定长的输出项之后这个固定长的输入项会被做为模型的初始隐藏态随后会对输出的单元逐个进行处理最终会得到不定长的输出序列模型模型能用来解决机器翻译之类的问题输入和输出都是一个不定长的序列可以看作是多对和对多的结合这样就存在了两个过程编码和解码在编码阶段会接收不定长度的输入整个输入会被编码器网络最终的隐藏态编码成一个单独向量是一个多对一的过程在解码阶段输入是编码好的单一向量经过网络会生成一个不定长的输出序列这个不定长的输出需要在每一个下做出预测比如预测接下来的用词随后将整个计算图展开对输出序列的损失求和像之前一样用反向传播来训练模型二应用语言建模经常会被应用到语言建模问题上如果想通过读取大量语句从而让神经网络在一定程度上学会生成自然语言这在字符水平上是可行的即让模型逐个生成字符也可以在单词层面上让模型逐个生成单词下面是一个字符水平上的例子假如有一个字符级的语言模型网络会读取一串字符序列随后需要去预测该文本流的下一个字符是什么假设有一个很小的字符表以及一个训练序列样本在这个语言模型的训练阶段会将整个字符序列作为输入项考虑到输入实例是一个个字母所以需要想办法在神经网络中表示这些字母在这个例子中因为单词是由个字母组成的所以每个字母将会由一个其他位置都为对应字母所在位置为的单位向量构成如下图所示随着网络的前向传播在第一个时步中神经网络会接收到输入字母该输入项会进入到第一个单元内得到第一层的输出隐藏层计算完成后会计算输出即网络对组成单词的每个字母做出的预测预测结果就是他认为接下来最可能出现的字母我们的训练字母序列是下一个正确的字母应该是但模型却认为是因为对应的数值最大在这种预测错误的情况下我们可以使用损失函数度量预测差距用一个字母计算完成后模型会读入第二个字母重复执行上述步骤输入上一个隐藏态计算出一个新的隐藏态用这个隐藏态再次对字母做出预测如果用不同的字母序列去训练这个模型最终它将会学习到如何基于之前出现过的字符来预测接下来出现的字符在模型的测试阶段一般采用的方法是输入文本的前缀来测试该模型比如输入一个字母得到输出结果再用函数得到对应的概率分布进而得到输出生成序列中的第二个字母在这种情况下即使得分情况非常不好也有可能在概率分布图中得到字母这里是为什么为什么不选择概率最高的字母并不直接取概率最高的字符输出而是基于输出的概率分布进行采样高概率的字符自然更易被采样并输出但低概率的也有几率被输出从而增加了输出的不确定性多样性现在我们将得到的字母在下一步中重新输入到神经网络中不断重复上述步骤可以生成一个新的序列沿时间的截断反向传播法提供一个序列给模型模型能在每个时间步上产生一个输出结果最后再计算这就是沿时间的反向传播算法因为在前向传播过程中是沿着时间做前向计算在反向传播过程中要逆着时间反向计算所有的梯度如果想要训练一个很长的序列这个误差反向传播过程实际上有些麻烦且非常费时而且每次计算梯度时都必须进行前向传播遍历所有的输入误差反向传播也会遍历所有的文本模型难以收敛内存占用极高在实际应用中人们通常会采用一种近似的方法称之为沿时间的截断反向传播法这个方法的思想是在模型训练阶段前向计算若干步比如步计算出子序列的损失值然后沿着这个子序列返现传播误差并计算梯度更新参数不断重复上述过程仍然会得到网络中的隐藏态就能用这些隐藏态计算下一批数据所以前向传播的计算是一样的但是反向传播只能根据第二批数据计算误差和梯度的其他应用理解了语言模型的训练过程就可以用它来做很多有意义的事情我们可以使用任何文本训练语言模型随后生成新的文本比如使用莎士比亚的诗集训练模型可以看到模型一开始的输出是毫无意义的内容但是随着整个训练过程的进行最终能够产生相对可以理解的内容当模型训练的非常好后会让其具有特定作者的写作风格还有一些更有趣的内容比如输入一本复杂的数学教科书或复杂的代码来训练模型也能够让模型学到一些对应的有趣的输出我们的初衷是为了让模型可以预测出下一个字符但是在用不同的数据集进行训练的时候会发现模型甚至可以学习到一些更复杂的特征为了理解模型的训练细节我们可以从模型的隐藏态取出元素找到隐藏态中某些可以解释的语义如果这样做会发现中间大多数内容都是无法理解的如下图所示图中每个字符的颜色对应着隐藏向量的单个标量元素的大小但是其中有一些结果非常不错比如有试着进行符号匹配的有试着统计回车符的有试着判断语句判断条件的有试着判断是否为代码注释的有试着检查代码缩进的这代表着不仅可以训练得出预测下一字符的模型还能学到很多其他关于输入数据结构的东西模型图像标注模型的作用是输入一张图像让模型输出用于描述图像语义信息的自然语言输出的语义信息是可变长的字符序列因为不同的有不同的字母数量所以十分适合使用因为和图像相关所以模型应该存在一部分用于产生图像的特征向量图像特征将输入到接下来的中在模型训练完成后会一次一个地产生标题单词如同字符级的语言预测一样一个例子首先把输入图像通过卷积神经网络但不需要得到分值而是使用来自模型末端被展平的向量来概述整个图像的内容随后需要使用使用需要知道模型的第一个初始化输入来告诉模型需要开始生成文字在这个例子中用一个特殊的开始记号来表示在之前的语言模型中我们已经知道了如何去计算每个的输出而现在需要添加图片信息有两种方式用完全不同的方式整合这些信息加入第三个权重矩阵在每个中加入图像信息使隐藏态表达式改为随后将计算词汇表中所有的分数分布这里的词汇表可能是所有的英文单词数据量很大所以将从分布中采样并在下一次当作输入传入得到关于所有词汇的分布再取样产生下一个词这样持续下去就能生成完整的句子了直到采样到特殊停止标记就停止生成单词一个真实模型在样本集上的输出结果但是这样训练出的模型总是不完美的如果让模型描述与训练数据相似的图片时可能会表现得很好但却很难描述与数据集相差太大的图片内容下面是几个预测错误的案例因为训练集中没有对应的样本注意力模型注意力模型是的一种类型当生成图像描述语言时可以引导模型的注意力到图像的不同部分卷积神经网络更倾向于产生由向量构成的网格而不是用一个单独的向量来整合整个图像图片中每个特殊的地方都用一个向量表示当模型进行前向传播时除了在每一步进行采样还会产生一个分布即在图像中模型想看的位置图像位置的分布可以看为一种张量这种张量描述了模型在训练过程中应该关注哪里第一个隐藏态计算图片位置的分布随后将分布返回到向量集合进行矩阵乘法这可能会把注意力集中在图像的一部分上现在向量得到了反馈作为神经网络下一步计算的额外输入下一步计算将产生两个输出一个是在词汇表上的分布一个是图像位置的分布随后将得到的位置分布再次和向量集合进行相乘作为下一步的输入进行前向传播如此循环往复模型训练完成后可以看到它会在图像中转移注意力在生成每个图片的单词时注意力会展现在图像中的不同位置注意力分为软注意力和硬注意力软注意力采用的是加权组合关注所有图像位置的特征而硬注意力限制模型每一步只能选择一个位置来观察图片下面是几个真实案例可以看到模型在生成时会倾向于集中注意力到图像中的某一位置视觉问答与的想法还可以用于视觉问答领域在整个问题中需要为模型准备两个输入一张图像一段针对图像的自然语言描述问题如下图所示我们可以将这个问题看作是把和连接起来得到的模型它需要将自然语言序列作为输入我们可以设想针对输入问题的每个元素建立从而将输入的问题概括为一个向量同样的也可以用将图像概括为一个向量随后将问题向量和图像向量进行结合直接连接或进行相乘通过编程来预测答案的概率分布三长短时记忆神经网络概要目前为止我们讨论的单层只有一种隐藏态而多层也是很常见的下图是一个三层的网络结构输入传入以后在第一层的网络中会产生一系列的隐藏态在运行了的一层后可以将隐藏态序列作为第二层的输入序列以此类推在神经网络中一般来说更深的模型会有更好的表现对来说也是如此许多问题都会用到两层或三层的模型但一般不会使用非常深的回想一下之前学习过的单层它的网络结构如下图所示当反向传播时模型首先会得到关于的导数经过各个节点后需要计算出关于的导数梯度流向如下图所示现在可以设想将许多连接成一个序列可以思考一下梯度流过一系列这样的层时会发生什么当我们想要计算关于的梯度时反向传播需要经过的每一个单元每经过一个单元时都要使用其中某一个权重的转置这意味着最终的表达式将会包含很多权重矩阵因子如果权重矩阵奇异值过大则会产生梯度爆炸如果权重矩阵奇异值过小则会发生梯度消失只有权重为时才不会发生这两种情况中的一种梯度截断是解决梯度爆炸问题的一个方法即在计算完梯度后如果梯度范式大于某个阈值就将它剪短并作除法这种做法就是既保证了梯度在每一次反向中可能是下降的又保证了不会爆炸虽然比较粗暴但是使用的比较多对于梯度消失问题常见的做法是换一个更复杂的结构这就是使用长短时记忆神经网络的原因是的一种更高级的递归结构他被设计用来缓解梯度爆炸和梯度消失问题它并不是直接在输出上想办法而是设计出更好的网络结构来获取更好的梯度流动的函数形式如下所示在中的每个中都维持了两个隐藏态一个是隐藏态一个是单元态其中隐藏态可以类比于中的隐藏态而单元态向量就相当于保留在内部的隐藏态并且不会暴露到外部具体计算过程是通过输入和两个隐藏态计算出四个值然后使用这四个值更新单元态再将作为下一个的隐藏态的细节中一共有个门分别是遗忘门更新门输入门输出门基本结构如下可以看到共有三个输入分别是短期记忆长期记忆输入数据会产生对应的两个输出首先看长期记忆他首先需要乘以一个函数如果值很小就表示忘记长期记忆反之则保留长期记忆该处就称为遗忘门随后会为其增加一些记忆长期记忆在网络中是贯通的对应的操作是最少的遗忘门控制长期记忆的遗忘程度输入当前输入数据和上一的隐藏态进行处理由于使用了如果数值过小会遗忘掉对应的数据数值过大则会保留如下图所示更新门的输入与遗忘门的输入相同但是用到了不同的一套权重和并且使用到了函数将计算结果相乘得到的就是需要增加的记忆计算完成后接下来就需要更新长期记忆新的短期记忆需要使用到新的权重输入是新的长期记忆当前输入以及上一短期记忆另一种形式权重可以看作是一个很大的矩阵也可也看作是个小矩阵反向传播在原始的中进行反向传播往往会出现梯度爆炸或梯度消失的现象原因是在反向传播的过程中会一直乘以一个矩阵但是在中就完全不同了在反向传播路径中需要计算单元态的梯度就能得到一个不错的结果我们通过传输进来的单元获得了上游梯度随后通过加法运算加法在反向传播中只起到分配梯度的作用向后进行反向传播并通过元素相乘的方式直接贯穿了反向传播过程随后上游梯度最终通过遗忘门得到相乘后的元素当我们通过单元态向后反向传播时对于上游的单元态梯度唯一会发生的事情就是它最终会通过遗忘门得到相乘后的元素这样比原始的好很多原因有两个遗忘门是矩阵元素相乘而不是矩阵乘法而矩阵元素相乘会比矩阵乘法更好一些矩阵元素相乘可能会在不同的时间点乘以一个不同的遗忘门而不是不断地乘以相同的权重矩阵因为遗忘门是一个函数所以矩阵元素相乘的结果会保证在到之间这会使数值性质更好在原始的每一步反向传播中都会经过一个激活函数但是在中使用了隐藏态来计算输出因此每一个隐藏态都从最后的单元态反向传播到第一个单元态只通过一个单一的向后反向传播而不是在每一步都单独设置函数结合以上内容可以发现通过单元态进行反向传播的路径类似于一种梯度高速公路使梯度相对畅通无阻地从模型最末端的损失函数返回到模型的初始单元态的梯度呢梯度的传播方式是在每一个时间步长中获得当前的单元态和隐藏态这会提供该时间点的局部梯度而的局部梯度会经过和由于很好地保存了的梯度所以在每一个时间步长的局部梯度也会随着时间的推移更加平稳地前向和反向传播实际上和非常相似在中有一条反向穿过网络的链接路径为反向传播提供了高速公路而在中单元态加法和元素乘法一起作用同样为反向传播提供了类似的高速公路门控循环单元是的变种在单元中使用元素乘法门和加法一起相互作用以避免梯度消失问题结构如下仅有一个重置门和一个更新门总结可以非常灵活地处理序列数据一对多多对一多对多初始十分简单但是有梯度消失和梯度爆炸的问题和的加法交互能够改善梯度流问题的反向传播会产生梯度爆炸和消失的问题梯度爆炸可以用梯度截断来解决梯度消失需要改进结构比如使用研究更好的结构是一个很热门的话题优缺点',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-04 13:22:06',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Kuhne" type="application/atom+xml">
</head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="/img/head.jpg"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><div class="back-home-button"><i class="anzhiyufont anzhiyu-icon-grip-vertical"></i><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" href="/null" title="暂无"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="暂无"/><span class="back-menu-item-text">暂无</span></a></div></div></div></div><a id="site-name" href="/" accesskey="h"><div class="title">Kuhne</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/?id=883682303&amp;server=netease&amp;type=playlist"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li></ul></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/essay/"><span> 闲言碎语</span></a></div></div></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><div class="nav-button" id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" title="搜索🔍" accesskey="s"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span> 搜索</span></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="/null" target="_blank"><img class="post-qr-code-img" alt="微信" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/null"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/null" target="_blank"><img class="post-qr-code-img" alt="支付宝" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/null"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/2022CVPR/" style="font-size: 1.05rem;">2022CVPR<sup>2</sup></a><a href="/tags/Git/" style="font-size: 1.05rem;">Git<sup>1</sup></a><a href="/tags/Kaggle/" style="font-size: 1.05rem;">Kaggle<sup>2</sup></a><a href="/tags/Linux/" style="font-size: 1.05rem;">Linux<sup>2</sup></a><a href="/tags/PyTorch/" style="font-size: 1.05rem;">PyTorch<sup>9</sup></a><a href="/tags/Python/" style="font-size: 1.05rem;">Python<sup>6</sup></a><a href="/tags/Pytorch/" style="font-size: 1.05rem;">Pytorch<sup>1</sup></a><a href="/tags/SpringCloud/" style="font-size: 1.05rem;">SpringCloud<sup>9</sup></a><a href="/tags/Vue/" style="font-size: 1.05rem;">Vue<sup>3</sup></a><a href="/tags/nginx/" style="font-size: 1.05rem;">nginx<sup>1</sup></a><a href="/tags/pycharm/" style="font-size: 1.05rem;">pycharm<sup>1</sup></a><a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 1.05rem; font-weight: 500; color: var(--anzhiyu-lighttext)">人工智能<sup>21</sup></a><a href="/tags/%E4%BD%9C%E4%B8%9A/" style="font-size: 1.05rem;">作业<sup>8</sup></a><a href="/tags/%E5%8A%A0%E5%AF%86/" style="font-size: 1.05rem;">加密<sup>1</sup></a><a href="/tags/%E5%90%8E%E7%AB%AF/" style="font-size: 1.05rem;">后端<sup>3</sup></a><a href="/tags/%E5%9F%BA%E7%A1%80/" style="font-size: 1.05rem;">基础<sup>12</sup></a><a href="/tags/%E5%B7%A5%E5%85%B7/" style="font-size: 1.05rem;">工具<sup>3</sup></a><a href="/tags/%E5%BD%92%E7%BA%B3/" style="font-size: 1.05rem;">归纳<sup>3</sup></a><a href="/tags/%E6%95%99%E7%A8%8B/" style="font-size: 1.05rem;">教程<sup>10</sup></a><a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 1.05rem;">服务器<sup>1</sup></a><a href="/tags/%E6%9C%AA%E5%AE%8C%E6%88%90/" style="font-size: 1.05rem;">未完成<sup>1</sup></a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">机器学习<sup>21</sup></a><a href="/tags/%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0/" style="font-size: 1.05rem;">模型复现<sup>8</sup></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem; font-weight: 500; color: var(--anzhiyu-lighttext)">深度学习<sup>15</sup></a><a href="/tags/%E7%BD%91%E9%A1%B5%E5%89%8D%E7%AB%AF/" style="font-size: 1.05rem;">网页前端<sup>4</sup></a><a href="/tags/%E8%80%83%E7%A0%94/" style="font-size: 1.05rem;">考研<sup>1</sup></a><a href="/tags/%E8%A7%86%E8%A7%89%E5%85%B3%E7%B3%BB%E6%8E%A2%E6%B5%8B/" style="font-size: 1.05rem;">视觉关系探测<sup>1</sup></a><a href="/tags/%E8%A7%86%E8%A7%89%E9%97%AE%E7%AD%94/" style="font-size: 1.05rem;">视觉问答<sup>1</sup></a><a href="/tags/%E8%A7%86%E9%A2%91%E9%97%AE%E7%AD%94/" style="font-size: 1.05rem;">视频问答<sup>1</sup></a><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" style="font-size: 1.05rem;">计算机视觉<sup>12</sup></a><a href="/tags/%E8%AE%BA%E6%96%87/" style="font-size: 1.05rem;">论文<sup>4</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="anzhiyufont anzhiyu-icon-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/09/"><span class="card-archive-list-date">九月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/02/"><span class="card-archive-list-date">二月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/09/"><span class="card-archive-list-date">九月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/08/"><span class="card-archive-list-date">八月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">12</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/07/"><span class="card-archive-list-date">七月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">11</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/06/"><span class="card-archive-list-date">六月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">8</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/05/"><span class="card-archive-list-date">五月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">16</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/04/"><span class="card-archive-list-date">四月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">8</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item on" id="consoleCommentBarrage" onclick="anzhiyu.switchCommentBarrage()" title="热评开关"><a class="commentBarrage"><i class="anzhiyufont anzhiyu-icon-message"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div><div class="console-btn-item" id="consoleKeyboard" onclick="anzhiyu.keyboardToggle()" title="快捷键开关"><a class="keyboard-switch"><i class="anzhiyufont anzhiyu-icon-keyboard"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="post-meta-categories"><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url">深度学习</a></span><span class="article-meta tags"><a class="article-meta__tags" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>计算机视觉</span></a><a class="article-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>深度学习</span></a></span></div></div><h1 class="post-title" itemprop="name headline">cs231n 学习笔记（9）循环神经网络</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2022-07-10T08:43:51.000Z" title="发表于 2022-07-10 16:43:51">2022-07-10</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2023-03-04T05:22:06.615Z" title="更新于 2023-03-04 13:22:06">2023-03-04</time></span></div><div class="meta-secondline"><span class="post-meta-separator"></span><span class="post-meta-wordcount"><i class="anzhiyufont anzhiyu-icon-file-word post-meta-icon" title="文章字数"></i><span class="post-meta-label" title="文章字数">字数总计:</span><span class="word-count" title="文章字数">6.7k</span><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-clock post-meta-icon" title="阅读时长"></i><span class="post-meta-label" title="阅读时长">阅读时长:</span><span>20分钟</span></span><span class="post-meta-separator"></span><span class="post-meta-pv-cv" id="" data-flag-title="cs231n 学习笔记（9）循环神经网络"><i class="anzhiyufont anzhiyu-icon-fw-eye post-meta-icon"></i><span class="post-meta-label" title="阅读量">阅读量:</span><span id="busuanzi_value_page_pv"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-spin"></i></span></span><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为杭州"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>杭州</span><span class="post-meta-separator"></span><span class="post-meta-commentcount"><i class="anzhiyufont anzhiyu-icon-comments post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/posts/3fc42b2c.html#post-comment" tabindex="-1"><span id="twikoo-count"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-spin"></i></span></a></span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg"></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="https://kuhne.gitee.io/kuhne.gitee.io/posts/3fc42b2c.html"><header><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url">深度学习</a><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" tabindex="-1" itemprop="url">计算机视觉</a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" tabindex="-1" itemprop="url">深度学习</a><h1 id="CrawlerTitle" itemprop="name headline">cs231n 学习笔记（9）循环神经网络</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">Kuhne</span><time itemprop="dateCreated datePublished" datetime="2022-07-10T08:43:51.000Z" title="发表于 2022-07-10 16:43:51">2022-07-10</time><time itemprop="dateCreated datePublished" datetime="2023-03-04T05:22:06.615Z" title="更新于 2023-03-04 13:22:06">2023-03-04</time></header><h1>cs231n 学习笔记（9）循环神经网络</h1>
<h2 id="一、-循环神经网络介绍">一、 循环神经网络介绍</h2>
<h3 id="1-1-RNN-与-NN">1.1 RNN 与 NN</h3>
<p>所有的神经网络架构都有类似于 “Vanilla” 神经网络（如下图所示）的基础架构，<strong>输入</strong>是固定尺寸的对象，比如一幅图片或一个向量，它在经过一些隐藏层后，会给出<strong>单一</strong>的输出结果（比如一个分类）。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/115411-77598.png" alt="image-20220709115411489"></p>
<p>但是在机器学习中，有时候我们希望计算机有更高的灵活性，能处理不同数据类型。当谈到<strong>循环神经网络</strong> （RNN） 时，处理各种类型的输入和输出数据就有很大的发挥空间。</p>
<p>一旦使用了 RNN，就可以实现 <strong>1 对多</strong>的模型，输入是固定尺寸的对象，如一幅图片，输出是可变长度的序列，比如一段文字描述（Caption）；也可也实现 <strong>1 对 1</strong> 的模型，但输入的尺寸是可变的，比如输入一段文字，得到该文字的感情属性（积极 / 消极），或是对视频进行分类；  可以实现<strong>多对多</strong>的模型，比如机器翻译，输入是英文句子，输出中文句子，长度都可变；最后一种是<strong>多对多</strong>的另一形式，对输入的每一单元做出处理，比如输入一段视频，要求对视频的每一帧进行分类决策。</p>
<p>RNN 就是用于<strong>处理大小可变的有序数据</strong>的一类模型，使我们可以比较自然地理解模型中这些不同的架构，因此 RNN 非常重要，且 RNN 对<strong>固定输入和输出</strong>问题也同样有效。</p>
<p>如果我们想要对输入进行<strong>序列化处理</strong>，比如收到了一个固定大小的图片，希望做出分类决策，比如判断图像中的数字是多少，但 RNN 不是做单一的前向传播，随后马上做出决策，而是<strong>观察一幅图片的不同部分，完成一组观察后，再最终做出判断。</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/130153-36938.png" alt="image-20220709130153766"></p>
<p>还可以使用 RNN 来生成图像，让模型来合成一些类似训练集图片的图像，一次大概能生成一张，但是可以让模型一直工作，每次计算出图像的一部分。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/130548-38fc1.png" alt=""></p>
<h3 id="1-2-RNN-结构">1.2 RNN 结构</h3>
<p>每个 RNN 网络都有一个小小的<strong>循环核心单元</strong>，他把 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 作为输入，将其传入 RNN ，而 RNN 有一个内部隐藏态，这一隐藏态会<strong>在 RNN 每次读取新的输入时更新</strong>，在下一次读取输入时，隐藏态会将结果反馈至模型。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/131118-dbee2.png" alt="image-20220709131118789"></p>
<p>通常来说，我们希望 RNN 在每一步都能给出输出，因此就有了这样的模式：<strong>读取数据 - &gt; 更新隐藏态 - &gt; 生成输出</strong>。这一循环过程的计算公式如下：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/131328-95c19.png" alt="image-20220709131328570"></p>
<p>在一个 RNN 模块中，我们对某种循环关系用函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> 进行计算，该函数依赖权重 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>，他接收隐藏态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9028em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span> 和输入数据 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> ，随后可以输出更新后的隐藏态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。在读取下一个输入时，更新后的隐藏态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>  将再作为输入传入同一个函数。</p>
<p>如果我们希望在网络的每一步都生成一些输出，就可以增加全连接层，每一步都将 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 作为输入，根据每一步的隐藏态来做出决策。</p>
<blockquote>
<p>需要注意，每一步计算使用的是<strong>同样的</strong>函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>W</mi></msub></mrow><annotation encoding="application/x-tex">f_W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和权重 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span> 。</p>
</blockquote>
<h3 id="1-3-Vanilla-RNN">1.3 Vanilla RNN</h3>
<p>最简单的 RNN 被称为 Vanilla RNN：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><msub><mi>W</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>W</mi><mrow><mi>x</mi><mi>h</mi></mrow></msub><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_t=tanh(W_{hh}h_{t-1}+W_{xh}x_t)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">anh</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">hh</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mord mathnormal mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>该模型仅使用了非线性函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">tanh</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">anh</span></span></span></span> ，在此架构中，如果希望每一步都生成 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">y_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> ，就需要另外一个矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">W_y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span> 来接受这一隐藏态，将其转化为某个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span> 。</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>=</mo><msub><mi>W</mi><mrow><mi>h</mi><mi>y</mi></mrow></msub><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">y_t=W_{hy}h_t
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<h3 id="1-4-RNN-计算图">1.4 RNN 计算图</h3>
<p>提到 RNN 时，可以从两方面来思考：</p>
<ol>
<li>他有一种隐藏态</li>
<li>可以将隐藏态循环反馈给自身</li>
</ol>
<p>RNN 的计算图如下， 有输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，初始隐藏态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">h_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>（通常情况下 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">h_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 等于 0 ），输入函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>W</mi></msub></mrow><annotation encoding="application/x-tex">f_W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> ，可以计算出下一个隐藏态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">h_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/133042-448d6.png" alt=""></p>
<p>该计算图可能让人感到疑惑，将其展开更多的步时有助于更好地理解，该计算步骤会一直持续，直到用完输入项 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> （多对多情况）：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/133150-d4758.png" alt=""></p>
<p>现在可以将权重矩阵标注在计算流程图上，可以看到每次计算过程中都在<strong>重复使用相同的权重矩阵</strong>。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/133748-1ebe7.png" alt=""></p>
<p>如果将反向传播的原理应用到这个模型中，就能得到每一步计算出的梯度，最终 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span> 的梯度是在所有过程中独立算出的梯度和。</p>
<p>同样的，我们也可以直接把 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">y_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 画在计算图中，这样每个 time step 下输出的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 会重新作为输入，计算出对应的 $ y_t$ 。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/134320-4bd18.png" alt=""></p>
<p>如果每一个 time step 都有一个<strong>对应的真实 label</strong> ，这样就能计算出对应的 loss 。这里的 loss 通常是 softmax 损失，计算这样的损失需要序列在每个时步下都有对应的真实 label ，损失值是整个训练中单独损失值的总和。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/140313-35fd1.png" alt="="></p>
<p>如果是<strong>多对 1</strong> 的情况，通常会根据网络最终的隐藏层做出决策，并计算损失，因为最终隐藏态整合了序列中包含的所有情况；如果是 <strong>1 对多</strong>情况，则会接受固定长度的输入项，然后输出不定长的输出项，之后这个固定长的输入项会被做为<strong>模型的初始隐藏态</strong>，随后 RNN 会对输出的单元逐个进行处理，最终会得到不定长的输出序列。</p>
<h3 id="1-5-Sequence-to-Sequence-模型">1.5 Sequence to Sequence 模型</h3>
<p>Sequence to Sequence 模型能用来解决机器翻译之类的问题，输入和输出都是一个不定长的序列，可以看作是多对 1 和 1 对多的结合，这样就存在了两个过程：<strong>编码</strong>和<strong>解码</strong>。</p>
<p>在<strong>编码阶段</strong>，会接收不定长度的输入，整个输入会被编码器网络最终的隐藏态编码成一个单独向量，是一个<strong>多对一</strong>的过程。</p>
<p>在<strong>解码阶段</strong>，  输入是编码好的单一向量，经过网络会生成一个不定长的输出序列，这个不定长的输出需要在每一个 time step 下做出预测（比如预测接下来的用词）。</p>
<p>随后将整个计算图展开，对输出序列的损失求和，像之前一样用反向传播来训练模型。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/142755-f7406.png" alt=""></p>
<h2 id="二、RNN-应用">二、RNN 应用</h2>
<h3 id="2-1-语言建模">2.1 语言建模</h3>
<p>RNN 经常会被应用到语言建模问题上，如果想通过读取大量语句，从而让神经网络在一定程度上学会生成自然语言，这在<strong>字符水平</strong>上是可行的，即让模型逐个生成字符，也可以在单词层面上让模型逐个生成单词，下面是一个字符水平上的例子。</p>
<p>假如有一个字符级的语言模型，网络会读取一串字符序列，随后需要去预测该文本流的下一个字符是什么 。假设有一个很小的字符表：h, e, l, o，以及一个训练序列样本 “hello”。在这个语言模型的训练阶段，会将整个字符序列作为输入项 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 。</p>
<p>考虑到输入实例是一个个字母，所以需要想办法在神经网络中表示这些字母，在这个例子中，因为单词是由 4 个字母组成的，所以每个字母将会由一个<strong>其他位置都为 0 ，对应字母所在位置为 1 的单位向量</strong>构成，如下图所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/144541-efaf6.png" alt=""></p>
<p>随着网络的前向传播，在第一个时步中，神经网络会接收到输入字母 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span></span></span></span>， 该输入项会进入到第一个 RNN 单元内，得到第一层的输出 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/145621-0a475.png" alt=""></p>
<p>隐藏层计算完成后会计算输出 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">y_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> ，即网络对组成单词的每个字母做出的预测，预测结果就是他认为接下来最可能出现的字母。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/145722-cd3ed.png" alt=""></p>
<p>我们的训练字母序列是 hello ，下一个正确的字母应该是 e ，但模型却认为是 o （因为对应的数值最大），在这种预测错误的情况下，我们可以使用 <code>softmax</code> 损失函数度量预测差距。</p>
<p>用一个字母计算完成后，模型会读入第二个字母 e ，重复执行上述步骤，输入上一个隐藏态，计算出一个新的隐藏态，用这个隐藏态再次对字母做出预测。如果用不同的字母序列去训练这个模型，最终它将会学习到如何基于之前出现过的字符来预测接下来出现的字符。</p>
<p>在模型的<strong>测试阶段</strong>，一般采用的方法是<strong>输入文本的前缀</strong>来测试该模型，比如输入一个字母 h，得到输出结果，再用 <code>softmax</code> 函数得到对应的概率分布，进而得到输出，生成序列中的第二个字母，在这种情况下，即使得分情况非常不好，也有可能在概率分布图中得到字母 e。</p>
<blockquote>
<p>这里是为什么？？</p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/152945-63dda.png" alt=""></p>
<blockquote>
<p>Q：为什么不选择概率最高的字母？</p>
<p>A：并不直接取概率最高的字符输出，而是基于 <code>softmax</code> 输出的概率分布进行<strong>采样</strong>，高概率的字符自然更易被采样并输出，但低概率的也有几率被输出，从而增加了输出的不确定性/多样性</p>
</blockquote>
<p>现在我们将得到的字母 e 在下一步中重新输入到神经网络中，不断重复上述步骤，可以生成一个新的序列。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/153002-dde4c.png" alt=""></p>
<h3 id="2-2-沿时间的截断反向传播法">2.2 沿时间的截断反向传播法</h3>
<p>提供一个序列给模型，模型能在每个时间步上产生一个输出结果，最后再计算 loss ，这就是<strong>沿时间的反向传播算法</strong>，因为在前向传播过程中，是沿着时间做前向计算，在反向传播过程中，要<strong>逆着时间反向计算所有的梯度</strong>。如果想要训练一个很长的序列 ，这个误差反向传播过程实际上有些<strong>麻烦</strong>，且非常<strong>费时</strong>，而且每次计算梯度时都必须进行前向传播，遍历所有的输入，误差反向传播也会遍历所有的文本，模型难以收敛，<strong>内存占用极高</strong>。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/154608-e17a4.png" alt=""></p>
<p>在实际应用中，人们通常会采用一种近似的方法，称之为<strong>沿时间的截断反向传播法</strong>。这个方法的思想是，<strong>在模型训练阶段，前向计算若干步（比如 100 步），计算出子序列的损失值，然后沿着这个子序列返现传播误差，并计算梯度更新参数</strong>。不断重复上述过程，仍然会得到网络中的隐藏态，就能用这些隐藏态计算下一批数据，所以前向传播的计算是一样的，但是反向传播只能根据第二批数据计算误差和梯度。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/155848-81b92.png" alt=""></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/155857-49962.png" alt=""></p>
<h3 id="2-3-RNN-的其他应用">2.3 RNN 的其他应用</h3>
<p>理解了 RNN 语言模型的训练过程，就可以用它来做很多有意义的事情，我们可以使用任何文本，训练 RNN 语言模型，随后生成新的文本，比如使用莎士比亚的诗集训练模型。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/160422-ec315.png" alt="image-20220709160422520"></p>
<p>可以看到，模型一开始的输出是毫无意义的内容，但是随着整个训练过程的进行，最终能够产生相对可以理解的内容，当模型训练的非常好后，会让其具有特定作者的<strong>写作风格</strong>。</p>
<p>还有一些更有趣的内容，比如输入一本复杂的数学教科书或复杂的代码来训练模型，也能够让模型学到一些对应的有趣的输出。</p>
<p>我们的初衷是为了让模型可以预测出下一个字符，但是在用不同的数据集进行训练的时候，会发现模型甚至可以学习到一些更复杂的特征，为了理解模型的训练细节，我们可以从模型的隐藏态取出元素，找到隐藏态中某些可以解释的语义。</p>
<p>如果这样做，会发现中间大多数内容都是无法理解的，如下图所示，图中每个字符的颜色对应着隐藏向量的单个标量元素的大小：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/163137-5f047.png" alt=""></p>
<p>但是其中有一些结果非常不错，比如有试着进行<strong>符号匹配</strong>的：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/163359-c1cd5.png" alt=""></p>
<p>有试着<strong>统计回车符</strong>的：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/163420-20d7b.png" alt=""></p>
<p>有试着<strong>判断 <code>if</code> 语句判断条件</strong>的：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/163539-5a9aa.png" alt=""></p>
<p>有试着判断是否为<strong>代码注释</strong>的：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/163612-74102.png" alt=""></p>
<p>有试着检查<strong>代码缩进</strong>的：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/163642-dfcc4.png" alt=""></p>
<p>这代表着 RNN 不仅可以训练得出预测下一字符的模型，还能学到很多其他关于输入数据结构的东西。</p>
<h3 id="2-4-Image-Caption-模型">2.4 Image Caption 模型</h3>
<p><strong>图像标注</strong>（Image Caption）<strong>模型</strong>的作用是，输入一张图像，让模型输出用于描述图像语义信息的自然语言，输出的语义信息是可变长的字符序列，因为不同的 label 有不同的字母数量，所以十分适合使用 RNN ，因为和图像相关，所以模型应该存在<strong>一部分 CNN</strong>，用于产生图像的特征向量，图像特征将输入到接下来的 RNN 中，在模型训练完成后，会一次一个地产生标题单词，如同字符级的语言预测一样。</p>
<p>一个例子：首先把输入图像通过卷积神经网络，但不需要得到 <code>softmax</code> 分值，而是使用来自模型末端被展平的向量，来概述整个图像的内容。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/172051-4fd07.png" alt=""></p>
<p>随后需要使用 RNN，使用 RNN 需要知道模型的第一个初始化输入，来告诉模型需要开始生成文字（在这个例子中用一个特殊的开始记号 <strong>&lt;START&gt;</strong> 来表示）。</p>
<p>在之前的 RNN 语言模型中，我们已经知道了如何去计算每个 time step 的输出，而现在需要添加图片信息，有两种方式：</p>
<ol>
<li>
<p>用完全不同的方式整合这些信息；</p>
</li>
<li>
<p>加入第三个权重矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{in}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">in</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，在每个 time step 中加入图像信息，使隐藏态表达式改为：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>h</mi><mo>=</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><msub><mi>W</mi><mrow><mi>x</mi><mi>h</mi></mrow></msub><mo>∗</mo><mi>x</mi><mo>+</mo><msub><mi>W</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><mo>∗</mo><mi>h</mi><mo>+</mo><msub><mi>W</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>∗</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h=tanh(W_{xh}*x+W_{hh}*h+W_{in}*v)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">anh</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mord mathnormal mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">hh</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">in</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mclose">)</span></span></span></span></span></p>
</li>
</ol>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/173600-d40ec.png" alt="image-20220709173600077"></p>
<p>随后将计算词汇表中所有的分数分布，这里的词汇表可能是<strong>所有的英文单词</strong>，数据量很大， 所以将从分布中采样，并在下一次 time step 当作输入传入，得到关于所有词汇的分布， 再取样产生下一个词，这样持续下去，就能生成完整的句子了，直到采样到特殊停止标记 &lt;END&gt; ，就停止生成单词。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/173618-461b8.png" alt=""></p>
<p>一个真实模型在样本集上的输出结果：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/173856-d676f.png" alt="image-20220709173855775"></p>
<p>但是这样训练出的模型总是不完美的，如果让模型描述<strong>与训练数据相似的图片</strong>时，可能会表现得很好，但却很难描述<strong>与数据集相差太大的图片</strong>内容，下面是几个预测错误的案例，因为训练集中没有对应的样本。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/174402-c6e0f.png" alt=""></p>
<h3 id="2-5-注意力模型（Attention）">2.5 注意力模型（Attention）</h3>
<p>注意力模型是 Image Caption 的一种类型，当生成图像描述语言时，可以引导模型的注意力到图像的不同部分。卷积神经网络更倾向于产生<strong>由向量构成的网格</strong>，而不是用一个单独的向量来整合整个图像，图片中每个特殊的地方都用一个向量表示：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/101936-7e4b5.png" alt=""></p>
<p>当模型进行前向传播时，除了在每一步进行采样，还会产生一个分布，即在图像中模型想看的位置。图像位置的分布可以看为一种张量，这种张量描述了模型在训练过程中应该关注哪里。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/102040-d5101.png" alt=""></p>
<p>第一个隐藏态计算<strong>图片位置的分布</strong>，随后将分布返回到向量集合（进行矩阵乘法），这可能会把注意力集中在图像的一部分上，现在向量得到了反馈，作为神经网络下一步计算的额外输入。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/102320-50202.png" alt=""></p>
<p>下一步计算将产生两个输出，一个是在<strong>词汇表上的分布</strong>，一个是<strong>图像位置的分布</strong>。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/102747-82f90.png" alt=""></p>
<p>随后将得到的位置分布再次和向量集合进行相乘，作为下一步的输入进行前向传播，如此循环往复。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/103011-b1a4e.png" alt=""></p>
<p>模型训练完成后，可以看到它会在图像中转移注意力，在生成每个图片 caption 的单词时，注意力会展现在图像中的不同位置。注意力分为软注意力（soft attention） 和硬注意力（Hard attention），软注意力采用的是加权组合， 关注<strong>所有图像位置的特征</strong>；而硬注意力限制模型每<strong>一步只能选择一个位置</strong>来观察图片。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/103202-ab04a.png" alt="image-20220710103202119"></p>
<p>下面是几个真实案例，可以看到模型在生成 caption 时，会倾向于集中注意力到图像中的某一位置</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/103639-2ef15.png" alt=""></p>
<h3 id="2-6-视觉问答（Visual-Question-Answering）">2.6 视觉问答（Visual Question Answering）</h3>
<p>RNN 与 Attention 的想法还可以用于视觉问答领域，在整个问题中，需要为模型准备两个输入：</p>
<ul>
<li>一张图像</li>
<li>一段针对图像的自然语言描述问题</li>
</ul>
<p>如下图所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/104105-0e5c6.png" alt=""></p>
<p>我们可以将这个问题看作是把 CNN 和 RNN 连接起来得到的模型，它需要将自然语言序列作为输入，我们可以设想针对输入问题的每个元素建立 RNN ，从而将输入的问题概括为一个向量，同样的，也可以用 CNN 将图像概括为一个向量。随后将<strong>问题向量</strong>和<strong>图像向量</strong>进行结合（直接连接或进行相乘），通过 RNN 编程来预测答案的概率分布。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/104458-be0da.png" alt=""></p>
<h2 id="三、长短时记忆神经网络（LSTM）">三、长短时记忆神经网络（LSTM）</h2>
<h3 id="3-1-LSTM-概要">3.1 LSTM 概要</h3>
<p>目前为止，我们讨论的<strong>单层 RNN</strong>只有一种隐藏态，而多层 RNN 也是很常见的，下图是一个三层 RNN 的网络结构：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/105202-d3473.png" alt=""></p>
<p>输入传入以后，在第一层的 RNN 网络中会产生一系列的隐藏态，在运行了 RNN 的一层后，可以将隐藏态序列作为第二层 RNN 的输入序列，以此类推。</p>
<p>在神经网络中，一般来说更深的模型会有更好的表现，对 RNN 来说也是如此，许多问题都会用到两层或三层的 RNN 模型，但一般不会使用非常深的 RNN 。</p>
<p>回想一下之前学习过的单层 RNN ，它的网络结构如下图所示：<br>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/110229-68321.png" alt="image-20220710110229515"></p>
<p>当反向传播时，模型首先会得到 loss function 关于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的导数，经过各个节点后，需要计算出 loss function 关于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9028em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span> 的导数，梯度流向如下图所示：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/110639-1a667.png" alt=""></p>
<p>现在可以设想，将许多 RNN 连接成一个序列，可以思考一下梯度流过一系列这样的层时，会发生什么？</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/110812-ae507.png" alt=""></p>
<p>当我们想要计算 lossfn 关于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">h_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的梯度时，反向传播需要经过 RNN 的每一个单元，每经过一个单元时，都要使用其中某一个权重 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span> 的转置，这意味着最终的表达式将会包含很多权重矩阵因子，如果权重矩阵奇异值过大，则会产生<strong>梯度爆炸</strong>；如果权重矩阵奇异值过小，则会发生<strong>梯度消失</strong>，只有权重为 1 时才不会发生这两种情况中的一种。</p>
<p><strong>梯度截断</strong>是解决梯度<strong>爆炸问题</strong>的一个方法，即在计算完梯度后，如果梯度 L2 范式大于某个阈值，就将它剪短，并作除法。这种做法就是既保证了梯度在每一次反向中可能是下降的，又保证了不会爆炸，虽然比较粗暴，但是使用的比较多。</p>
<p>对于<strong>梯度消失</strong>问题，常见的做法是换一个更复杂的 RNN 结构，这就是使用长短时记忆神经网络（ <code>LSTM</code>）的原因。</p>
<p>LSTM 是 RNN 的一种更高级的递归结构，他被设计用来缓解梯度爆炸和梯度消失问题，它并不是直接在输出上想办法，而是设计出更好的网络结构，来获取更好的梯度流动，LSTM 的函数形式如下所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/112124-aff1b.png" alt=""></p>
<p>在 LSTM 中的每个 time step 中，都维持了两个隐藏态，一个是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> （隐藏态）一个是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">c_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>（单元态）。</p>
<p>其中<strong>隐藏态</strong>可以类比于 Vanilla RNN 中的隐藏态，而单元态向量就相当于保留在 LSTM 内部的隐藏态，并且不会暴露到外部。</p>
<p>具体计算过程是，通过输入和两个隐藏态，计算出四个值 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo separator="true">,</mo><mi>f</mi><mo separator="true">,</mo><mi>o</mi><mo separator="true">,</mo><mi>g</mi></mrow><annotation encoding="application/x-tex">i,f,o,g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">o</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span></span></span></span>，然后使用这四个值更新单元态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">c_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> ，再将 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">c_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 作为下一个 time step 的隐藏态。</p>
<h3 id="3-2-LSTM-的细节">3.2 LSTM 的细节</h3>
<p>LSTM 中一共有 4 个门，分别是遗忘门 f、更新门 g、输入门 i、输出门 o，基本结构如下：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/113417-fec7f.png" alt=""></p>
<p>可以看到，LSTM 共有三个输入，分别是短期记忆 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9028em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span> 、长期记忆 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">c_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span>、输入数据 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，会产生对应的两个输出 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">c_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 。</p>
<p>首先看长期记忆 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">c_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span>，他首先需要乘以一个 <code>sigmoid </code> 函数，如果 <code>sigmoid</code> 值很小，就表示忘记长期记忆，反之则保留长期记忆，该处就称为<strong>遗忘门</strong>，随后会为其<strong>增加</strong>一些记忆。 长期记忆在网络中是贯通的，对应的操作是最少的。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/114029-690c7.png" alt=""></p>
<p><strong>遗忘门</strong>控制长期记忆的遗忘程度，输入<strong>当前输入数据</strong>和上一 time step 的<strong>隐藏态</strong>，进行处理，由于使用了 <code>Sigmoid</code> ，如果数值过小，会遗忘掉对应的数据，数值过大则会保留，如下图所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/114257-1f45c.png" alt=""></p>
<p><strong>更新门</strong>的输入与遗忘门的输入相同，但是用到了不同的一套权重（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">W_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>C</mi></msub></mrow><annotation encoding="application/x-tex">W_C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>），并且使用到了 tanh 函数，将计算结果相乘，得到的就是需要增加的记忆</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/115108-0ede1.png" alt=""></p>
<p>计算完成后，接下来就需要更新长期记忆:</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/115211-16770.png" alt=""></p>
<p>新的短期记忆需要使用到新的权重 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>o</mi></msub></mrow><annotation encoding="application/x-tex">W_o</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> ，输入是新的长期记忆、当前输入以及上一短期记忆：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/115352-55af2.png" alt="image-20220710115352371"></p>
<p>LSTM 另一种形式：<br>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/123042-7e0f8.png" alt=""></p>
<blockquote>
<p>权重 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span> 可以看作是一个很大的矩阵，也可也看作是 4 个小矩阵</p>
</blockquote>
<h3 id="3-3-LSTM-反向传播">3.3 LSTM 反向传播</h3>
<p>在原始的 RNN 中，进行反向传播往往会出现梯度爆炸或梯度消失的现象，原因是在反向传播的过程中会一直乘以一个矩阵 W，但是在 LSTM 中就完全不同了。</p>
<p>在反向传播路径中，需要计算单元态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">c</span></span></span></span> 的梯度，就能得到一个不错的结果。我们通过传输进来的单元 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">c_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 获得了上游梯度，随后通过加法运算（加法在反向传播中只起到分配梯度的作用），向后进行反向传播，并通过元素相乘的方式直接贯穿了反向传播过程，随后上游梯度最终通过遗忘门，得到相乘后的元素，当我们通过单元态向后反向传播时，对于上游的单元态梯度唯一会发生的事情就是，它最终会通过遗忘门得到相乘后的元素，这样比原始的 RNN 好很多，原因有两个：</p>
<ol>
<li>遗忘门是<strong>矩阵元素相乘</strong>，而不是矩阵乘法，而矩阵元素相乘会比矩阵乘法更好一些</li>
<li>矩阵元素相乘可能会在<strong>不同的时间点</strong>乘以一个<strong>不同的遗忘门</strong>，而不是不断地乘以相同的权重矩阵</li>
</ol>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/142020-e787c.png" alt=""></p>
<p>因为遗忘门是一个 <code>sigmoid</code> 函数，所以矩阵元素相乘的结果会保证在 0 到 1 之间，这会使数值性质更好。</p>
<p>在原始 RNN 的每一步反向传播中，都会经过一个 <code>tanh</code> 激活函数，但是在 LSTM 中，使用了隐藏态来计算输出 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">y_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>， 因此每一个隐藏态，都从最后的单元态反向传播到第一个单元态，只通过一个单一的 <code>tanh</code> 向后反向传播，而不是在每一步都单独设置 tanh 函数。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/144323-0f2a0.png" alt=""></p>
<p>结合以上内容，可以发现，通过单元态进行反向传播的路径类似于一种 “梯度高速公路” ，使梯度相对畅通无阻地从模型最末端的损失函数返回到模型的初始单元态。</p>
<blockquote>
<p>Q：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span> 的梯度呢？</p>
<p>A：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span> 梯度的传播方式是在每一个时间步长中获得当前的<strong>单元态</strong>和<strong>隐藏态</strong>，这会提供该时间点 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span> 的局部梯度，而 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span> 的局部梯度会经过 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">c</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span></span></span></span>，由于 LSTM 很好地保存了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">c</span></span></span></span> 的梯度，所以在每一个时间步长 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span> 的局部梯度也会随着时间的推移更加平稳地前向和反向传播。</p>
</blockquote>
<p>LSTM 实际上和 ResNet 非常相似，在 ResNet 中有一条反向穿过网络的链接路径，为反向传播提供了 “高速公路”，而在 LSTM 中单元态加法和元素乘法一起作用，同样为反向传播提供了类似的  “高速公路”。</p>
<blockquote>
<p>门控循环单元（ GRU）是 LSTM 的变种，在 GRU 单元中，使用元素乘法门和加法一起相互作用，以避免梯度消失问题，结构如下，仅有一个重置 (reset) 门和一个更新 (update) 门</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/10/145935-5d5f1.png" alt=""></p>
</blockquote>
<h2 id="总结">总结</h2>
<ul>
<li>
<p><strong>RNN 可以非常灵活地处理序列数据</strong>（一对多、多对一、多对多）；</p>
</li>
<li>
<p><strong>初始 RNN 十分简单，但是有梯度消失和梯度爆炸的问题</strong>；</p>
</li>
<li>
<p>LSTM 和 GRU 的<strong>加法交互</strong>，能够改善梯度流问题；</p>
</li>
<li>
<p>RNN 的反向传播会产生梯度爆炸和消失的问题，梯度爆炸可以用<strong>梯度截断</strong>来解决；梯度消失需要改进 RNN 结构，比如使用 LSTM；</p>
</li>
<li>
<p>研究更好的 RNN 结构是一个很热门的话题。</p>
</li>
<li>
<p>RNN 优缺点：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/09/171153-2b7fb.png" alt=""></p>
</li>
</ul>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/head.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/head.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">Kuhne</div><div class="post-copyright__author_desc">间接性内卷 持续性摆烂</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="https://kuhne.gitee.io/kuhne.gitee.io/posts/3fc42b2c.html">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('https://kuhne.gitee.io/kuhne.gitee.io/posts/3fc42b2c.html')">cs231n 学习笔记（9）循环神经网络</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="/null" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/null" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/null" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/null" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="https://kuhne.gitee.io/kuhne.gitee.io/posts/3fc42b2c.html"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=cs231n 学习笔记（9）循环神经网络&amp;url=https://kuhne.gitee.io/kuhne.gitee.io/posts/3fc42b2c.html&amp;pic=https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="rm.copyPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kuhne.gitee.io/kuhne.gitee.io" target="_blank">Kuhne</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>计算机视觉<span class="tagsPageCount">12</span></a><a class="post-meta__box__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>深度学习<span class="tagsPageCount">15</span></a></div></div></div><div class="post_share"><div class="social-share" data-image="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2023/09/07/20230907192257-a360e8.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/2609eb73.html"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/1cc2cc32c428a61ce3eb2f191668eb9c.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">PyTorch 学习笔记</div></div></a></div><div class="next-post pull-right"><a href="/posts/1819e07.html"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">cs231n 学习笔记（10）目标检测与图像分割</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="anzhiyufont anzhiyu-icon-thumbs-up fa-fw" style="font-size: 1.5rem; margin-right: 4px"></i><span>喜欢这篇文章的人也看了</span></div><div class="relatedPosts-list"><div><a href="/posts/4a818336.html" title="cs231n 学习笔记（12）生成模型"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2022-07-24</div><div class="title">cs231n 学习笔记（12）生成模型</div></div></a></div><div><a href="/posts/1af2ae7a.html" title="cs231n 学习笔记（11）可视化与理解"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2022-07-15</div><div class="title">cs231n 学习笔记（11）可视化与理解</div></div></a></div><div><a href="/posts/1819e07.html" title="cs231n 学习笔记（10）目标检测与图像分割"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2022-07-15</div><div class="title">cs231n 学习笔记（10）目标检测与图像分割</div></div></a></div><div><a href="/posts/dc7d8d44.html" title="cs231n 学习笔记（8）经典的 CNN 架构"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2022-07-03</div><div class="title">cs231n 学习笔记（8）经典的 CNN 架构</div></div></a></div><div><a href="/posts/90deef5.html" title="cs231n 学习笔记（7）深度学习软件"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2022-06-26</div><div class="title">cs231n 学习笔记（7）深度学习软件</div></div></a></div><div><a href="/posts/39060a00.html" title="cs231n 学习笔记（6）训练神经网络（二）"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2022-06-22</div><div class="title">cs231n 学习笔记（6）训练神经网络（二）</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="anzhiyufont anzhiyu-icon-comments"></i><span> 评论</span></div><div class="comment-randomInfo"><a onclick="anzhiyu.addRandomCommentInfo()" href="javascript:void(0)">匿名评论</a><a href="/privacy" style="margin-left: 4px">隐私政策</a></div><div class="comment-tips" id="comment-tips"><span>✅ 你无需删除空行，直接评论以获取最佳展示效果</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div><div class="comment-barrage"></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info__sayhi" id="author-info__sayhi" onclick="anzhiyu.changeSayHelloText()"></div><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-status"><img class="g-status" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://emotion.acs.pw/emotion/bilibili_hotKey/危.jpg" ait="status"/></div></div><div class="author-info__description"><div style="line-height:1.38;margin:0.6rem 0;text-align:justify;color:rgba(255, 255, 255, 0.8);">一个菜鸟的垃圾加工厂。</div><div style="line-height:1.38;margin:0.6rem 0;text-align:justify;color:rgba(255, 255, 255, 0.8);">希望你你可以在这里找到对你有用的<b style="color:#fff">有用</b>的<b style="color:#fff">垃圾</b>。</div></div><div class="author-info__bottom-group"><a class="author-info__bottom-group-left" href="/"><h1 class="author-info__name">Kuhne</h1><div class="author-info__desc">间接性内卷 持续性摆烂</div></a><div class="card-info-social-icons is-center"><a class="social-icon faa-parent animated-hover" href="https://github.com/" target="_blank" title="Github"><i class="anzhiyufont anzhiyu-icon-github"></i></a><a class="social-icon faa-parent animated-hover" href="https://space.bilibili.com/" target="_blank" title="BiliBili"><i class="anzhiyufont anzhiyu-icon-bilibili"></i></a></div></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bullhorn anzhiyu-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来看我的博客~</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">cs231n 学习笔记（9）循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.1.</span> <span class="toc-text">一、 循环神经网络介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-RNN-%E4%B8%8E-NN"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 RNN 与 NN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-RNN-%E7%BB%93%E6%9E%84"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2 RNN 结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-Vanilla-RNN"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.3 Vanilla RNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-RNN-%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">1.1.4.</span> <span class="toc-text">1.4 RNN 计算图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-Sequence-to-Sequence-%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.5.</span> <span class="toc-text">1.5 Sequence to Sequence 模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81RNN-%E5%BA%94%E7%94%A8"><span class="toc-number">1.2.</span> <span class="toc-text">二、RNN 应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E8%AF%AD%E8%A8%80%E5%BB%BA%E6%A8%A1"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 语言建模</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%B2%BF%E6%97%B6%E9%97%B4%E7%9A%84%E6%88%AA%E6%96%AD%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B3%95"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 沿时间的截断反向传播法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-RNN-%E7%9A%84%E5%85%B6%E4%BB%96%E5%BA%94%E7%94%A8"><span class="toc-number">1.2.3.</span> <span class="toc-text">2.3 RNN 的其他应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-Image-Caption-%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.2.4.</span> <span class="toc-text">2.4 Image Caption 模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B%EF%BC%88Attention%EF%BC%89"><span class="toc-number">1.2.5.</span> <span class="toc-text">2.5 注意力模型（Attention）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-%E8%A7%86%E8%A7%89%E9%97%AE%E7%AD%94%EF%BC%88Visual-Question-Answering%EF%BC%89"><span class="toc-number">1.2.6.</span> <span class="toc-text">2.6 视觉问答（Visual Question Answering）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88LSTM%EF%BC%89"><span class="toc-number">1.3.</span> <span class="toc-text">三、长短时记忆神经网络（LSTM）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-LSTM-%E6%A6%82%E8%A6%81"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 LSTM 概要</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-LSTM-%E7%9A%84%E7%BB%86%E8%8A%82"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 LSTM 的细节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-LSTM-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.3 LSTM 反向传播</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.4.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/45e85deb.html" title="Nginx 基本使用"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2023/09/07/20230907192257-a360e8.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Nginx 基本使用"/></a><div class="content"><a class="title" href="/posts/45e85deb.html" title="Nginx 基本使用">Nginx 基本使用</a><time datetime="2023-09-07T11:19:06.000Z" title="发表于 2023-09-07 19:19:06">2023-09-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/8d037627.html" title="Git 及 GitHub 常用命令记录"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2023/02/27/20230227141226-160f64.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Git 及 GitHub 常用命令记录"/></a><div class="content"><a class="title" href="/posts/8d037627.html" title="Git 及 GitHub 常用命令记录">Git 及 GitHub 常用命令记录</a><time datetime="2023-02-27T06:08:31.000Z" title="发表于 2023-02-27 14:08:31">2023-02-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/a6d8a9c3.html" title="个人常用 Linux 命令"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2023/02/26/20230226105141-94929b.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="个人常用 Linux 命令"/></a><div class="content"><a class="title" href="/posts/a6d8a9c3.html" title="个人常用 Linux 命令">个人常用 Linux 命令</a><time datetime="2023-02-26T02:47:48.000Z" title="发表于 2023-02-26 10:47:48">2023-02-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/3b91aa4a.html" title="Linux 服务器环境配置"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/4c417afa91736428def17dcc96aa64b8.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Linux 服务器环境配置"/></a><div class="content"><a class="title" href="/posts/3b91aa4a.html" title="Linux 服务器环境配置">Linux 服务器环境配置</a><time datetime="2022-09-14T08:01:29.000Z" title="发表于 2022-09-14 16:01:29">2022-09-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/165b6b1f.html" title="论文整理（3）Measuring Compositional Consistency for Video Question Answering"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/top2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文整理（3）Measuring Compositional Consistency for Video Question Answering"/></a><div class="content"><a class="title" href="/posts/165b6b1f.html" title="论文整理（3）Measuring Compositional Consistency for Video Question Answering">论文整理（3）Measuring Compositional Consistency for Video Question Answering</a><time datetime="2022-09-03T07:28:11.000Z" title="发表于 2022-09-03 15:28:11">2022-09-03</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div id="footer_deal"><a class="deal_link" href="mailto:kunhaofu1@gmail.com" title="email"><i class="anzhiyufont anzhiyu-icon-envelope"></i></a><a class="deal_link" href="/atom.xml" title="RSS"><i class="anzhiyufont anzhiyu-icon-rss"></i></a><img class="footer_mini_logo" title="返回顶部" alt="返回顶部" onclick="anzhiyu.scrollToDest(0, 500)" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/head.jpg" size="50px"/><a class="deal_link" target="_blank" rel="noopener" href="https://github.com/kuhne12" title="Github"><i class="anzhiyufont anzhiyu-icon-github"></i></a><a class="deal_link" href="/copyright" title="CC"><i class="anzhiyufont anzhiyu-icon-copyright-line"></i></a></div><div id="anzhiyu-footer"><div class="footer-group"><div class="footer-title">服务</div><div class="footer-links"><a class="footer-item" title="等待开发" href="/wait/">等待开发</a></div></div><div class="footer-group"><div class="footer-title">主题</div><div class="footer-links"><a class="footer-item" title="文档" href="/docs/">文档</a></div></div><div class="footer-group"><div class="footer-title">导航</div><div class="footer-links"><a class="footer-item" title="等待开发" href="/wait/">等待开发</a></div></div><div class="footer-group"><div class="footer-title">协议</div><div class="footer-links"><a class="footer-item" title="隐私协议" href="/privacy/">隐私协议</a></div></div></div><div id="workboard"><img class="workSituationImg boardsign" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2023/09/07/20230907103822-7fe708.svg" alt="距离月入25k也就还差一个大佬带我~" title="距离月入25k也就还差一个大佬带我~"/><div id="runtimeTextTip"></div></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v5.4.0" title="博客框架为Hexo_v5.4.0"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.1.5/img/badge/Frame-Hexo.svg" alt="博客框架为Hexo_v5.4.0"/></a><a class="github-badge" target="_blank" href="https://blog.anheyu.com/" style="margin-inline:5px" data-title="本站使用AnZhiYu主题" title="本站使用AnZhiYu主题"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.9/img/Theme-AnZhiYu-2E67D3.svg" alt="本站使用AnZhiYu主题"/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title="本站项目由Github托管"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.1.5/img/badge/Source-Github.svg" alt="本站项目由Github托管"/></a><a class="github-badge" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.2.0/img/badge/Copyright-BY-NC-SA.svg" alt="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"/></a><a class="github-badge" target="_blank" href="https://beian.miit.gov.cn/" style="margin-inline:5px" data-title="本站已在浙备案" title="本站已在浙备案"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2023/09/07/20230907104640-663234.svg" alt="本站已在浙备案"/></a></p></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2020 - 2023 By <a class="footer-bar-link" href="/" title="Kuhne" target="_blank">Kuhne</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://beian.miit.gov.cn/" title="浙ICP备2023005409号-1">浙ICP备2023005409号-1</a><a class="footer-bar-link cc" href="/copyright" title="cc协议"><i class="anzhiyufont anzhiyu-icon-copyright-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-by-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-nc-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-nd-line"></i></a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">75</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">31</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">8</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" href="/null" title="暂无"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="暂无"/><span class="back-menu-item-text">暂无</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/?id=883682303&amp;server=netease&amp;type=playlist"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li></ul></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/essay/"><span> 闲言碎语</span></a></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/2022CVPR/" style="font-size: 0.88rem;">2022CVPR<sup>2</sup></a><a href="/tags/Git/" style="font-size: 0.88rem;">Git<sup>1</sup></a><a href="/tags/Kaggle/" style="font-size: 0.88rem;">Kaggle<sup>2</sup></a><a href="/tags/Linux/" style="font-size: 0.88rem;">Linux<sup>2</sup></a><a href="/tags/PyTorch/" style="font-size: 0.88rem;">PyTorch<sup>9</sup></a><a href="/tags/Python/" style="font-size: 0.88rem;">Python<sup>6</sup></a><a href="/tags/Pytorch/" style="font-size: 0.88rem;">Pytorch<sup>1</sup></a><a href="/tags/SpringCloud/" style="font-size: 0.88rem;">SpringCloud<sup>9</sup></a><a href="/tags/Vue/" style="font-size: 0.88rem;">Vue<sup>3</sup></a><a href="/tags/nginx/" style="font-size: 0.88rem;">nginx<sup>1</sup></a><a href="/tags/pycharm/" style="font-size: 0.88rem;">pycharm<sup>1</sup></a><a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 0.88rem; font-weight: 500; color: var(--anzhiyu-lighttext)">人工智能<sup>21</sup></a><a href="/tags/%E4%BD%9C%E4%B8%9A/" style="font-size: 0.88rem;">作业<sup>8</sup></a><a href="/tags/%E5%8A%A0%E5%AF%86/" style="font-size: 0.88rem;">加密<sup>1</sup></a><a href="/tags/%E5%90%8E%E7%AB%AF/" style="font-size: 0.88rem;">后端<sup>3</sup></a><a href="/tags/%E5%9F%BA%E7%A1%80/" style="font-size: 0.88rem;">基础<sup>12</sup></a><a href="/tags/%E5%B7%A5%E5%85%B7/" style="font-size: 0.88rem;">工具<sup>3</sup></a><a href="/tags/%E5%BD%92%E7%BA%B3/" style="font-size: 0.88rem;">归纳<sup>3</sup></a><a href="/tags/%E6%95%99%E7%A8%8B/" style="font-size: 0.88rem;">教程<sup>10</sup></a><a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 0.88rem;">服务器<sup>1</sup></a><a href="/tags/%E6%9C%AA%E5%AE%8C%E6%88%90/" style="font-size: 0.88rem;">未完成<sup>1</sup></a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">机器学习<sup>21</sup></a><a href="/tags/%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0/" style="font-size: 0.88rem;">模型复现<sup>8</sup></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem; font-weight: 500; color: var(--anzhiyu-lighttext)">深度学习<sup>15</sup></a><a href="/tags/%E7%BD%91%E9%A1%B5%E5%89%8D%E7%AB%AF/" style="font-size: 0.88rem;">网页前端<sup>4</sup></a><a href="/tags/%E8%80%83%E7%A0%94/" style="font-size: 0.88rem;">考研<sup>1</sup></a><a href="/tags/%E8%A7%86%E8%A7%89%E5%85%B3%E7%B3%BB%E6%8E%A2%E6%B5%8B/" style="font-size: 0.88rem;">视觉关系探测<sup>1</sup></a><a href="/tags/%E8%A7%86%E8%A7%89%E9%97%AE%E7%AD%94/" style="font-size: 0.88rem;">视觉问答<sup>1</sup></a><a href="/tags/%E8%A7%86%E9%A2%91%E9%97%AE%E7%AD%94/" style="font-size: 0.88rem;">视频问答<sup>1</sup></a><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" style="font-size: 0.88rem;">计算机视觉<sup>12</sup></a><a href="/tags/%E8%AE%BA%E6%96%87/" style="font-size: 0.88rem;">论文<sup>4</sup></a></div></div><hr/></div></div><div id="keyboard-tips"><div class="keyboardTitle">博客快捷键</div><div class="keybordList"><div class="keybordItem"><div class="keyGroup"><div class="key">shift K</div></div><div class="keyContent"><div class="content">关闭快捷键功能</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift A</div></div><div class="keyContent"><div class="content">打开/关闭中控台</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift M</div></div><div class="keyContent"><div class="content">播放/暂停音乐</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift D</div></div><div class="keyContent"><div class="content">深色/浅色显示模式</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift S</div></div><div class="keyContent"><div class="content">站内搜索</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift R</div></div><div class="keyContent"><div class="content">随机访问</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift H</div></div><div class="keyContent"><div class="content">返回首页</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift F</div></div><div class="keyContent"><div class="content">友链鱼塘</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift L</div></div><div class="keyContent"><div class="content">友链页面</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift P</div></div><div class="keyContent"><div class="content">关于本站</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift I</div></div><div class="keyContent"><div class="content">原版/本站右键菜单</div></div></div></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="anzhiyufont anzhiyu-icon-comments"></i></a><a id="switch-commentBarrage" href="javascript:anzhiyu.switchCommentBarrage();" title="开关弹幕"><i class="anzhiyufont anzhiyu-icon-danmu"></i></a><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="60198" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random"></meting-js></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="anzhiyufont anzhiyu-icon-xmark"></i></button></nav><div class="is-center" id="loading-database"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-pulse-icon"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://music.163.com/#/playlist&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div id="he-plugin-simple"></div><script>var WIDGET = {
  "CONFIG": {
    "modules": "0124",
    "background": "2",
    "tmpColor": "FFFFFF",
    "tmpSize": "16",
    "cityColor": "FFFFFF",
    "citySize": "16",
    "aqiColor": "E8D87B",
    "aqiSize": "16",
    "weatherIconSize": "24",
    "alertIconSize": "18",
    "padding": "10px 10px 10px 10px",
    "shadow": "0",
    "language": "auto",
    "borderRadius": "20",
    "fixed": "true",
    "vertical": "top",
    "horizontal": "left",
    "left": "20",
    "top": "7.1",
    "key": "df245676fb434a0691ead1c63341cd94"
  }
}
</script><link rel="stylesheet" href="https://widget.qweather.net/simple/static/css/he-simple.css?v=1.4.0"/><script src="https://widget.qweather.net/simple/static/js/he-simple.js?v=1.4.0"></script><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.4/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script async src="/anzhiyu/random.js"></script><script async="async">(function () {
  var grt = new Date("04/01/2021 00:00:00"); //设置网站上线时间
  var now = new Date();
  var dnum;
  var hnum;
  var mnum;
  var snum;
  var nowHour;

  // 计算并更新天数、小时数、分钟数和秒数
  function updateTime() {
    now = new Date(); // 更新 now 的值
    nowHour = now.getHours(); // 更新 nowHour 的值
    var days = (now - grt) / 1000 / 60 / 60 / 24;
    dnum = Math.floor(days);
    var hours = (now - grt) / 1000 / 60 / 60 - 24 * dnum;
    hnum = Math.floor(hours);
    if (String(hnum).length == 1) {
      hnum = "0" + hnum;
    }
    var minutes = (now - grt) / 1000 / 60 - 24 * 60 * dnum - 60 * hnum;
    mnum = Math.floor(minutes);
    if (String(mnum).length == 1) {
      mnum = "0" + mnum;
    }
    var seconds = (now - grt) / 1000 - 24 * 60 * 60 * dnum - 60 * 60 * hnum - 60 * mnum;
    snum = Math.round(seconds);
    if (String(snum).length == 1) {
      snum = "0" + snum;
    }
  }

  // 更新网页中显示的网站运行时间
  function updateHtml() {
    const footer = document.getElementById("footer");
    if (!footer) return
    let currentTimeHtml = "";
    if (nowHour < 18 && nowHour >= 9) {
      // 如果是上班时间，默认就是"安知鱼-上班摸鱼中.svg"图片，不需要更改
      currentTimeHtml = `本站居然运行了 ${dnum} 天<span id='runtime'> ${hnum} 小时 ${mnum} 分 ${snum} 秒 </span><i class='anzhiyufont anzhiyu-icon-heartbeat' style='color:red'></i>`;
    } else {
      // 如果是下班时间，插入"安知鱼-下班啦.svg"图片
      let img = document.querySelector("#workboard .workSituationImg");
      img.src = "https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2023/09/07/20230907103822-7fe708.svg";
      img.title = "下班了就该开开心心的玩耍，嘿嘿~";
      img.alt = "下班了就该开开心心的玩耍，嘿嘿~";

      currentTimeHtml = `本站居然运行了 ${dnum} 天<span id='runtime'> ${hnum} 小时 ${mnum} 分 ${snum} 秒 </span><i class='anzhiyufont anzhiyu-icon-heartbeat' style='color:red'></i>`;
    }

    if (document.getElementById("runtimeTextTip")) {
      document.getElementById("runtimeTextTip").innerHTML = currentTimeHtml;
    }
  }

  setInterval(() => {
    updateTime();
    updateHtml();
  }, 1000);
})();</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.cbd.int/katex@0.16.0/dist/katex.min.css"><script src="https://cdn.cbd.int/katex@0.16.0/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    anzhiyu.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://twikoo-api-pearl.vercel.app',
      region: '',
      onCommentLoaded: () => {
        anzhiyu.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') setTimeout(runFn,0)
    else getScript('https://cdn.cbd.int/twikoo@1.6.18/dist/twikoo.all.min.js').then(runFn)
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://twikoo-api-pearl.vercel.app',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const runFn = () => {
    init();
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) anzhiyu.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else {
      loadTwikoo()
    }
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script><input type="hidden" name="page-type" id="page-type" value="post"><script async src="/js/anzhiyu/comment_barrage.js"></script></div><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getComment = () => {
    const runTwikoo = () => {
      twikoo.getRecentComments({
        envId: 'https://twikoo-api-pearl.vercel.app',
        region: '',
        pageSize: 6,
        includeReply: true
      }).then(function (res) {
        const twikooArray = res.map(e => {
          return {
            'content': changeContent(e.comment),
            'avatar': e.avatar,
            'nick': e.nick,
            'url': e.url + '#' + e.id,
            'date': new Date(e.created).toISOString()
          }
        })

        saveToLocal.set('twikoo-newest-comments', JSON.stringify(twikooArray), 10/(60*24))
        generateHtml(twikooArray)
      }).catch(function (err) {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.textContent= "无法获取评论，请确认相关配置是否正确"
      })
    }

    if (typeof twikoo === 'object') {
      runTwikoo()
    } else {
      getScript('https://cdn.cbd.int/twikoo@1.6.18/dist/twikoo.all.min.js').then(runTwikoo)
    }
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'data-lazy-src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'><div class='name'><span>${array[i].nick} </span></div></a>`
        }
        
        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <time datetime="${array[i].date}">${anzhiyu.diffDate(array[i].date, true)}</time></div>
        </div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('twikoo-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script>var visitorMail = "visitor@anheyu.com";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><script src="/js/anzhiyu/right_click_menu.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["meta[property=\"og:image\"]","meta[property=\"og:title\"]","meta[property=\"og:url\"]","meta[property=\"og:type\"]","meta[property=\"og:site_name\"]","meta[property=\"og:description\"]","head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>
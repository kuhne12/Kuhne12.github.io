<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>PyTorch 学习笔记 | Kuhne</title><meta name="keywords" content="Python,PyTorch"><meta name="author" content="Kuhne"><meta name="copyright" content="Kuhne"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="PyTorch 学习笔记"><meta name="application-name" content="PyTorch 学习笔记"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="PyTorch 学习笔记"><meta property="og:url" content="https://kuhne.gitee.io/kuhne.gitee.io/posts/2609eb73.html"><meta property="og:site_name" content="Kuhne"><meta property="og:description" content="记录 Pytorch 的入门使用"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/1cc2cc32c428a61ce3eb2f191668eb9c.jpeg"><meta property="article:author" content="Kuhne"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/1cc2cc32c428a61ce3eb2f191668eb9c.jpeg"><meta name="description" content="记录 Pytorch 的入门使用"><link rel="shortcut icon" href="/img/head2.png"><link rel="canonical" href="https://kuhne.gitee.io/kuhne.gitee.io/posts/2609eb73"><link rel="preconnect" href="//npm.elemecdn.com"/><link rel="preconnect" href="//npm.onmicrosoft.cn"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/swiper/swiper.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: {"enable":true,"img":"https://upload-bbs.miyoushe.com/upload/2023/09/03/125766904/ee23df8517f3c3e3efc4145658269c06_5714860933110284659.png"},
  postHeadAiDescription: undefined,
  diytitle: undefined,
  LA51: {"enable":true,"ck":"3Fpd8lKaw5wGj1Fj","LingQueMonitorID":null},
  greetingBox: {"enable":true,"default":"晚上好👋","list":[{"greeting":"晚安😴","startTime":0,"endTime":5},{"greeting":"早上好鸭👋, 祝你一天好心情！","startTime":6,"endTime":9},{"greeting":"上午好👋, 状态很好，鼓励一下～","startTime":10,"endTime":10},{"greeting":"11点多啦, 在坚持一下就吃饭啦～","startTime":11,"endTime":11},{"greeting":"午安👋，中午要好好休息一下～","startTime":12,"endTime":14},{"greeting":"🌈充实的一天辛苦啦！","startTime":14,"endTime":18},{"greeting":"19点喽, 奖励一顿丰盛的大餐吧🍔。","startTime":19,"endTime":19},{"greeting":"晚上好👋, 在属于自己的时间好好放松😌~","startTime":20,"endTime":24}]},
  twikooEnvId: 'https://twikoo-api-pearl.vercel.app',
  commentBarrageConfig:{"enable":true,"maxBarrage":1,"barrageTime":4000,"accessToken":"","mailMd5":""},
  root: '/',
  preloader: {"source":3},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: {"mode":"both","api":"https://img2color-go.vercel.app/api?img=","cover_change":true},
  authorStatus: {"skills":["🐟 上班摸鱼佼佼者"]},
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: Kuhne","link":"链接: ","source":"来源: Kuhne","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: {"enable":true,"delay":100,"shiftDelay":200},
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'Kuhne',
  title: 'PyTorch 学习笔记',
  postAI: '',
  pageFillDescription: 'PyTorch 学习笔记, 一、PyTorch 介绍, 1.1 Why PyTorch ?, 1.2 Numpy or Torch?, 二、PyTorch 基础, 2.1 加载数据, 2.2 TensorBoard 使用, 2.3 TensorBoard add_image() 用法, 2.3 transform 的使用, 2.4 常见的 Transform, 2.4 torchvision 数据集使用, 2.5 DataLoader 的使用, 三、神经网络结构, 3.1 PyTorch 搭建神经网络介绍, 3.2 搭建自己的神经网络, 3.3 PyTorch 卷积操作, 3.4 卷积层, 3.5 池化层, 3.6 非线性激活函数, 3.7 批量归一化, 3.8 线性层, 3.9 Dropout 层, 3.10 Sequential, 四、神经网络简单搭建, 4.1 CIFAR-10 网络结构, 4.2 网络搭建, 五、损失函数、反向传播及优化, 5.1 损失函数, 5.2 反向传播, 5.3 优化器, 六、模型使用, 6.1 现有模型使用方法, 6.2 模型的保存与读取, 七、 完整的模型训练, 7.1 训练模型完整步骤, 7.2 使用 GPU 训练模型, 八、模型验证学习笔记一介绍是在上的衍生是一个使用语言的神经网络库但是不是特别流行所以的开发团队就将其移植到了更流行的语言上下面是一些使用的企业和机构搭建的计算图是动态的并不是首先搭建静态的图中再将数据放入静态图中去对比静态的他能更有效地处理一些问题比如说变化时间长度的输出自称是神经网络界的因为它能将生产的张量放在中加速运算就像会把放入中加速运算一样之前一直习惯于用进行编程也不必担心和能很好地兼容这样就能自由地转换和了下面是一段示例代码数据转为数据转为数组输出二基础加载数据在中读取数据主要涉及个类分别是用于获取数据及其对应获取每一个数据及其告知总共有多少数据为网络提供不同的数据形式下面是一个创建数据读取对象的例子代码创建数据集类初始化创建实例时会运行该函数主要内容是为提供全局变量获得所有图片的路径所有子类都必须重写方法用于获取数据样本并给与对应的标签所有子类也必须重写方法返回数据集长度创建数据集实例拼接数据集图片选择图片展示结果本次使用的数据集共有两个分别是和每一张图片都对应一个同名的文件文件内容是对应文件名图像的使用是中用于数据可视化的包它的存在对于模型训练很有帮助可以可视化模型在不同阶段的状态首先需要导入相应的包学习一个包的第一步一般是查看相应的在中可以直接进入类中寻找帮助信息了解到该类会向文件夹写入事件文件该文件可以被解析查看初始化方法中的介绍内容这里面介绍了初始化该类的参数信息参数是一个内容是用于保存事件文件夹的路径其他的参数用的不多下面是一个具体的例子初始化实例传入事件文件夹路径添加标量参数指定图像名称参数指定参数指定这里相当于绘制了一个的图像这里报错了需要下载代码报错需要下载运行成功后目录中会新增一个文件夹可以在中的中打开生成的文件输入代码事件文件夹名随后打开生成的网址就可以成功启动了为了避免端口冲突可以在命令后面加上端口信息现在需要打开这个网址打开后就得到了对应的图像在中可以对数据图像进行缩放坐标轴变换鼠标移动到图像上会显示坐标位置同理也可以绘制的函数图像初始化实例传入事件文件夹路径添加标量运行成功后刷新网页得到这样的结果如果没有修改图像名称只修改了的信息就会出现错误初始化实例传入事件文件夹路径添加标量注意这里是出现这种情况就需要将事件文件夹进行删除随后重新绘制图像用法与相同使用之前我们需要进入该方法查看它的使用介绍这里指出了使用该方法需要的参数列表之前使用的类生成的图像类型并不满足要求必须是类型所以必须换一种图像格式比如使用类型同时也指定了输入数据的只有满足了指定的才能成功运行代码编写初始化实例传入事件文件夹路径图像路径获取图像转化为格式添加图像参数名称参数图像参数运行后会发现抱错原因是不匹配格式的图片的是但是默认的是如果需要指定成需要修改属性为修改后代码如下修改后就能成功运行程序运行结果如果需要添加图片仅需修改图片路径和对应代码这里改成了输出结果由于未修改图像名称是在同一片区域下现实的所以需要拖动上方的滑块如果希望在不同区域显示则需要修改标题名称输出结果通过这种方式可以很直观地了解到我们给模型的数据的使用中的包主要是用来预处理图片导入方式法为我们可以进入该包可以看到包内定义了很多类比如类作用是对图片做中心裁剪随后返回一个对应的但是最常用的类是类该类可以将或转化为代码编写类型实例化对象相当于调用了的方法类型查看的内容里面封装了一些反向传播所需要的数据有了格式的数据就可以直接使用方法绘制图像了代码编写类型实例化对象输出结果常见的的功能已经在上方解释过了不再赘述代码编写的作用是归一化具体过程是将减去均值再除以标准差输入均值标准差传入的是数组有几个通道就传几个元素代码编写传入的两个数组分别是均值和标准差传入运行结果的作用是将输入的图像转化为指定的参数为时会将图像转为改大小若参数为一个整数则会让图片的最小边匹配该数值进行等比缩放代码编写成的图像运行结果以上是使用的模块进行图片读取但常用的方式是使用安装代码使用读取图片会转换为格式的作用是将不同的组合在一起比如先将图片进行随后将图片进行代码编写数据的变化过程是新版的的可以接受输入了所以可以直接用进行运行结果可以对图片进行随机裁剪可以指定裁剪的尺寸利用循环可以进行多次裁剪代码编写该代码对目标图像进行了次随机裁剪每次裁剪的尺寸是运行结果数据集使用在的官方网站中提供了很多包括关于文本图像音频等等的文档在这些文档里都能找到标准的数据集供模型训练这一节将结合和学习如何对数据集进行处理的数据集参数都比较相近首先来了解一下本次使用的数据集数据集包括了张的图像总共有个类别其中张是训练图片张是验证图片必须设置指定数据集位置为表示该数据集是训练集为表示该数据集为测试集数据集的预处理对指定的进行为则会自动下载数据集接下来将使用该数据集进行一系列的数据处理代码编写创建训练集自动下载创建测试集输出输出包含了两部分分别是和图片文件图片对应的序号我们也可以通过属性输出对应的名称输出可以看到我们得到的图片数据都是类型的但是在中最好还是使用类型的数据所以就需要做一些变换对数据进行将应用于训练集将应用于测试集输出转化为格式后就能将图像放入进行可视化了代码如下运行结果的使用是将数据读入而的作用是将的数据加载到神经网络中就相当于是一副牌堆而就相当于是一张张的手牌每次都需要从牌堆中取牌而取牌的方法就是由控制的中有很多参数但是其中有很多参数是已经有默认值的需要加载的数据集相当于一整副扑克每次取数据的数量相当于次摸得牌数是否打乱数据相当于洗牌设置多进程取不尽时是否舍去剩余数据接下来看看具体代码代码编写取测试集设置这里设置了表示一次会取个数据表示每次取完数据后都会打乱数据接下来使用取出数据输出张图片通道尺寸为四张图片对应的有了数据后就可以通过进行可视化了但是这次不是单独的数据而是多个数据打包在一起所以代码和之前有些许区别代码编写取测试集每次取张设置全局路径每次添加完图片后增加输出结果可以看到每个有张图片但是最后一步并没有张这是因为数据集不能被整除由于设置了剩下的图像被合并成最后一步由于设置了所以每个取得图片顺序都不会相同三神经网络结构搭建神经网络介绍中关于神经网络的文档都可以在官网的模块中找到包括骨架池化层非线性激活函数等等都可以在官网中找到其中中包含六个类最常用的是类它为所有神经网络提供了最基本的骨架神经网络必须继承这个类官网给出的案例每次都必须重写该方法这个首先是继承了类进行了初始化随后进行前向传播前向传播的过程是输入卷积卷积输出搭建自己的神经网络了解了搭建神经网络的方法接下来就开始搭建一个最简单的神经网络代码编写创建自己的网络类前向传播让输入加实例化对象前向传播因为方法的实现中调用了所以会直接执行输出卷积操作在官网中可以找到有关卷积层操作的文档地址其中表示数据是一维数据表示数据是二维的比如图像就是二维数据有关神经网络的有两个分别是和其中是对的封装是更底层的代码在使用卷积层操作之前先来了解一下这个中的模块和的模块是相同的首先来看看模块它包含了以下参数其中就是卷积核是偏置项是卷积核移动的步长是为卷积结果进行零填充的层数下面是一个具体例子代码编写创建二维矩阵创建卷积核输出卷积核步长为输出卷积核步长为输出卷积核步长为零填充层运行结果输出输出输出卷积层中的卷积层主要使用的类首先来看看官方文档文档指出了使用该类所需要的参数包括输入通道数彩色图片一般为通道必须指定输出通道数设置为几就有几个卷积核必须指定卷积核大小整型或者元组必须指定卷积核步长一般需要自己指定填充层数一般需要自己指定卷积核元素间隔一般不常用一般设置为分组卷积时进行修改基本遇不到是否启用偏置项一般为填充模式比如填充使用设备数据类型下面是一个实例代码编写获取数据集设置创建自己的卷积神经网络创建卷积层重写前向传播实例化网络为了方便理解在中进行可视化遍历进行卷积由于卷积后尺寸并不匹配需要进行设置会让框架进行自动计算输出结果池化层池化层文档中的池化层包含很多类比如下采样上采样平均池化但是我们最常用的还是接下来看看对应的文档它的参数有池化核大小同卷积核大小必须设置池化核移动步长默认值是填充层数卷积核元素间隔一般不设置用的很少不做了解设置为时使用模式丢弃多余数据设置为时使用模式保留多余数据如下图所示下面是一个具体例子代码编写创建输入把输入成指定定义网络网络初始化重写前向传播网络实例化前向传播输出如果设置输出为为了清楚池化层的作用我们可以通过进行可视化代码编写运行结果可以看到经过池化层后图片仍然保存了本身的特征而且体积被大大缩小了非线性激活函数官方文档在神经网络中非线性激活函数的作用是为网络引入非线性的特质其中最常用的是代码编写若为会返回一个新的变量若为则会修改原来的变量输出同样的为了显示非线性函数的效果可以将处理过的数据进行可视化代码编写这里使用函数因为用函数处理的图像区别看起来更明显输出结果批量归一化官方文档使用批量归一化可以解决梯度消失和梯度爆炸的问题常用的是类官方文档内容如下参数输入的通道数中的其他参数一半使用默认值线性层官方文档线性层即神经网络中的全连接层包含大量的参数一般放在网络的最后方如下图所示除了每一层的元素都是上层元素的加权和在中计算线性层非常方便首先来看看官方文档确认这一层的参数参数输入的特征数输出的特征数是否添加偏置项使用设备指定数据类型下面是一个例子代码编写线性层输入维度时输出维度是将图像展平全连接层前向传播层官方文档层的主要作用是在模型训练阶段随机地将输入张量的某些元素以概率归零是卷积神经网络中防止过拟合的一种方式官方文档使用可以将网络结构组合起来按照我们设定的顺序一步步进行前向传播使代码更加简洁易懂更加方便管理下面是一个官方示例四神经网络简单搭建了解了中神经网络各层的代码编写规则接下来就可以搭建自己的卷积神经网络了本节将实现使用卷积神经网络和数据集完成图像分类的功能网络结构想要实现一个卷积神经网络首先需要设计网络的结构这里我们不自己设计网络结构而是使用别人已经设计好的结构来实现分类功能具体的网络结构如下图所示网络的结构是输入卷积层最大池化层卷积层最大池化层卷积层池化展开成一维全连接层输出可以发现网络结构十分简单也没有引入非线性函数接下来我们就根据这个结构来训练一个图像分类器网络搭建代码编写为了使输出尺寸保持不变需要计算和计算结果为重写前向传播网络结构检验输出卷积层计算公式可以看到这样编写代码会让前向传播显得十分冗余我们可以使用来简化代码代码编写使用整合网络结构前向传播代码三行就搞定了网络结构检验输出通过也可以将我们的卷积神经网络结构进行可视化代码编写接上方代码可视化网络结构计算图运行结果可以看到可视化后的网络结构十分清晰我们还可以进一步细化进入每一层的内部观察数据的五损失函数反向传播及优化损失函数官方文档损失函数是衡量模型预测结果与实际数据差距的数值包括等等不同的损失函数在官方文档都有很详细的解释下面来看一个例子代码编写输出目标损失值输出和目标输出上一节我们已经编写了一个用于分类数据集的神经网络模型现在我们就可以来看看这个模型在数据集上的损失函数代码编写实例化损失计算计算损失运行结果运行的结果就是网络输出与目标之间的差距有了这些差距就为我们提供了参数更新的依据反向传播反向传播为计算图中的每一个节点提供了一个参数用于保存该节点的梯度有了就有了反向传播的依据我们只需要一行代码就能实现梯度的自动计算在运行这行代码之前节点的梯度值都是运行完该段代码后会自动计算出来优化器官方文档为了实现参数更新我们需要使用到中的优化器在官方文档中有详细的介绍且提供了构造优化器的案例在上节中我们已经介绍了如何计算节点的梯度有了梯度后就可以调用来进行参数更新下面来看一个例子代码编写构建优化器初始化一个的先将梯度归零反向传播计算梯度参数更新将一批数据的加到整个的上输出结果可以看到每一轮的整体都在减小模型正在不断拟合数据六模型使用现有模型使用方法中已经提供了许多已经训练好的模型本节将介绍如何使用这些模型和如何对模型做一些自定义的修改官方文档模型的类型包括分类语义分割目标检测实例分割和人物关键点检测视频分类本节将使用提供的分类模型来实现图片数据集分类的功能代码编写的训练集未训练好的已训练好的运行结果输出结果很清晰的展示了的网络结构大体上分为三个部分特征处理平均池化分类器观察最后一层可以看到输出是维的向量而数据库只有类数据我们可以想办法修改这个已有的模型方法有将最后的输出层元素数量由改为代码编写输出结果在最后一层的后方在添加一层网络代码编写在后添加一层网络去掉也可也在后方加上一层只不过是在之外输出结果模型的保存与读取模型保存保存方式方式这种方式不仅保存了网络还保存了网络中的参数这种方式存在陷阱保存后在使用之前必须引入对应的包保存方式方式仅保存网络参数不保存模型官方推荐模型读取读取方式方式保存方式对应的加载模型不仅读取了模型还读取了网络参数输出网络结构读取方式方式保存方式对应的模型加载为模型载入参数输出字典七完整的模型训练训练模型完整步骤准备数据集训练集测试集训练集长度测试集长度加载数据集编写网络模型新起一个文件专门保存模型代码测试网络正确性通过测试后就可以将该模型引入其他文件实例化网络模型损失函数优化器创建网络模型创建损失函数定义优化器定义模型训练训练次数第次训练开始开始训练前向传播损失计算梯度清零反向传播更新参数训练次数模型及保存测试测试步骤仅需要进行测试不需要调整梯度最好设置为整体整体保存模型方式模型已保存输出结果第次训练开始训练次数训练次数训练次数训练次数训练次数训练次数整体整体模型已保存在读别人的代码的时候在训练之前有时候会加上让模型进入训练状态在验证之前会加上让模型进入验证状态这并不代表加上这些代码才可以运行程序而是说如果模型中有些一些特殊的层时需要添加上如果没有即便不添加上这两个代码也可以运行使用训练模型可以使用的进行模型训练分为两种方式方式找到模型以下三个部分网络模型数据损失函数调用对应对象的方法即可实现在上进行训练完整代码准备数据集训练集长度测试集长度加载数据集创建网络模型网络模型使用计算无法使用创建损失函数损失函数使用计算定义优化器定义训练模型第次训练开始开始训练训练数据使用计算前向传播损失计算梯度清零反向传播更新参数运行时间训练次数测试步骤测试数据使用计算整体整体保存模型方式模型已保存方式在程序的一开始用方法定义设备随后在需要加速的地方调用即可实现使用加速定义设备处可以使用语法糖方式编写比如完整代码准备数据集定义训练的设备训练集长度测试集长度加载数据集创建网络模型网络模型使用计算无法使用创建损失函数损失函数使用计算定义优化器定义训练模型第次训练开始开始训练训练数据使用计算前向传播损失计算梯度清零反向传播更新参数运行时间训练次数测试步骤测试数据使用计算整体整体保存模型方式模型已保存八模型验证模型训练并保存完成后接下来就需要进行进一步的验证了我们可以载入保存好的模型并在网络上查找图片让模型对图片进行分类具体代码如下图片代码编写载入模型从映射到上图片处理输入图片是需要一个的所以需要一下这一步可以提高性能预测类别运行结果预测类别',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2023-09-06 11:41:02',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Kuhne" type="application/atom+xml">
</head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="https://kuhne.gitee.io/kuhne.gitee.io/img/head.jpg"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><div class="back-home-button"><i class="anzhiyufont anzhiyu-icon-grip-vertical"></i><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" href="/null" title="暂无"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="暂无"/><span class="back-menu-item-text">暂无</span></a></div></div></div></div><a id="site-name" href="/" accesskey="h"><div class="title">Kuhne</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/?id=883682303&amp;server=netease&amp;type=playlist"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li></ul></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/essay/"><span> 闲言碎语</span></a></div></div></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><div class="nav-button" id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" title="搜索🔍" accesskey="s"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span> 搜索</span></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" alt="微信" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" alt="支付宝" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/2022CVPR/" style="font-size: 1.05rem;">2022CVPR<sup>2</sup></a><a href="/tags/Git/" style="font-size: 1.05rem;">Git<sup>1</sup></a><a href="/tags/Kaggle/" style="font-size: 1.05rem;">Kaggle<sup>2</sup></a><a href="/tags/Linux/" style="font-size: 1.05rem;">Linux<sup>2</sup></a><a href="/tags/PyTorch/" style="font-size: 1.05rem;">PyTorch<sup>9</sup></a><a href="/tags/Python/" style="font-size: 1.05rem;">Python<sup>6</sup></a><a href="/tags/Pytorch/" style="font-size: 1.05rem;">Pytorch<sup>1</sup></a><a href="/tags/SpringCloud/" style="font-size: 1.05rem;">SpringCloud<sup>9</sup></a><a href="/tags/Vue/" style="font-size: 1.05rem;">Vue<sup>3</sup></a><a href="/tags/pycharm/" style="font-size: 1.05rem;">pycharm<sup>1</sup></a><a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 1.05rem; font-weight: 500; color: var(--anzhiyu-lighttext)">人工智能<sup>21</sup></a><a href="/tags/%E4%BD%9C%E4%B8%9A/" style="font-size: 1.05rem;">作业<sup>8</sup></a><a href="/tags/%E5%8A%A0%E5%AF%86/" style="font-size: 1.05rem;">加密<sup>1</sup></a><a href="/tags/%E5%90%8E%E7%AB%AF/" style="font-size: 1.05rem;">后端<sup>2</sup></a><a href="/tags/%E5%9F%BA%E7%A1%80/" style="font-size: 1.05rem;">基础<sup>12</sup></a><a href="/tags/%E5%B7%A5%E5%85%B7/" style="font-size: 1.05rem;">工具<sup>3</sup></a><a href="/tags/%E5%BD%92%E7%BA%B3/" style="font-size: 1.05rem;">归纳<sup>3</sup></a><a href="/tags/%E6%95%99%E7%A8%8B/" style="font-size: 1.05rem;">教程<sup>10</sup></a><a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 1.05rem;">服务器<sup>1</sup></a><a href="/tags/%E6%9C%AA%E5%AE%8C%E6%88%90/" style="font-size: 1.05rem;">未完成<sup>1</sup></a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">机器学习<sup>21</sup></a><a href="/tags/%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0/" style="font-size: 1.05rem;">模型复现<sup>8</sup></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem; font-weight: 500; color: var(--anzhiyu-lighttext)">深度学习<sup>15</sup></a><a href="/tags/%E7%BD%91%E9%A1%B5%E5%89%8D%E7%AB%AF/" style="font-size: 1.05rem;">网页前端<sup>4</sup></a><a href="/tags/%E8%80%83%E7%A0%94/" style="font-size: 1.05rem;">考研<sup>1</sup></a><a href="/tags/%E8%A7%86%E8%A7%89%E5%85%B3%E7%B3%BB%E6%8E%A2%E6%B5%8B/" style="font-size: 1.05rem;">视觉关系探测<sup>1</sup></a><a href="/tags/%E8%A7%86%E8%A7%89%E9%97%AE%E7%AD%94/" style="font-size: 1.05rem;">视觉问答<sup>1</sup></a><a href="/tags/%E8%A7%86%E9%A2%91%E9%97%AE%E7%AD%94/" style="font-size: 1.05rem;">视频问答<sup>1</sup></a><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" style="font-size: 1.05rem;">计算机视觉<sup>12</sup></a><a href="/tags/%E8%AE%BA%E6%96%87/" style="font-size: 1.05rem;">论文<sup>4</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="anzhiyufont anzhiyu-icon-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/02/"><span class="card-archive-list-date">二月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/09/"><span class="card-archive-list-date">九月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/08/"><span class="card-archive-list-date">八月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">12</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/07/"><span class="card-archive-list-date">七月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">11</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/06/"><span class="card-archive-list-date">六月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">8</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/05/"><span class="card-archive-list-date">五月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">16</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/04/"><span class="card-archive-list-date">四月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">8</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/03/"><span class="card-archive-list-date">三月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item on" id="consoleCommentBarrage" onclick="anzhiyu.switchCommentBarrage()" title="热评开关"><a class="commentBarrage"><i class="anzhiyufont anzhiyu-icon-message"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div><div class="console-btn-item" id="consoleKeyboard" onclick="anzhiyu.keyboardToggle()" title="快捷键开关"><a class="keyboard-switch"><i class="anzhiyufont anzhiyu-icon-keyboard"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="post-meta-categories"><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/categories/Python/" itemprop="url">Python</a></span><span class="article-meta tags"><a class="article-meta__tags" href="/tags/Python/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Python</span></a><a class="article-meta__tags" href="/tags/PyTorch/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>PyTorch</span></a></span></div></div><h1 class="post-title" itemprop="name headline">PyTorch 学习笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2022-07-07T06:31:48.000Z" title="发表于 2022-07-07 14:31:48">2022-07-07</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2023-09-06T03:41:02.163Z" title="更新于 2023-09-06 11:41:02">2023-09-06</time></span></div><div class="meta-secondline"><span class="post-meta-separator"></span><span class="post-meta-wordcount"><i class="anzhiyufont anzhiyu-icon-file-word post-meta-icon" title="文章字数"></i><span class="post-meta-label" title="文章字数">字数总计:</span><span class="word-count" title="文章字数">11.5k</span><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-clock post-meta-icon" title="阅读时长"></i><span class="post-meta-label" title="阅读时长">阅读时长:</span><span>50分钟</span></span><span class="post-meta-separator"></span><span class="post-meta-pv-cv" id="" data-flag-title="PyTorch 学习笔记"><i class="anzhiyufont anzhiyu-icon-fw-eye post-meta-icon"></i><span class="post-meta-label" title="阅读量">阅读量:</span><span id="busuanzi_value_page_pv"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-spin"></i></span></span><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为杭州"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>杭州</span><span class="post-meta-separator"></span><span class="post-meta-commentcount"><i class="anzhiyufont anzhiyu-icon-comments post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/posts/2609eb73.html#post-comment" tabindex="-1"><span id="twikoo-count"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-spin"></i></span></a></span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/1cc2cc32c428a61ce3eb2f191668eb9c.jpeg"></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="https://kuhne.gitee.io/kuhne.gitee.io/posts/2609eb73.html"><header><a class="post-meta-categories" href="/categories/Python/" itemprop="url">Python</a><a href="/tags/Python/" tabindex="-1" itemprop="url">Python</a><a href="/tags/PyTorch/" tabindex="-1" itemprop="url">PyTorch</a><h1 id="CrawlerTitle" itemprop="name headline">PyTorch 学习笔记</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">Kuhne</span><time itemprop="dateCreated datePublished" datetime="2022-07-07T06:31:48.000Z" title="发表于 2022-07-07 14:31:48">2022-07-07</time><time itemprop="dateCreated datePublished" datetime="2023-09-06T03:41:02.163Z" title="更新于 2023-09-06 11:41:02">2023-09-06</time></header><h1>PyTorch 学习笔记</h1>
<h2 id="一、PyTorch-介绍">一、PyTorch 介绍</h2>
<h3 id="1-1-Why-PyTorch">1.1 Why PyTorch ?</h3>
<p>PyTorch 是 Torch 在 Python 上的衍生，Torch 是一个使用 Lua 语言的神经网络库，但是 Lua 不是特别流行，所以 Touch 的开发团队就将其移植到了更流行的 Python 语言上。</p>
<p>下面是一些使用 PyTorch 的企业和机构：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/06/28/191019-1019b.png" alt=""></p>
<p>PyTorch 搭建的计算图是动态的，并不是首先搭建静态的图中，再将数据放入静态图中去。PyTorch 对比静态的 Tensorflow，他能更有效地处理一些问题， 比如说 RNN 变化时间长度的输出。</p>
<h3 id="1-2-Numpy-or-Torch">1.2 Numpy or Torch?</h3>
<p>Torch 自称是神经网络界的 Numpy，因为它能将 torch 生产的 tensor （张量），放在 GPU 中加速运算，就像 Numpy 会把 array 放入 CPU 中加速运算一样。</p>
<p>之前一直习惯于用 Numpy 进行编程也不必担心，PyTorch 和 Numpy 能很好地兼容，这样就能自由地转换 numpy array 和 torch tensor 了。</p>
<p>下面是一段示例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np_data = np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="comment"># numpy 数据转为 tensor</span></span><br><span class="line">torch_data = torch.from_numpy(np_data)</span><br><span class="line"><span class="comment"># tensor 数据转为 numpy 数组</span></span><br><span class="line">tensor2array = torch_data.numpy()</span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nnumpy&#x27;</span>, np_data,</span><br><span class="line">    <span class="string">&#x27;\ntorch&#x27;</span>, torch_data,</span><br><span class="line">    <span class="string">&#x27;\nnumpy&#x27;</span>, tensor2array</span><br><span class="line"></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">numpy [[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span> <span class="number">5</span>]] </span><br><span class="line">torch tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]], dtype=torch.int32) </span><br><span class="line">numpy [[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span> <span class="number">5</span>]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="二、PyTorch-基础">二、PyTorch 基础</h2>
<h3 id="2-1-加载数据">2.1 加载数据</h3>
<p>在 PyTorch 中，读取数据主要涉及 2 个类，分别是：</p>
<ul>
<li><strong>Dataset</strong>
<ul>
<li>用于获取数据及其对应 label
<ul>
<li>获取每一个数据及其 label</li>
<li>告知总共有多少数据</li>
</ul>
</li>
</ul>
</li>
<li><strong>Dataloader</strong>
<ul>
<li>为网络提供不同的数据形式</li>
</ul>
</li>
</ul>
<p>下面是一个创建数据读取对象的例子：</p>
<ul>
<li>
<p>代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyData</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="comment"># 初始化，创建实例时会运行该函数</span></span><br><span class="line">    <span class="comment"># 主要内容是为 class 提供全局变量</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, label_dir</span>):</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.label_dir = label_dir</span><br><span class="line">        self.path = os.path.join(self.root_dir, self.label_dir)</span><br><span class="line">        self.img_path = os.listdir(self.path)  <span class="comment"># 获得所有图片的路径</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 所有子类都必须重写 _getitem__() 方法，用于获取数据样本并给与对应的标签</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img_name = self.img_path[idx]</span><br><span class="line">        img_item_path = os.path.join(self.root_dir, self.label_dir, img_name)</span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_item_path)</span><br><span class="line">        label = self.label_dir</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 所有子类也必须重写 __len__() 方法，返回数据集长度</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root_dir = <span class="string">&quot;Dataset\\train&quot;</span></span><br><span class="line">ants_label = <span class="string">&quot;ants&quot;</span></span><br><span class="line">bees_label = <span class="string">&quot;bees&quot;</span></span><br><span class="line"><span class="comment"># 创建数据集实例</span></span><br><span class="line">ants_dataset = MyData(root_dir, ants_label)</span><br><span class="line">bees_dataset = MyData(root_dir, bees_label)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拼接数据集</span></span><br><span class="line">train_dataset = ants_dataset + bees_dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图片选择</span></span><br><span class="line">img, <span class="built_in">len</span> = bees_dataset[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 图片展示</span></span><br><span class="line">img.show()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>结果</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/02/203638-49a0b.png" alt=""></p>
</li>
</ul>
<p>本次使用的数据集共有两个 <code>label</code>，分别是 <code>ants</code> 和 <code>bees</code>，每一张图片都对应一个同名的 <code>txt</code> 文件，<code>txt</code> 文件内容是对应文件名图像的 <code>label </code>。</p>
<h3 id="2-2-TensorBoard-使用">2.2 TensorBoard 使用</h3>
<p>TensorBoard 是 PyTorch 中用于数据可视化的包，它的存在对于模型训练很有帮助，可以可视化模型在不同阶段的状态。</p>
<p>首先需要导入相应的包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br></pre></td></tr></table></figure>
<p>学习一个包的第一步一般是查看相应的 <code>help()</code> ，在 PyCharm 中可以直接进入类中寻找帮助信息：</p>
<blockquote>
<p>“”&quot;</p>
<p>Writes entries directly to event files in the log_dir to be consumed by TensorBoard.</p>
<p>The <code>SummaryWriter</code> class provides a high-level API to create an event file in a given directory and add summaries and events to it. The class updates the file contents asynchronously. This allows a training program to call methods to add data to the file directly from the training loop, without slowing down training.</p>
<p>“”&quot;</p>
</blockquote>
<p>了解到该类会向 <code>log_dir</code> 文件夹写入<strong>事件文件</strong>，该文件可以被 TensorBoard 解析。查看初始化方法中的介绍内容：</p>
<blockquote>
<p>“”&quot;Creates a <code>SummaryWriter</code> that will write out events and summaries<br>
to the event file.</p>
<p>Args:<br>
<strong>log_dir (string):</strong> Save directory location. Default is<br>
runs/CURRENT_DATETIME_HOSTNAME, which changes after each run.<br>
Use hierarchical folder structure to compare<br>
between runs easily. e.g. pass in ‘runs/exp1’, ‘runs/exp2’, etc.<br>
for each new experiment to compare across them.<br>
<strong>comment (string):</strong> Comment log_dir suffix appended to the default<br>
<code>log_dir</code>. If <code>log_dir</code> is assigned, this argument has no effect.<br>
······</p>
<p>“”&quot;</p>
</blockquote>
<p>这里面介绍了初始化该类的参数信息，<code>log_dir</code> 参数是一个 String ，内容是用于保存事件文件夹的路径，其他的参数用的不多。</p>
<p>下面是一个具体的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化实例，传入事件文件夹路径</span></span><br><span class="line">writter = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加标量</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="comment"># 参数1：指定图像名称</span></span><br><span class="line">    <span class="comment"># 参数2：指定 x</span></span><br><span class="line">    <span class="comment"># 参数3：指定 y</span></span><br><span class="line">    <span class="comment"># 这里相当于绘制了一个 y=x 的图像</span></span><br><span class="line">    writter.add_scalar(<span class="string">&quot;y=x&quot;</span>, i, i)</span><br><span class="line"></span><br><span class="line">writter.close()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里报错了需要下载 tensorboard，代码 <code>pip install tensorboard</code></p>
<p>version 报错需要下载 <code>pip install setuptools==59.5.0</code></p>
</blockquote>
<p>运行成功后，目录中会新增一个 <code>log</code> 文件夹，可以在 PyCharm 中的 Terminal 中打开生成的文件，输入代码：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=logs	# logdir=事件文件夹名</span><br></pre></td></tr></table></figure>
<p>随后打开生成的<a target="_blank" rel="noopener" href="http://localhost:6006/#scalars">网址</a>，就可以成功启动 TensorBoard 了</p>
<p>为了避免端口冲突，可以在命令后面加上端口信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=logs --port=6007</span><br></pre></td></tr></table></figure>
<p>现在需要打开这个网址：<a target="_blank" rel="noopener" href="http://localhost:6007/%EF%BC%8C%E6%89%93%E5%BC%80%E5%90%8E%E5%B0%B1%E5%BE%97%E5%88%B0%E4%BA%86%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%BE%E5%83%8F">http://localhost:6007/，打开后就得到了对应的图像</a></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/111650-c2031.png" alt=""></p>
<p>在 TensorBoard 中，可以对数据图像进行缩放、坐标轴变换，鼠标移动到图像上会显示坐标位置。</p>
<p>同理，也可以绘制 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mn>2</mn><mi>x</mi></mrow><annotation encoding="application/x-tex">y=2x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span><span class="mord mathnormal">x</span></span></span></span> 的函数图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化实例，传入事件文件夹路径</span></span><br><span class="line">writter = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加标量</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writter.add_scalar(<span class="string">&quot;y=2x&quot;</span>, <span class="number">2</span>*i, i)</span><br><span class="line"></span><br><span class="line">writter.close()</span><br></pre></td></tr></table></figure>
<p>运行成功后刷新网页，得到这样的结果</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/114128-ddf85.png" style="zoom:67%;" />
<p>如果没有修改图像名称，只修改了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span> 的信息，就会出现错误</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化实例，传入事件文件夹路径</span></span><br><span class="line">writter = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加标量</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="comment"># 注意这里是 y=x</span></span><br><span class="line">    writter.add_scalar(<span class="string">&quot;y=x&quot;</span>, <span class="number">2</span>*i, i)</span><br><span class="line"></span><br><span class="line">writter.close()</span><br></pre></td></tr></table></figure>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/114850-ba1fa.png" style="zoom:67%;" />
<p>出现这种情况就需要将事件文件夹进行删除，随后重新绘制图像</p>
<h3 id="2-3-TensorBoard-add-image-用法">2.3 TensorBoard add_image() 用法</h3>
<p>与 <code>writter.add_scalar()</code> 相同，使用 <code>writter.add_image()</code> 之前，我们需要进入该方法查看它的使用介绍：</p>
<blockquote>
<p>“”&quot;Add image data to summary.</p>
<p>Note that this requires the <code>pillow</code> package.</p>
<p>Args:<br>
<strong>tag (string):</strong> Data identifier<br>
<strong>img_tensor (torch.Tensor, numpy.array, or string/blobname):</strong> Image data<br>
<strong>global_step (int):</strong> Global step value to record<br>
<strong>walltime (float):</strong> Optional override default walltime (time.time())<br>
seconds after epoch of event<br>
Shape:<br>
img_tensor: Default is :math:<code>(3, H, W)</code>. You can use <code>torchvision.utils.make_grid()</code> to<br>
convert a batch of tensor into 3xHxW format or call <code>add_images</code> and let us do the job.<br>
Tensor with :math:<code>(1, H, W)</code>, :math:<code>(H, W)</code>, :math:<code>(H, W, 3)</code> is also suitable as long as<br>
corresponding <code>dataformats</code> argument is passed, e.g. <code>CHW</code>, <code>HWC</code>, <code>HW</code>.</p>
<p>“”&quot;</p>
</blockquote>
<p>这里指出了使用该方法需要的参数列表，之前使用的 <code>Image </code> 类生成的图像类型并不满足要求（必须是<code>torch.Tensor</code>, <code>numpy.array</code>, or <code>string/blobname</code> 类型），所以必须换一种图像格式，比如使用 <code>numpy.array</code>类型，同时也指定了输入数据的 shape，只有满足了指定的 shape 才能成功运行。-</p>
<ul>
<li>
<p>代码编写：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化实例，传入事件文件夹路径</span></span><br><span class="line">writter = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"><span class="comment"># 图像路径</span></span><br><span class="line">image_path = <span class="string">&#x27;./Dataset/train/ants_image/0013035.jpg&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取图像</span></span><br><span class="line">img_PIL = Image.<span class="built_in">open</span>(image_path)</span><br><span class="line"><span class="comment"># 转化为 numpy.array 格式</span></span><br><span class="line">img_array = numpy.array(img_PIL)</span><br><span class="line"><span class="comment"># 添加图像,参数1: 名称，参数2: 图像，参数3: global_step</span></span><br><span class="line">writter.add_image(<span class="string">&#x27;test&#x27;</span>, img_array, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">writter.close()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>运行后会发现抱错，原因是 shape 不匹配，numpy 格式的图片的 shape 是（H, W, 3），但是 <code>writter.add_image()</code> 默认的 shape 是 （3, H, W），如果需要指定成（H, W, 3），需要修改 <code>dataformats</code> 属性为 <code>HWC</code></p>
<ul>
<li>
<p>修改后代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">writter.add_image(<span class="string">&#x27;test&#x27;</span>, img_array, <span class="number">1</span>, ataformats=<span class="string">&#x27;HWC&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>修改后就能成功运行程序。</p>
</li>
<li>
<p>运行结果</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/133740-62958.png" alt=""></p>
</li>
</ul>
<p>如果需要添加图片，仅需修改图片路径和对应代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">image_path = <span class="string">&#x27;./Dataset/train/bees_image/16838648_415acd9e3f.jpg&#x27;</span></span><br><span class="line"><span class="comment"># 这里改成了 2 </span></span><br><span class="line">writter.add_image(<span class="string">&#x27;test&#x27;</span>, img_array, <span class="number">2</span>, dataformats=<span class="string">&#x27;HWC&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>输出结果</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/134155-0e5ca.png" alt=""></p>
</li>
</ul>
<p>由于未修改图像名称，是在同一片区域下现实的，所以需要拖动上方的滑块，如果希望在不同区域显示，则需要修改标题名称：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">image_path = <span class="string">&#x27;./Dataset/train/bees_image/95238259_98470c5b10.jpg&#x27;</span></span><br><span class="line">writter.add_image(<span class="string">&#x27;train&#x27;</span>, img_array, <span class="number">1</span>, dataformats=<span class="string">&#x27;HWC&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>输出结果</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/134437-5782b.png" style="zoom:67%;" />
</li>
</ul>
<p>通过这种方式，可以很直观地了解到我们给模型的数据。</p>
<h3 id="2-3-transform-的使用">2.3 transform 的使用</h3>
<p>PyTorch 中的 transform 包主要是用来预处理图片，导入方式法为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br></pre></td></tr></table></figure>
<p>我们可以进入该包，可以看到包内定义了很多类</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/135408-6e618.png" alt=""></p>
<p>比如 <code>Compose</code>类，作用是对图片做<strong>中心裁剪</strong>，随后返回一个对应的 tensor。</p>
<p>但是最常用的类是 <code>ToTensor</code> 类，该类可以将 <code>PIL Image</code> 或 <code>numpy.ndarray</code> 转化为 tensor。</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&quot;Dataset/train/ants_image/0013035.jpg&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># img 类型：&lt;class &#x27;PIL.JpegImagePlugin.JpegImageFile&#x27;&gt;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化对象</span></span><br><span class="line">tensor_trans = transforms.ToTensor()</span><br><span class="line"><span class="comment"># 相当于调用了 ToTensor 的 __call__ 方法</span></span><br><span class="line"><span class="comment"># tensor_img 类型：&lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br><span class="line">tensor_img = tensor_trans(img)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>查看 <code>tensor_img</code> 的内容，里面封装了一些反向传播所需要的数据：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/142552-4bc19.png" alt=""></p>
<p>有了 tensor 格式的数据，就可以直接使用 <code>writter.add_image()</code> 方法绘制图像了</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&quot;Dataset/train/ants_image/0013035.jpg&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># img 类型：&lt;class &#x27;PIL.JpegImagePlugin.JpegImageFile&#x27;&gt;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化对象</span></span><br><span class="line">tensor_trans = transforms.ToTensor()</span><br><span class="line">tensor_img = tensor_trans(img)</span><br><span class="line"></span><br><span class="line">writer.add_image(<span class="string">&quot;Tensor_img&quot;</span>, tensor_img)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>输出结果</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/143724-bf6ae.png" style="zoom:67%;" />
</li>
</ul>
<h3 id="2-4-常见的-Transform">2.4 常见的 Transform</h3>
<ul>
<li>
<p><strong>ToTensor</strong></p>
<p>ToTensor 的功能已经在上方解释过了，不再赘述</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;image/4f8ac1e08099aa4c3d8601b5a6c3d604.jpeg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">trans_toTensor = transforms.ToTensor()</span><br><span class="line">img_tensor = trans_toTensor(img)</span><br><span class="line"></span><br><span class="line">writer.add_image(<span class="string">&quot;ToTensor&quot;</span>, img_tensor)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p><strong>Normalize</strong></p>
<p>Normalize 的作用是归一化 tensor ，具体过程是将 Tensor 减去均值再除以标准差</p>
<p>输入：means（均值）、std（标准差），传入的是数组，有几个通道就传几个元素</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;image/4f8ac1e08099aa4c3d8601b5a6c3d604.jpeg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ToTensor</span></span><br><span class="line">trans_toTensor = transforms.ToTensor()</span><br><span class="line">img_tensor = trans_toTensor(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize</span></span><br><span class="line"><span class="comment"># 传入的两个数组分别是均值和标准差</span></span><br><span class="line">trans_norm = transforms.Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line"><span class="comment"># 传入 tensor</span></span><br><span class="line">img_norm = trans_norm(img_tensor)</span><br><span class="line">writer.add_image(<span class="string">&#x27;Normalize&#x27;</span>, img_norm)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>运行结果</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/153225-321b0.png" alt=""></p>
</li>
</ul>
</li>
<li>
<p><strong>Resize</strong></p>
<p>Resize 的作用是将输入的  PIL 图像转化为指定的 size ，参数为 (W, H) 时，会将图像转为改大小，若参数为一个整数，则会让图片的最小边匹配该数值，进行等比缩放</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;image/4f8ac1e08099aa4c3d8601b5a6c3d604.jpeg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ToTensor</span></span><br><span class="line">trans_toTensor = transforms.ToTensor()</span><br><span class="line">img_tensor = trans_toTensor(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Resize 成 512 * 512 的图像</span></span><br><span class="line">trans_resize = transforms.Resize((<span class="number">512</span>, <span class="number">512</span>))</span><br><span class="line">img_resize = trans_resize(img_tensor)</span><br><span class="line">writer.add_image(<span class="string">&#x27;Resize&#x27;</span>, img_resize)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>运行结果</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/154703-7e64a.png" alt=""></p>
</li>
</ul>
</li>
</ul>
<p>以上是使用 <code>PIL</code> 的 <code>Image</code> 模块进行图片读取，但常用的方式是使用 Opencv，安装代码：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install opencv-python</span><br></pre></td></tr></table></figure>
<p>使用 Opencv 读取图片会转换为 <code>numpy.array</code> 格式，</p>
<ul>
<li>
<p><strong>Compose</strong></p>
<p>Compose 的作用是将不同的 transform 组合在一起，比如先将图片进行 Resize，随后将图片进行 ToTensor</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;image/4f8ac1e08099aa4c3d8601b5a6c3d604.jpeg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ToTensor</span></span><br><span class="line">trans_toTensor = transforms.ToTensor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Resize</span></span><br><span class="line">trans_resize_2 = transforms.Resize(<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compose</span></span><br><span class="line">trans_compose = transforms.Compose([trans_resize_2, trans_toTensor])</span><br><span class="line">img_compose = trans_compose(img)</span><br><span class="line">writer.add_image(<span class="string">&quot;Compose&quot;</span>, img_compose)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>数据的变化过程是：PIL -&gt; PIL -&gt; Tensor</p>
<p>新版的 PyTorch 的 Resize 可以接受 Tensor 输入了，所以可以直接用 Tensor 进行 Resize</p>
</blockquote>
</li>
<li>
<p>运行结果</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/155923-1da7c.png" alt=""></p>
</li>
</ul>
</li>
<li>
<p><strong>RandomCrop</strong></p>
<p>RandomCrop 可以对图片进行随机裁剪，可以指定裁剪的尺寸，利用 for 循环可以进行多次裁剪。</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;image/4f8ac1e08099aa4c3d8601b5a6c3d604.jpeg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ToTensor</span></span><br><span class="line">trans_toTensor = transforms.ToTensor()</span><br><span class="line">img_tensor = trans_toTensor(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># RandomCrop()</span></span><br><span class="line">trans_random = transforms.RandomCrop(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    img_random = trans_random(img_tensor)</span><br><span class="line">    writer.add_image(<span class="string">&quot;RandomCrop&quot;</span>, img_random, i)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>该代码对目标图像进行了 10 次随机裁剪，每次裁剪的尺寸是 512 * 512.</p>
</li>
<li>
<p>运行结果</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/161544-e886a.png" alt=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/161554-c064d.png" alt=""></p>
</li>
</ul>
</li>
</ul>
<h3 id="2-4-torchvision-数据集使用">2.4 torchvision 数据集使用</h3>
<p>在 PyTorch 的官方网站中，提供了很多包括关于文本、图像、音频等等的文档，在这些文档里都能找到标准的数据集，供模型训练。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/164221-12a79.png" alt=""></p>
<p>这一节将结合 dataset 和 transform，学习如何对数据集进行处理。</p>
<p>torchvision 的数据集参数都比较相近，首先来了解一下本次使用的 <code>CIFAR-10</code> 数据集：</p>
<p><code>CIFAR-10</code> 数据集包括了 60000 张 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>32</mn><mo>×</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">32\times32</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">32</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">32</span></span></span></span> 的图像，总共有 10 个类别，其中 50000 张是训练图片， 10000 张是验证图片。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/165203-d27da.png" alt=""></p>
<ul>
<li><strong>root:</strong> 必须设置， 指定数据集位置</li>
<li><strong>train:</strong> 为 True 表示该数据集是训练集，为 False 表示该数据集为测试集</li>
<li><strong>transform:</strong> 数据集的预处理</li>
<li><strong>target_transform:</strong> 对指定的 target 进行 transform</li>
<li><strong>download:</strong> 为 True 则会自动下载数据集</li>
</ul>
<p>接下来将使用该数据集进行一系列的数据处理。</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 CIFAR-10 训练集，download=True 自动下载</span></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 CIFAR-10 测试集</span></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(test_set[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(&lt;PIL.Image.Image image mode=RGB size=32x32 at <span class="number">0x10CE0532080</span>&gt;, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>输出包含了两部分，分别是 <code>img</code> 和 <code>target</code>：</p>
<ul>
<li><code>img</code>：图片文件</li>
<li><code>target</code>：图片对应 label 的序号</li>
</ul>
<p>我们也可以通过 <code>class</code> 属性输出对应的 label 名称</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">img, target = test_set[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(test_set.classes[target])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出 cat</span></span><br></pre></td></tr></table></figure>
<p>可以看到，我们得到的图片数据都是 PIL 类型的，但是在 PyTorch 中，最好还是使用 Tensor 类型的数据，所以就需要做一些变换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对数据进行 ToTensor</span></span><br><span class="line">dataset_transform = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 dataset_transform 应用于训练集</span></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, transform=dataset_transform, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 dataset_transform 应用于测试集</span></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=dataset_transform, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">img, target = test_set[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(img))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出 &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<p>转化为 Torch 格式后，就能将图像放入 Tensorboard 进行可视化了，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    img, target = train_set[i]</span><br><span class="line">    writer.add_image(<span class="string">&#x27;train_set&#x27;</span>, img, i)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/180802-96105.png" style="zoom:67%;" />
<h3 id="2-5-DataLoader-的使用">2.5 DataLoader 的使用</h3>
<p>Dataset 是将数据读入，而 DataLoader 的作用是将 Dataset 的数据加载到神经网络中。Dataset 就相当于是一副牌堆，而 DataLoader 就相当于是一张张的手牌，每次都需要从牌堆中取牌，而取牌的方法就是由 DataLoader 控制的。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/182209-85f20.png" alt=""></p>
<p>DataLoader 中有很多参数，但是其中有很多参数是已经有默认值的。</p>
<ul>
<li><strong>dataset</strong> ：Dataset ，需要加载的数据集，相当于一整副扑克</li>
<li><strong>batch_size</strong> ：int，每次取数据的数量，相当于次摸得牌数</li>
<li><strong>shuffle</strong>：bool，是否打乱数据，相当于洗牌</li>
<li><strong>num_workers</strong> ：int，设置多进程</li>
<li><strong>drop_last</strong> ：bool，取不尽时是否舍去剩余数据</li>
</ul>
<p>接下来看看具体代码。</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取测试集</span></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>,</span><br><span class="line">                                         transform=torchvision.transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 DataLoader</span></span><br><span class="line">test_loader = DataLoader(dataset=test_data, batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>,</span><br><span class="line">                         drop_last=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">img, target = test_data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(img.shape)		<span class="comment"># torch.Size([3, 32, 32])</span></span><br><span class="line"><span class="built_in">print</span>(target)			<span class="comment"># 3</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里设置了 <code>batch_size=4</code>，表示一次会取 4 个数据；<code>shuffle=True</code> 表示每次取完数据后都会打乱数据。</p>
<p>接下来使用 DataLoader 取出数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)</span><br><span class="line">    <span class="built_in">print</span>(targets)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>]) 	<span class="comment"># 4张图片 3通道 尺寸为 32 * 32</span></span><br><span class="line">tensor([<span class="number">5</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">2</span>])		<span class="comment"># 四张图片对应的 label </span></span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">tensor([<span class="number">2</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">4</span>])</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">tensor([<span class="number">7</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">tensor([<span class="number">7</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">9</span>])</span><br><span class="line">···· </span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>有了数据后就可以通过 TensorBoard 进行可视化了，但是这次不是单独的数据，而是多个数据打包在一起，所以代码和之前有些许区别：</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取测试集</span></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>,</span><br><span class="line">                                         transform=torchvision.transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每次取64张</span></span><br><span class="line">test_loader = DataLoader(dataset=test_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>,</span><br><span class="line">                         drop_last=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">img, target = test_data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">writter = SummaryWriter(<span class="string">&#x27;DataLoader&#x27;</span>)</span><br><span class="line"><span class="comment"># 设置全局路径</span></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    writter.add_images(<span class="string">&quot;test_DataLoader&quot;</span>, imgs, step)</span><br><span class="line">    <span class="comment"># 每次添加完图片后增加 step</span></span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">writter.close()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>输出结果</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/194542-abfb2.png" style="zoom:67%;" />
</li>
</ul>
<p>可以看到每个 step 有 64 张图片，但是最后一步并没有 64 张，这是因为数据集不能被 64 整除，由于设置了 <code>drop_last=False</code> ，剩下的图像被合并成最后一步。</p>
<p>由于设置了 <code>shuffle=True</code> ，所以每个 epoch 取得图片顺序都不会相同：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        writer.add_images(<span class="string">&quot;test_DataLoader_epoch_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch), imgs, step)</span><br><span class="line">        step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/200730-8b4ce.png" alt=""></p>
<h2 id="三、神经网络结构">三、神经网络结构</h2>
<h3 id="3-1-PyTorch-搭建神经网络介绍">3.1 PyTorch 搭建神经网络介绍</h3>
<p>PyTorch 中关于神经网络的文档都可以在 PyTorch 官网的 <code>torch.nn</code> 模块中找到，包括<strong>骨架（Containers）</strong>、<strong>池化层（Pooling Layers）</strong>、 **非线性激活函数（Non-linear-Activation）**等等，都可以在 PyTorch 官网中找到。</p>
<p>其中 Containers  中包含六个类，最常用的是 <code>Module</code> 类 ，它为所有神经网络提供了最基本的骨架，神经网络必须继承这个类。</p>
<p>官网给出的案例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每次都必须重写该方法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        <span class="keyword">return</span> F.relu(self.conv2(x))</span><br></pre></td></tr></table></figure>
<p>这个 Demo 首先是继承了 nn.Module 类，进行了初始化，随后进行前向传播，前向传播的过程是：输入 -&gt; 卷积 -&gt;  ReLU -&gt; 卷积 -&gt; ReLU -&gt; 输出。</p>
<h3 id="3-2-搭建自己的神经网络">3.2 搭建自己的神经网络</h3>
<p>了解了 PyTorch 搭建神经网络的方法，接下来就开始搭建一个最简单的神经网络：</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建自己的网络类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net01</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># 前向传播让输入加 1 </span></span><br><span class="line">        output = <span class="built_in">input</span> + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化对象</span></span><br><span class="line">net01 = Net01()</span><br><span class="line">x = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line"><span class="comment"># 前向传播，因为 __call__() 方法的实现 _call_impl() 中调用了 forward() ，所以会直接执行 forward()</span></span><br><span class="line">output = net01(x)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="comment"># 输出 2.</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="3-3-PyTorch-卷积操作">3.3 PyTorch 卷积操作</h3>
<p>在 PyTorch 官网中可以找到有关卷积层操作的文档：[地址](<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#convolution-layers">torch.nn — PyTorch 1.12 documentation</a>)。其中 <code>nn.Conv1d</code> 表示数据是一维数据，<code>nn.Conv2d</code> 表示数据是二维的（比如图像就是二维数据）。</p>
<p>PyTorch有关神经网络的 API 有两个，分别是 <code>torch.nn</code> 和 <code>torch.nn.function</code>，其中 <code>torch.nn</code> 是对 <code>torch.nn.function</code> 的封装，是更底层的代码，在使用卷积层操作之前，先来了解一下 <code>torch.nn.function</code> 这个 API 。</p>
<p><code>torch.nn.function</code>  中的模块和 <code>torch.nn</code> 的模块是相同的，首先来看看 <code>nn.Conv1d</code> 模块，它包含了以下参数：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/04/191536-95e57.png" alt=""></p>
<p>其中 <code>weight</code> 就是卷积核，<code>bias</code> 是偏置项，<code>stride</code> 是卷积核移动的步长，<code>padding</code> 是为卷积结果进行零填充的层数。</p>
<p>下面是一个具体例子：</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建二维矩阵</span></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                      [<span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建卷积核</span></span><br><span class="line">kernel = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                       [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">kernel = torch.reshape(kernel, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出1：卷积核步长为 1 </span></span><br><span class="line">output1 = F.conv2d(<span class="built_in">input</span>, kernel, stride=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(output1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出2：卷积核步长为 2 ，</span></span><br><span class="line">output2 = F.conv2d(<span class="built_in">input</span>, kernel, stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(output2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出2：卷积核步长为 1 ，零填充 1 层</span></span><br><span class="line">output2 = F.conv2d(<span class="built_in">input</span>, kernel, stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(output2)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>运行结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出 1 </span></span><br><span class="line">tensor([[[[<span class="number">10</span>, <span class="number">12</span>, <span class="number">12</span>],</span><br><span class="line">          [<span class="number">18</span>, <span class="number">16</span>, <span class="number">16</span>],</span><br><span class="line">          [<span class="number">13</span>,  <span class="number">9</span>,  <span class="number">3</span>]]]])</span><br><span class="line"><span class="comment"># 输出 2</span></span><br><span class="line">tensor([[[[<span class="number">10</span>, <span class="number">12</span>],</span><br><span class="line">          [<span class="number">13</span>,  <span class="number">3</span>]]]])</span><br><span class="line"><span class="comment"># 输出 3</span></span><br><span class="line">tensor([[[[ <span class="number">1</span>,  <span class="number">3</span>,  <span class="number">4</span>, <span class="number">10</span>,  <span class="number">8</span>],</span><br><span class="line">          [ <span class="number">5</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">12</span>,  <span class="number">6</span>],</span><br><span class="line">          [ <span class="number">7</span>, <span class="number">18</span>, <span class="number">16</span>, <span class="number">16</span>,  <span class="number">8</span>],</span><br><span class="line">          [<span class="number">11</span>, <span class="number">13</span>,  <span class="number">9</span>,  <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">          [<span class="number">14</span>, <span class="number">13</span>,  <span class="number">9</span>,  <span class="number">7</span>,  <span class="number">4</span>]]]])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="3-4-卷积层">3.4 卷积层</h3>
<p>PyTorch 中的卷积层主要使用 <code>torch.nn</code> 的 <code>Conv2d</code> 类，首先来看看官方文档：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/04/195436-39e11.png" alt=""></p>
<p>文档指出了使用该类所需要的参数，包括：</p>
<ul>
<li><em><strong>in_channels</strong></em>：输入通道数，彩色图片一般为 3 通道，必须指定</li>
<li><em><strong>out_channels</strong></em>：输出通道数，设置为几就有几个卷积核，必须指定</li>
<li><em><strong>kernel_size</strong></em>：卷积核大小，整型或者元组，必须指定</li>
<li><em>stride</em>：卷积核步长，一般需要自己指定</li>
<li><em>padding</em>：填充层数，一般需要自己指定</li>
<li><em>dilation</em>：卷积核元素间隔，一般不常用</li>
<li><em>groups</em>：一般设置为 1 ，分组卷积时进行修改，基本遇不到</li>
<li><em>bias</em>：是否启用偏置项，一般为 True</li>
<li><em>padding_mode</em>：填充模式（比如 0 填充）</li>
<li><em>device</em>：使用设备</li>
<li><em>dtype</em>：数据类型</li>
</ul>
<p>下面是一个实例：</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取数据集</span></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 dataLoader</span></span><br><span class="line">dataLoader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建自己的卷积神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 创建卷积层</span></span><br><span class="line">        self.conv1 = Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重写前向传播</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化网络</span></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了方便理解，在 tensorboard 中进行可视化</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 遍历 dataLoader，进行卷积</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataLoader:</span><br><span class="line">    <span class="comment"># torch.Size([64, 3, 30, 30])</span></span><br><span class="line">    imgs, targets = data</span><br><span class="line">    <span class="comment"># torch.Size([64, 6, 30, 30])</span></span><br><span class="line">    output = myNet(imgs)</span><br><span class="line">    </span><br><span class="line">    writer.add_images(<span class="string">&#x27;input&#x27;</span>, imgs, step)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 由于卷积后尺寸并不匹配 add_images()，需要进行 reshape,</span></span><br><span class="line">    <span class="comment"># 设置 -1 会让框架进行自动计算</span></span><br><span class="line">    output = torch.reshape(output, (-<span class="number">1</span>, <span class="number">3</span>, <span class="number">30</span>, <span class="number">30</span>))</span><br><span class="line">    writer.add_images(<span class="string">&#x27;output&#x27;</span>, output, step)</span><br><span class="line"></span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>输出结果</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/04/210712-28c62.png" alt=""></p>
</li>
</ul>
<h3 id="3-5-池化层">3.5 池化层</h3>
<p>[池化层文档](<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#pooling-layers">torch.nn — PyTorch 1.12 documentation</a>)，PyTorch 中的池化层包含很多类，比如 <code>nn.MaxPool2d</code>（下采样）、<code>nn.MaxUnpool2d</code>（上采样）、<code>nn.AvgPool2d</code>（平均池化）但是我们最常用的还是 <code>nn.MaxPool2d</code>。接下来看看对应的文档：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/093527-c6fd5.png" alt=""></p>
<p>它的参数有：</p>
<ul>
<li>
<p><em><strong>kernel_size</strong></em>：池化核大小，同卷积核大小，必须设置</p>
</li>
<li>
<p><em>stride</em>：池化核移动步长，默认值是 kernel_size</p>
</li>
<li>
<p><em>padding</em>：填充层数</p>
</li>
<li>
<p><em>dilation</em>：卷积核元素间隔，一般不设置</p>
</li>
<li>
<p><em>return_indices</em>：用的很少，不做了解</p>
</li>
<li>
<p><em>ceil_mode</em>：bool，设置为 False 时，使用 floor 模式，丢弃多余数据；设置为 True 时，使用 ceil 模式，保留多余数据，如下图所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/100158-53e0f.png" alt=""></p>
</li>
</ul>
<p>下面是一个具体例子：</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建输入</span></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                      [<span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把输入 reshape 成指定 shape</span></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (-<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 网络初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        self.pooling = nn.MaxPool2d(kernel_size=<span class="number">3</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重写前向传播</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        output = self.pooling(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络实例化</span></span><br><span class="line">myNet = MyNet()</span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">output = myNet(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出1</span></span><br><span class="line">tensor([[[[<span class="number">2.</span>]]]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果设置 ceil_mode=True,输出为</span></span><br><span class="line">tensor([[[[<span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">          [<span class="number">5.</span>, <span class="number">1.</span>]]]])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>为了清楚池化层的作用，我们可以通过 tensorboard 进行可视化：</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                      [<span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (-<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line">dataLoader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        self.pooling = nn.MaxPool2d(kernel_size=<span class="number">3</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        output = self.pooling(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line">myNet = MyNet()</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataLoader:</span><br><span class="line">    img, target = data</span><br><span class="line">    output = myNet(img)</span><br><span class="line">    writer.add_images(<span class="string">&#x27;input&#x27;</span>, img, step)</span><br><span class="line">    writer.add_images(<span class="string">&#x27;output&#x27;</span>, output, step)</span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>运行结果</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/103334-4073f.png" alt=""></p>
</li>
</ul>
<p>可以看到，经过池化层后，图片仍然保存了本身的特征，而且体积被大大缩小了。</p>
<h3 id="3-6-非线性激活函数">3.6 非线性激活函数</h3>
<p>Pytorch [官方文档](<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">torch.nn — PyTorch 1.12 documentation</a>)。在神经网络中，非线性激活函数的作用是为网络引入非线性的特质，其中最常用的是 <code>nn.ReLU</code> 。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="C:/Users/Fkhsns/AppData/Roaming/Typora/typora-user-images/image-20220705105339920.png" style="zoom:67%;" />
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1.1</span>, -<span class="number">0.5</span>],</span><br><span class="line">                      [-<span class="number">1</span>, <span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, [-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        <span class="comment"># 若 inplace 为 False 会返回一个新的变量，</span></span><br><span class="line">        <span class="comment"># 若 inplace 为 True 则会修改原来的变量</span></span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.relu()</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line">output = myNet(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([[[[<span class="number">1.1000</span>, <span class="number">0.0000</span>],</span><br><span class="line">          [<span class="number">0.0000</span>, <span class="number">3.0000</span>]]]])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>同样的，为了显示非线性函数的效果，可以将处理过的数据进行可视化</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line">dataLoader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">False</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># 这里使用 sigmoid 函数，</span></span><br><span class="line">        <span class="comment"># 因为用 sigmoid 函数处理的图像区别看起来更明显</span></span><br><span class="line">        output = self.sigmoid(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">myNet = MyNet()</span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataLoader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    writer.add_images(<span class="string">&#x27;input&#x27;</span>, imgs, step)</span><br><span class="line">    output = myNet(imgs)</span><br><span class="line">    writer.add_images(<span class="string">&#x27;output2&#x27;</span>, output, step)</span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>输出结果</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/113131-16f07.png" alt=""></p>
</li>
</ul>
<h3 id="3-7-批量归一化">3.7 批量归一化</h3>
<p>PyTorch <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#normalization-layers">官方文档</a>。使用批量归一化可以解决梯度消失和梯度爆炸的问题，常用的是 <code>nn.BatchNorm2d</code> 类，官方文档内容如下：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/123427-5a137.png" alt=""></p>
<p>参数：</p>
<ul>
<li><em><strong>num_features</strong></em>：输入的通道数， $(N, C, H, W) $ 中的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span></li>
<li>其他参数一半使用默认值</li>
</ul>
<h3 id="3-8-线性层">3.8 线性层</h3>
<p>PyTorch <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#linear-layers">官方文档</a>。线性层即神经网络中的全连接层，包含大量的参数，一般放在网络的最后方，如下图所示：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/124830-13cd8.png" style="zoom:67%;" />
<p>除了 Input layer ，每一层的元素都是上层元素的加权和，在 PyTorch 中计算线性层非常方便，首先来看看官方文档，确认这一层的参数：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/125034-97440.png" alt=""></p>
<p>参数：</p>
<ul>
<li><em><strong>in_features</strong></em>：输入的特征数</li>
<li><em><strong>out_features</strong></em>：输出的特征数</li>
<li><em>bias</em>：是否添加偏置项</li>
<li><em>device</em>： 使用设备</li>
<li><em>dtype</em>：指定数据类型</li>
</ul>
<p>下面是一个例子：</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line">dataLoader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        <span class="comment"># 线性层，输入维度时 196608 ,输出维度是 10 </span></span><br><span class="line">        self.linear = nn.Linear(<span class="number">196608</span>, <span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.linear(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataLoader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    <span class="comment"># 将图像展平</span></span><br><span class="line">    imgs = torch.flatten(imgs)</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)</span><br><span class="line">    <span class="comment"># 全连接层前向传播</span></span><br><span class="line">    output = myNet(imgs)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="3-9-Dropout-层">3.9 Dropout 层</h3>
<p>PyTorch <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#dropout-layers">官方文档</a>。Dropout 层的主要作用是在模型训练阶段，随机地将输入张量的某些元素以概率 p 归零，是卷积神经网络中防止过拟合的一种方式。</p>
<h3 id="3-10-Sequential">3.10 Sequential</h3>
<p>PyTorch <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential">官方文档</a>。使用 Squential 可以将网络结构组合起来，按照我们设定的顺序一步步进行前向传播，使代码更加简洁易懂，更加方便管理。</p>
<p>下面是一个官方示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Using Sequential to create a small model. When `model` is run,</span></span><br><span class="line"><span class="comment"># input will first be passed to `Conv2d(1,20,5)`. The output of</span></span><br><span class="line"><span class="comment"># `Conv2d(1,20,5)` will be used as the input to the first</span></span><br><span class="line"><span class="comment"># `ReLU`; the output of the first `ReLU` will become the input</span></span><br><span class="line"><span class="comment"># for `Conv2d(20,64,5)`. Finally, the output of</span></span><br><span class="line"><span class="comment"># `Conv2d(20,64,5)` will be used as input to the second `ReLU`</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">          nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU(),</span><br><span class="line">          nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using Sequential with OrderedDict. This is functionally the</span></span><br><span class="line"><span class="comment"># same as the above code</span></span><br><span class="line">model = nn.Sequential(OrderedDict([</span><br><span class="line">          (<span class="string">&#x27;conv1&#x27;</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU()),</span><br><span class="line">          (<span class="string">&#x27;conv2&#x27;</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">&#x27;relu2&#x27;</span>, nn.ReLU())</span><br><span class="line">        ]))</span><br></pre></td></tr></table></figure>
<h2 id="四、神经网络简单搭建">四、神经网络简单搭建</h2>
<blockquote>
<p>了解了 PyTorch 中神经网络各层的代码编写规则，接下来就可以搭建自己的卷积神经网络了，本节将实现使用卷积神经网络和 <code>CIFAR-10</code> 数据集，完成 图像分类的功能</p>
</blockquote>
<h3 id="4-1-CIFAR-10-网络结构">4.1 CIFAR-10 网络结构</h3>
<p>想要实现一个卷积神经网络，首先需要设计网络的结构，这里我们不自己设计网络结构，而是使用别人已经设计好的结构来实现分类功能，具体的网络结构如下图所示：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/141555-e209f.png" alt=""></p>
<p>网络的结构是：输入 - &gt; <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mo>×</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">5\times5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">5</span></span></span></span> 卷积层 - &gt; <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span></span></span></span> 最大池化层 - &gt; <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mo>×</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">5\times5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">5</span></span></span></span> 卷积层 - &gt; <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span></span></span></span> 最大池化层 - &gt; <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mo>×</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">5\times5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">5</span></span></span></span>  卷积层 -&gt; <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span></span></span></span> 池化 - &gt; 展开成一维 - &gt; 全连接层 - &gt; 输出。</p>
<p>可以发现网络结构十分简单，也没有引入非线性函数，接下来我们就根据这个结构来训练一个图像分类器。</p>
<h3 id="4-2-网络搭建">4.2 网络搭建</h3>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dataLoader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        <span class="comment"># 为了使输出尺寸保持不变，需要计算 padding 和 stride</span></span><br><span class="line">        <span class="comment"># 计算结果为 padding = 2, stride = 1</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        self.pooling1 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, padding=<span class="number">2</span>)</span><br><span class="line">        self.pooling2 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        self.pooling3 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">1024</span>, <span class="number">64</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">	<span class="comment"># 重写前向传播</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        x = self.conv1(<span class="built_in">input</span>)</span><br><span class="line">        x = self.pooling1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.pooling2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.pooling3(x)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.linear1(x)</span><br><span class="line">        output = self.linear2(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络结构检验</span></span><br><span class="line"><span class="built_in">input</span> = torch.ones((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">output = myNet(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape) <span class="comment"># 输出 torch.Size([64, 10])</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>卷积层计算公式：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/142813-be9d0.png" alt="卷积层 shape 计算公式"></p>
<p>可以看到这样编写代码，会让前向传播显得十分冗余，我们可以使用 <code>nn.Sequential</code> 来简化代码。</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dataLoader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        <span class="comment"># 使用 nn.Sequential 整合网络结构</span></span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播代码三行就搞定了</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.model(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络结构检验</span></span><br><span class="line"><span class="built_in">input</span> = torch.ones((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">output = myNet(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape) <span class="comment"># 输出 torch.Size([64, 10])</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>通过 tensorboard ，也可以将我们的卷积神经网络结构进行可视化。</p>
<ul>
<li>
<p>代码编写（接上方代码）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化网络结构（计算图)</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./logs_seq&#x27;</span>)</span><br><span class="line">writer.add_graph(myNet, <span class="built_in">input</span>)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>运行结果</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/150805-27392.png" alt=""></p>
</li>
</ul>
<p>可以看到 tensorboard 可视化后的网络结构十分清晰，我们还可以进一步细化，进入每一层的内部观察数据的 shape。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/151023-dcc2c.png" alt="第一层卷积层" style="zoom:67%;" />
<h2 id="五、损失函数、反向传播及优化">五、损失函数、反向传播及优化</h2>
<h3 id="5-1-损失函数">5.1 损失函数</h3>
<p>PyTorch <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#loss-functions">官方文档</a>。损失函数是衡量模型预测结果与实际数据差距的数值，包括 <code>L1loss</code>、<code>MSE  loss</code>、<code>nn.CrossEntropyLoss</code>  等等，不同的损失函数在官方文档都有很详细的解释，下面来看一个例子。</p>
<ul>
<li>
<p>代码编写  ( L1loss )</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出 [1, 2, 3]</span></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.float32)</span><br><span class="line"><span class="comment"># 目标 [1, 2, 5]</span></span><br><span class="line">targets = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>], dtype=torch.float32)</span><br><span class="line"><span class="comment"># 损失值： |(1 - 1 + 2 - 2 + 3 - 5)| / 3 = 0.66667</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># reshape 输出和目标</span></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line">targets = torch.reshape(targets, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        self.loss = nn.L1Loss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.loss(<span class="built_in">input</span>, targets)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line">loss = myNet(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出 tensor(0.6667)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>上一节我们已经编写了一个用于分类 <code>CIFAR-10</code> 数据集的神经网络模型，现在我们就可以来看看这个模型在数据集上的损失函数。</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dataLoader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        self.loss = nn.L1Loss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.model(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化损失计算</span></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataLoader:</span><br><span class="line">    imgs, target = data</span><br><span class="line">    output = myNet(imgs)</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    result_loss = loss(output, target)</span><br><span class="line">    <span class="built_in">print</span>(result_loss)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>运行结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">2.3061</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br><span class="line">tensor(<span class="number">2.3143</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br><span class="line">tensor(<span class="number">2.2893</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br><span class="line">tensor(<span class="number">2.2944</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br><span class="line">tensor(<span class="number">2.3121</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br><span class="line">tensor(<span class="number">2.3048</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br><span class="line">tensor(<span class="number">2.3090</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br><span class="line">···</span><br><span class="line">···</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>运行的结果就是<strong>网络输出</strong>与<strong>目标</strong>之间的差距，有了这些差距，就为我们提供了参数更新的依据（反向传播）。</p>
<h3 id="5-2-反向传播">5.2 反向传播</h3>
<p>PyTorch 为计算图中的每一个节点提供了一个 <code>grad</code> 参数，用于保存该节点的梯度，有了 <code>grad</code> 就有了反向传播的依据，我们只需要一行代码：<code>result_loss.backward()</code> ，就能实现梯度的自动计算。</p>
<p>在运行这行代码之前，节点的梯度值 <code>grad</code> 都是 None</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/165938-523bf.png" alt=""></p>
<p>运行完该段代码后，<code>grad</code> 会自动计算出来：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/170027-6b153.png" alt=""></p>
<h3 id="5-3-优化器">5.3 优化器</h3>
<p>Pytorch <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">官方文档</a>。为了实现参数更新，我们需要使用到 PyTorch 中的优化器，在官方文档中有详细的介绍，且提供了构造优化器的案例。在上节中我们已经介绍了如何计算节点的梯度，有了梯度后就可以调用<code>optimizer.step()</code> 来进行参数更新，下面来看一个例子。</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                       transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dataLoader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        self.loss = nn.L1Loss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.model(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">myNet = MyNet()</span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 构建优化器</span></span><br><span class="line">optmi = torch.optim.SGD(myNet.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    <span class="comment"># 初始化一个 epoch 的 loss</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataLoader:</span><br><span class="line">        imgs, target = data</span><br><span class="line">        output = myNet(imgs)</span><br><span class="line">        result_loss = loss(output, target)</span><br><span class="line">        <span class="comment"># 先将梯度归零</span></span><br><span class="line">        optmi.zero_grad()</span><br><span class="line">        <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">        result_loss.backward()</span><br><span class="line">        <span class="comment"># 参数更新</span></span><br><span class="line">        optmi.step()</span><br><span class="line">        <span class="comment"># 将一批数据的 loss 加到整个 epoch 的 loss 上</span></span><br><span class="line">        running_loss += result_loss</span><br><span class="line">    <span class="built_in">print</span>(running_loss)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>输出结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">360.9923</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">tensor(<span class="number">358.1781</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">tensor(<span class="number">345.0446</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">···</span><br><span class="line">tensor(<span class="number">237.9353</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">tensor(<span class="number">234.4855</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">tensor(<span class="number">231.1977</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>可以看到每一轮 epoch 的整体 loss 都在减小，模型正在不断拟合数据。</p>
<h2 id="六、模型使用">六、模型使用</h2>
<h3 id="6-1-现有模型使用方法">6.1 现有模型使用方法</h3>
<p>PyTorch 中已经提供了许多已经训练好的模型，本节将介绍如何使用这些模型和如何对模型做一些自定义的修改，<a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/models.html#">官方文档</a>。</p>
<p>模型的类型包括：</p>
<ul>
<li>分类</li>
<li>语义分割</li>
<li>目标检测、实例分割和人物关键点检测</li>
<li>视频分类</li>
</ul>
<p>本节将使用 PyTorch 提供的 <code>VGG16</code> 分类模型来实现 <code>CIFAR-10</code> 图片数据集分类的功能。</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 140G 的训练集</span></span><br><span class="line"><span class="comment"># train_data = torchvision.datasets.ImageNet(&#x27;./data_image_net&#x27;, split=&#x27;train&#x27;, download=True,</span></span><br><span class="line"><span class="comment">#                                            transform=torchvision.transforms.ToTensor())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 未训练好的 Vgg16</span></span><br><span class="line">vgg16_false = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 已训练好的 Vgg16</span></span><br><span class="line">vgg16_true = torchvision.models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vgg16_true)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>运行结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">VGG(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">3</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">4</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">5</span>): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">6</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">7</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">8</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">9</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">10</span>): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">11</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">12</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">13</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">14</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">15</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">16</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">17</span>): Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">18</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">19</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">20</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">21</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">22</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">23</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">24</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">25</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">26</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">27</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">28</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">29</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">30</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(<span class="number">7</span>, <span class="number">7</span>))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">25088</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">2</span>): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">3</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">4</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">5</span>): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">1000</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">Process finished <span class="keyword">with</span> exit code <span class="number">0</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>输出结果很清晰的展示了 Vgg16 的网络结构，大体上分为三个部分：</p>
<ul>
<li>特征处理  (features)</li>
<li>平均池化  (avgpool)</li>
<li>分类器      (classifier)</li>
</ul>
<p>观察最后一层，可以看到输出是 1000 维的向量，而 <code>CIFAR-10</code> 数据库只有 10 类数据，我们可以想办法修改这个已有的模型，方法有：</p>
<ol>
<li>
<p>将最后的输出层元素数量由 1000 改为 10</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                          download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">vgg16_false = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">vgg16_true = torchvision.models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># vgg16_true.classifier.add_module(&#x27;add_linear&#x27;, nn.Linear(1000, 10))</span></span><br><span class="line"></span><br><span class="line">vgg16_false.classifier[<span class="number">6</span>] = nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vgg16_false)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>输出结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">VGG(</span><br><span class="line">	···</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">	···</span><br><span class="line">    (<span class="number">5</span>): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p>在最后一层的后方在添加一层网络</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                          download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">vgg16_false = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">vgg16_true = torchvision.models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 classifier 后添加一层网络</span></span><br><span class="line"><span class="comment"># 去掉 .classifier 也可也在后方加上一层，只不过是在 classifier 之外</span></span><br><span class="line">vgg16_true.classifier.add_module(<span class="string">&#x27;add_linear&#x27;</span>, nn.Linear(<span class="number">1000</span>, <span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(vgg16_true)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>输出结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">VGG(</span><br><span class="line">	······</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">	······</span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">1000</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (add_linear): Linear(in_features=<span class="number">1000</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
<h3 id="6-2-模型的保存与读取">6.2 模型的保存与读取</h3>
<ul>
<li>
<p>模型保存</p>
<ul>
<li>
<p>保存方式 1：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式1，这种方式不仅保存了网络，还保存了网络中的参数</span></span><br><span class="line"><span class="comment"># 这种方式存在陷阱，保存后，在使用之前必须引入对应的包</span></span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">torch.save(vgg16, <span class="string">&#x27;./vgg16_method1.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>保存方式2：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式2，仅保存网络参数，不保存模型，官方推荐</span></span><br><span class="line">torch.save(vgg16.state_dict(), <span class="string">&#x27;./vgg16_method2.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p>模型读取</p>
<ul>
<li>
<p>读取方式 1：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式1 -&gt; 保存方式 1 对应的加载模型，不仅读取了模型，还读取了网络参数 </span></span><br><span class="line">model = torch.load(<span class="string">&#x27;./vgg16_method1.pth&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出网络结构</span></span><br><span class="line">VGG(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      ·······</span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">1000</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>读取方式2：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式2 -&gt; 保存方式 2 对应的模型加载</span></span><br><span class="line">model2 = torch.load(<span class="string">&#x27;./vgg16_method2.pth&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(model2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为模型载入参数</span></span><br><span class="line">vgg16 = torchvision.models.vgg16()</span><br><span class="line">vgg16.load_state_dict(model2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出字典，</span></span><br><span class="line">OrderedDict([(<span class="string">&#x27;features.0.weight&#x27;</span>, tensor([[[[-<span class="number">0.0072</span>, -<span class="number">0.0889</span>, -<span class="number">0.0209</span>],</span><br><span class="line">          [ <span class="number">0.0320</span>,  <span class="number">0.0086</span>, -<span class="number">0.0210</span>],</span><br><span class="line">          [-<span class="number">0.1112</span>, -<span class="number">0.1005</span>,  <span class="number">0.0080</span>]],</span><br><span class="line">······</span><br><span class="line"><span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,</span><br><span class="line">        <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]))])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h2 id="七、-完整的模型训练">七、 完整的模型训练</h2>
<h3 id="7-1-训练模型完整步骤">7.1 训练模型完整步骤</h3>
<ol>
<li>
<p>准备数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model_train.py</span></span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                          download=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                         download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line"><span class="comment"># 测试集长度</span></span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dataLoader 加载数据集</span></span><br><span class="line">train_dataLoader = DataLoader(dataset=train_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataLoader = DataLoader(dataset=test_data, batch_size=<span class="number">64</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>编写网络模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新起一个 Model.py 文件，专门保存模型代码</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">64</span> * <span class="number">4</span> * <span class="number">4</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.model(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试网络正确性，通过测试后就可以将该模型引入其他文件</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    myNet = MyNet()</span><br><span class="line">    <span class="built_in">input</span> = torch.ones((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">    output = myNet(<span class="built_in">input</span>)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>实例化网络模型、损失函数、优化器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model_train.py</span></span><br><span class="line"><span class="comment"># 创建网络模型</span></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = torch.optim.SGD(myNet.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 writer</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./test_logs&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>模型训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练次数</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-----第 &#123;&#125; 次训练开始-----&quot;</span>.<span class="built_in">format</span>(i + <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataLoader:</span><br><span class="line">        imgs, target = data</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        output = myNet(imgs)</span><br><span class="line">        <span class="comment"># 损失计算</span></span><br><span class="line">        loss = loss_fn(output, target)</span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_train_step += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;训练次数：&#123;&#125;， loss:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_train_step, loss.item()))</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>模型及保存测试</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment"># 测试步骤</span></span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 仅需要进行测试，不需要调整梯度，最好设置为 torch.no_grad()</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataLoader:</span><br><span class="line">            imgs, targets = data</span><br><span class="line">            output = myNet(imgs)</span><br><span class="line">            test_loss = loss_fn(output, targets)</span><br><span class="line">            total_test_loss += test_loss.item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;整体 test loos:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;整体 accuracy:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;test_loss&#x27;</span>, total_test_loss, total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;test_accuracy&#x27;</span>, total_accuracy, total_test_step)</span><br><span class="line">    total_test_step += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 保存模型</span></span><br><span class="line">    torch.save(myNet, <span class="string">&#x27;mynet_&#123;&#125;.pth&#x27;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    <span class="comment"># 方式2：torch.save(myNet.state_dict(), &#x27;mynet_&#123;&#125;.pth&#x27;.format(i))  </span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;模型已保存&#x27;</span>) </span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ul>
<li>
<p>输出结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">-----第 <span class="number">1</span> 次训练开始-----</span><br><span class="line">训练次数：<span class="number">1</span>， loss:<span class="number">2.3277838230133057</span></span><br><span class="line">训练次数：<span class="number">2</span>， loss:<span class="number">2.3247644901275635</span></span><br><span class="line">训练次数：<span class="number">3</span>， loss:<span class="number">2.2977490425109863</span></span><br><span class="line">······</span><br><span class="line">训练次数：<span class="number">7818</span>， loss:<span class="number">0.9464911222457886</span></span><br><span class="line">训练次数：<span class="number">7819</span>， loss:<span class="number">1.2017658948898315</span></span><br><span class="line">训练次数：<span class="number">7820</span>， loss:<span class="number">1.4486428499221802</span></span><br><span class="line">整体 test loos:<span class="number">206.69312530755997</span></span><br><span class="line">整体 accuracy:<span class="number">0.534500002861023</span></span><br><span class="line">模型已保存</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/07/121631-39c5e.png" alt="损失函数值"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/07/124144-78a33.png" alt="测试集准确率"></p>
</li>
</ul>
<blockquote>
<p>在读别人的代码的时候，在训练之前有时候会加上 <code>model.train()</code>，让模型进入训练状态；在验证之前会加上 <code>model.eval()</code>，让模型进入验证状态。这并不代表加上这些代码才可以运行程序，而是说如果模型中有些一些特殊的层（BN、Dropout）时，需要添加上，如果没有，即便不添加上这两个代码也可以运行。</p>
</blockquote>
<h3 id="7-2-使用-GPU-训练模型">7.2 使用 GPU 训练模型</h3>
<p>PyTorch 可以使用 NVIDIA 的 GPU 进行模型训练，分为两种方式：</p>
<ul>
<li>
<p>方式1：</p>
<p>找到模型以下三个部分：</p>
<ul>
<li>网络模型</li>
<li>数据（imgs，targets）</li>
<li>损失函数</li>
</ul>
<p>调用对应对象的 <code>.cuda()</code> 方法，即可实现在 GPU 上进行训练</p>
<ul>
<li>
<p>完整代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> Model <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                          download=<span class="literal">True</span>)</span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                         download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line"><span class="comment"># 测试集长度</span></span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dataLoader 加载数据集</span></span><br><span class="line">train_dataLoader = DataLoader(dataset=train_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataLoader = DataLoader(dataset=test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建网络模型</span></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络模型使用 GPU 计算</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    myNet.cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;cuda 无法使用&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 损失函数使用 GPU 计算</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    loss_fn = loss_fn.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = torch.optim.SGD(myNet.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 writer</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./test_logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-----第 &#123;&#125; 次训练开始-----&quot;</span>.<span class="built_in">format</span>(i + <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataLoader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练数据使用 GPU 计算</span></span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            imgs = imgs.cuda()</span><br><span class="line">            targets = targets.cuda()</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        output = myNet(imgs)</span><br><span class="line">        <span class="comment"># 损失计算</span></span><br><span class="line">        loss = loss_fn(output, targets)</span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_train_step += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            end_time = time.time()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;运行时间：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(end_time-start_time))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;训练次数：&#123;&#125;， loss:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_train_step, loss.item()))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试步骤</span></span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataLoader:</span><br><span class="line">            imgs, targets = data</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 测试数据使用 GPU 计算</span></span><br><span class="line">            <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">                imgs = imgs.cuda()</span><br><span class="line">                targets = targets.cuda()</span><br><span class="line"></span><br><span class="line">            output = myNet(imgs)</span><br><span class="line">            test_loss = loss_fn(output, targets)</span><br><span class="line">            total_test_loss += test_loss.item()</span><br><span class="line">            total_accuracy += (output.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;整体 test loos:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;整体 accuracy:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;test_loss&#x27;</span>, total_test_loss, total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;test_accuracy&#x27;</span>, total_accuracy, total_test_step)</span><br><span class="line">    total_test_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存模型</span></span><br><span class="line">    torch.save(myNet, <span class="string">&#x27;mynet_&#123;&#125;.pth&#x27;</span>.<span class="built_in">format</span>=(i))</span><br><span class="line">    <span class="comment"># 方式2：torch.save(myNet.state_dict(), &#x27;mynet_&#123;&#125;.pth&#x27;.format(i))</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;模型已保存&#x27;</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p>方式2：</p>
<p>在程序的一开始用 <code>torch.device()</code> 方法定义设备，随后在需要 GPU 加速的地方调用 <code>xx.to(device)</code> 即可实现使用 GPU 加速。</p>
<p>定义设备处可以使用语法糖方式编写，比如：</p>
<p><code>device = torch.device('cuda') if torch.cuda.is_available() else 'cpu' </code></p>
<ul>
<li>
<p>完整代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> Model <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练的设备</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line"></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                          download=<span class="literal">True</span>)</span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                         download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line"><span class="comment"># 测试集长度</span></span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dataLoader 加载数据集</span></span><br><span class="line">train_dataLoader = DataLoader(dataset=train_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataLoader = DataLoader(dataset=test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建网络模型</span></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络模型使用 GPU 计算</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    myNet = myNet.to(device)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;cuda 无法使用&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 损失函数使用 GPU 计算</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    loss_fn = loss_fn.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = torch.optim.SGD(myNet.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 writer</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./test_logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-----第 &#123;&#125; 次训练开始-----&quot;</span>.<span class="built_in">format</span>(i + <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataLoader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练数据使用 GPU 计算</span></span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            imgs = imgs.to(device)</span><br><span class="line">            targets = targets.to(device)</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        output = myNet(imgs)</span><br><span class="line">        <span class="comment"># 损失计算</span></span><br><span class="line">        loss = loss_fn(output, targets)</span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_train_step += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            end_time = time.time()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;运行时间：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(end_time-start_time))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;训练次数：&#123;&#125;， loss:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_train_step, loss.item()))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试步骤</span></span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataLoader:</span><br><span class="line">            imgs, targets = data</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 测试数据使用 GPU 计算</span></span><br><span class="line">            <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">                imgs = imgs.to(device)</span><br><span class="line">                targets = targets.to(device)</span><br><span class="line"></span><br><span class="line">            output = myNet(imgs)</span><br><span class="line">            test_loss = loss_fn(output, targets)</span><br><span class="line">            total_test_loss += test_loss.item()</span><br><span class="line"></span><br><span class="line">            total_accuracy += (output.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;整体 test loos:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;整体 accuracy:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;test_loss&#x27;</span>, total_test_loss, total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;test_accuracy&#x27;</span>, total_accuracy, total_test_step)</span><br><span class="line">    total_test_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存模型</span></span><br><span class="line">    torch.save(myNet, <span class="string">&#x27;mynet_&#123;&#125;.pth&#x27;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    <span class="comment"># 方式2：torch.save(myNet.state_dict(), &#x27;mynet_&#123;&#125;.pth&#x27;.format(i))</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;模型已保存&#x27;</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h2 id="八、模型验证">八、模型验证</h2>
<p>模型训练并保存完成后，接下来就需要进行进一步的验证了，我们可以载入保存好的模型，并在网络上查找图片，让模型对图片进行分类，具体代码如下。</p>
<ul>
<li>
<p>图片 <code>img.png</code></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/07/141828-90162.png" alt=""></p>
</li>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> Model <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor,</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">imgPath = <span class="string">&#x27;./imgs/img.png&#x27;</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(imgPath)</span><br><span class="line"></span><br><span class="line">transform = torchvision.transforms.Compose([torchvision.transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">                                            torchvision.transforms.ToTensor()])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入模型, 从 GPU 映射到 CPU 上</span></span><br><span class="line">model = torch.load(<span class="string">&#x27;mynet_19.pth&#x27;</span>, map_location=torch.device(<span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图片处理</span></span><br><span class="line">img = transform(image)</span><br><span class="line"><span class="comment"># 输入图片是需要一个 batch_size 的，所以需要 reshape 一下</span></span><br><span class="line">img = torch.reshape(img, (<span class="number">1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># 这一步可以提高性能</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output = model(img)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;预测类别：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(dataset.classes[output.argmax(<span class="number">1</span>)]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行结果：  预测类别：dog</span></span><br></pre></td></tr></table></figure></li>
</ul>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne.gitee.io/kuhne.gitee.io/img/head.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne.gitee.io/kuhne.gitee.io/img/head.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">Kuhne</div><div class="post-copyright__author_desc">间接性内卷 持续性摆烂</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="https://kuhne.gitee.io/kuhne.gitee.io/posts/2609eb73.html">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('https://kuhne.gitee.io/kuhne.gitee.io/posts/2609eb73.html')">PyTorch 学习笔记</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="https://kuhne.gitee.io/kuhne.gitee.io/posts/2609eb73.html"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=PyTorch 学习笔记&amp;url=https://kuhne.gitee.io/kuhne.gitee.io/posts/2609eb73.html&amp;pic=https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/1cc2cc32c428a61ce3eb2f191668eb9c.jpeg" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="rm.copyPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kuhne.gitee.io/kuhne.gitee.io" target="_blank">Kuhne</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/tags/Python/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>Python<span class="tagsPageCount">6</span></a><a class="post-meta__box__tags" href="/tags/PyTorch/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>PyTorch<span class="tagsPageCount">9</span></a></div></div></div><div class="post_share"><div class="social-share" data-image="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2023/02/27/20230227141226-160f64.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/dc7d8d44.html"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">cs231n 学习笔记（8）经典的 CNN 架构</div></div></a></div><div class="next-post pull-right"><a href="/posts/3fc42b2c.html"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">cs231n 学习笔记（9）循环神经网络</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="anzhiyufont anzhiyu-icon-thumbs-up fa-fw" style="font-size: 1.5rem; margin-right: 4px"></i><span>喜欢这篇文章的人也看了</span></div><div class="relatedPosts-list"><div><a href="/posts/96074ef.html" title="Conda 指令总结"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/ef3cfeb23ed8a41b1c7e2e714c5a9b2f.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2022-07-28</div><div class="title">Conda 指令总结</div></div></a></div><div><a href="/posts/af5d191f.html" title="Matplotlib 基础教程"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/1cc2cc32c428a61ce3eb2f191668eb9c.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2022-05-03</div><div class="title">Matplotlib 基础教程</div></div></a></div><div><a href="/posts/da1a8524.html" title="Pandas基础教程"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/Pandas_Logo.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2022-05-02</div><div class="title">Pandas基础教程</div></div></a></div><div><a href="/posts/38d2ff3.html" title="Numpy基础教程"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/Numpy_Logo2.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2022-05-02</div><div class="title">Numpy基础教程</div></div></a></div><div><a href="/posts/99832f47.html" title="Python基础"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/c59ba07eefc154fd81aac8f18b4b5513.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2022-04-11</div><div class="title">Python基础</div></div></a></div><div><a href="/posts/427694ff.html" title="PyTorch 模型复现（7）：经典卷积神经网络（下）."><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/1f7216973a48d25e4cfad1f4563f68b7.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2022-08-13</div><div class="title">PyTorch 模型复现（7）：经典卷积神经网络（下）.</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="anzhiyufont anzhiyu-icon-comments"></i><span> 评论</span></div><div class="comment-randomInfo"><a onclick="anzhiyu.addRandomCommentInfo()" href="javascript:void(0)">匿名评论</a><a href="/privacy" style="margin-left: 4px">隐私政策</a></div><div class="comment-tips" id="comment-tips"><span>✅ 你无需删除空行，直接评论以获取最佳展示效果</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div><div class="comment-barrage"></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info__sayhi" id="author-info__sayhi" onclick="anzhiyu.changeSayHelloText()"></div><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne.gitee.io/kuhne.gitee.io/img/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-status"><img class="g-status" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://twikoo-magic.oss-cn-hangzhou.aliyuncs.com/bilibili2233/[2233娘_吐魂].png" ait="status"/></div></div><div class="author-info__description"><div style="line-height:1.38;margin:0.6rem 0;text-align:justify;color:rgba(255, 255, 255, 0.8);">一个菜鸟的垃圾加工厂。</div><div style="line-height:1.38;margin:0.6rem 0;text-align:justify;color:rgba(255, 255, 255, 0.8);">希望你你可以在这里找到对你有用的<b style="color:#fff">有用</b>的<b style="color:#fff">垃圾</b>。</div></div><div class="author-info__bottom-group"><a class="author-info__bottom-group-left" href="/"><h1 class="author-info__name">Kuhne</h1><div class="author-info__desc">间接性内卷 持续性摆烂</div></a><div class="card-info-social-icons is-center"><a class="social-icon faa-parent animated-hover" href="https://github.com/" target="_blank" title="Github"><i class="anzhiyufont anzhiyu-icon-github"></i></a><a class="social-icon faa-parent animated-hover" href="https://space.bilibili.com/" target="_blank" title="BiliBili"><i class="anzhiyufont anzhiyu-icon-bilibili"></i></a></div></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bullhorn anzhiyu-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来看我的博客~</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">PyTorch 学习笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81PyTorch-%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.1.</span> <span class="toc-text">一、PyTorch 介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Why-PyTorch"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 Why PyTorch ?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Numpy-or-Torch"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2 Numpy or Torch?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81PyTorch-%E5%9F%BA%E7%A1%80"><span class="toc-number">1.2.</span> <span class="toc-text">二、PyTorch 基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 加载数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-TensorBoard-%E4%BD%BF%E7%94%A8"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 TensorBoard 使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-TensorBoard-add-image-%E7%94%A8%E6%B3%95"><span class="toc-number">1.2.3.</span> <span class="toc-text">2.3 TensorBoard add_image() 用法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-transform-%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">1.2.4.</span> <span class="toc-text">2.3 transform 的使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E5%B8%B8%E8%A7%81%E7%9A%84-Transform"><span class="toc-number">1.2.5.</span> <span class="toc-text">2.4 常见的 Transform</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-torchvision-%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BD%BF%E7%94%A8"><span class="toc-number">1.2.6.</span> <span class="toc-text">2.4 torchvision 数据集使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-DataLoader-%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">1.2.7.</span> <span class="toc-text">2.5 DataLoader 的使用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">1.3.</span> <span class="toc-text">三、神经网络结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-PyTorch-%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 PyTorch 搭建神经网络介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 搭建自己的神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-PyTorch-%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.3 PyTorch 卷积操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">1.3.4.</span> <span class="toc-text">3.4 卷积层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">1.3.5.</span> <span class="toc-text">3.5 池化层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.6.</span> <span class="toc-text">3.6 非线性激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-7-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">1.3.7.</span> <span class="toc-text">3.7 批量归一化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-8-%E7%BA%BF%E6%80%A7%E5%B1%82"><span class="toc-number">1.3.8.</span> <span class="toc-text">3.8 线性层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-9-Dropout-%E5%B1%82"><span class="toc-number">1.3.9.</span> <span class="toc-text">3.9 Dropout 层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-10-Sequential"><span class="toc-number">1.3.10.</span> <span class="toc-text">3.10 Sequential</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E5%8D%95%E6%90%AD%E5%BB%BA"><span class="toc-number">1.4.</span> <span class="toc-text">四、神经网络简单搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-CIFAR-10-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 CIFAR-10 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA"><span class="toc-number">1.4.2.</span> <span class="toc-text">4.2 网络搭建</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%8F%8A%E4%BC%98%E5%8C%96"><span class="toc-number">1.5.</span> <span class="toc-text">五、损失函数、反向传播及优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.5.1.</span> <span class="toc-text">5.1 损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.5.2.</span> <span class="toc-text">5.2 反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">1.5.3.</span> <span class="toc-text">5.3 优化器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8"><span class="toc-number">1.6.</span> <span class="toc-text">六、模型使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E7%8E%B0%E6%9C%89%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">1.6.1.</span> <span class="toc-text">6.1 现有模型使用方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96"><span class="toc-number">1.6.2.</span> <span class="toc-text">6.2 模型的保存与读取</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81-%E5%AE%8C%E6%95%B4%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">1.7.</span> <span class="toc-text">七、 完整的模型训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%AE%8C%E6%95%B4%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.7.1.</span> <span class="toc-text">7.1 训练模型完整步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-%E4%BD%BF%E7%94%A8-GPU-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.7.2.</span> <span class="toc-text">7.2 使用 GPU 训练模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81"><span class="toc-number">1.8.</span> <span class="toc-text">八、模型验证</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/8d037627.html" title="Git 及 GitHub 常用命令记录"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2023/02/27/20230227141226-160f64.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Git 及 GitHub 常用命令记录"/></a><div class="content"><a class="title" href="/posts/8d037627.html" title="Git 及 GitHub 常用命令记录">Git 及 GitHub 常用命令记录</a><time datetime="2023-02-27T06:08:31.000Z" title="发表于 2023-02-27 14:08:31">2023-02-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/a6d8a9c3.html" title="个人常用 Linux 命令"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2023/02/26/20230226105141-94929b.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="个人常用 Linux 命令"/></a><div class="content"><a class="title" href="/posts/a6d8a9c3.html" title="个人常用 Linux 命令">个人常用 Linux 命令</a><time datetime="2023-02-26T02:47:48.000Z" title="发表于 2023-02-26 10:47:48">2023-02-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/3b91aa4a.html" title="Linux 服务器环境配置"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/4c417afa91736428def17dcc96aa64b8.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Linux 服务器环境配置"/></a><div class="content"><a class="title" href="/posts/3b91aa4a.html" title="Linux 服务器环境配置">Linux 服务器环境配置</a><time datetime="2022-09-14T08:01:29.000Z" title="发表于 2022-09-14 16:01:29">2022-09-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/165b6b1f.html" title="论文整理（3）Measuring Compositional Consistency for Video Question Answering"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/top2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文整理（3）Measuring Compositional Consistency for Video Question Answering"/></a><div class="content"><a class="title" href="/posts/165b6b1f.html" title="论文整理（3）Measuring Compositional Consistency for Video Question Answering">论文整理（3）Measuring Compositional Consistency for Video Question Answering</a><time datetime="2022-09-03T07:28:11.000Z" title="发表于 2022-09-03 15:28:11">2022-09-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/22673bc3.html" title="论文学习笔记整理"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/dknjyLgQcEXhsuK.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文学习笔记整理"/></a><div class="content"><a class="title" href="/posts/22673bc3.html" title="论文学习笔记整理">论文学习笔记整理</a><time datetime="2022-08-24T05:50:52.000Z" title="发表于 2022-08-24 13:50:52">2022-08-24</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div id="footer_deal"><a class="deal_link" href="mailto:kunhaofu1@gmail.com" title="email"><i class="anzhiyufont anzhiyu-icon-envelope"></i></a><a class="deal_link" href="/atom.xml" title="RSS"><i class="anzhiyufont anzhiyu-icon-rss"></i></a><img class="footer_mini_logo" title="返回顶部" alt="返回顶部" onclick="anzhiyu.scrollToDest(0, 500)" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne.gitee.io/kuhne.gitee.io/img/head.jpg" size="50px"/><a class="deal_link" target="_blank" rel="noopener" href="https://github.com/kuhne12" title="Github"><i class="anzhiyufont anzhiyu-icon-github"></i></a><a class="deal_link" href="/copyright" title="CC"><i class="anzhiyufont anzhiyu-icon-copyright-line"></i></a></div><div id="anzhiyu-footer"><div class="footer-group"><div class="footer-title">服务</div><div class="footer-links"><a class="footer-item" title="等待开发" href="/wait/">等待开发</a></div></div><div class="footer-group"><div class="footer-title">主题</div><div class="footer-links"><a class="footer-item" title="文档" href="/docs/">文档</a></div></div><div class="footer-group"><div class="footer-title">导航</div><div class="footer-links"><a class="footer-item" title="等待开发" href="/wait/">等待开发</a></div></div><div class="footer-group"><div class="footer-title">协议</div><div class="footer-links"><a class="footer-item" title="隐私协议" href="/privacy/">隐私协议</a></div></div></div><div id="workboard"><img class="workSituationImg boardsign" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2023/09/07/20230907103822-7fe708.svg" alt="距离月入25k也就还差一个大佬带我~" title="距离月入25k也就还差一个大佬带我~"/><div id="runtimeTextTip"></div></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v5.4.0" title="博客框架为Hexo_v5.4.0"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.1.5/img/badge/Frame-Hexo.svg" alt="博客框架为Hexo_v5.4.0"/></a><a class="github-badge" target="_blank" href="https://blog.anheyu.com/" style="margin-inline:5px" data-title="本站使用AnZhiYu主题" title="本站使用AnZhiYu主题"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.9/img/Theme-AnZhiYu-2E67D3.svg" alt="本站使用AnZhiYu主题"/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title="本站项目由Github托管"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.1.5/img/badge/Source-Github.svg" alt="本站项目由Github托管"/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.2.0/img/badge/Copyright-BY-NC-SA.svg" alt="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"/></a><a class="github-badge" target="_blank" href="https://beian.miit.gov.cn/" style="margin-inline:5px" data-title="本站已在浙备案" title="本站已在浙备案"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2023/09/07/20230907104640-663234.svg" alt="本站已在浙备案"/></a></p></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2020 - 2023 By <a class="footer-bar-link" href="/" title="Kuhne" target="_blank">Kuhne</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://beian.miit.gov.cn/" title="浙ICP备2023005409号-1">浙ICP备2023005409号-1</a><a class="footer-bar-link cc" href="/copyright" title="cc协议"><i class="anzhiyufont anzhiyu-icon-copyright-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-by-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-nc-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-nd-line"></i></a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">74</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">30</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">8</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" href="/null" title="暂无"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="暂无"/><span class="back-menu-item-text">暂无</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/?id=883682303&amp;server=netease&amp;type=playlist"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li></ul></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/essay/"><span> 闲言碎语</span></a></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/2022CVPR/" style="font-size: 0.88rem;">2022CVPR<sup>2</sup></a><a href="/tags/Git/" style="font-size: 0.88rem;">Git<sup>1</sup></a><a href="/tags/Kaggle/" style="font-size: 0.88rem;">Kaggle<sup>2</sup></a><a href="/tags/Linux/" style="font-size: 0.88rem;">Linux<sup>2</sup></a><a href="/tags/PyTorch/" style="font-size: 0.88rem;">PyTorch<sup>9</sup></a><a href="/tags/Python/" style="font-size: 0.88rem;">Python<sup>6</sup></a><a href="/tags/Pytorch/" style="font-size: 0.88rem;">Pytorch<sup>1</sup></a><a href="/tags/SpringCloud/" style="font-size: 0.88rem;">SpringCloud<sup>9</sup></a><a href="/tags/Vue/" style="font-size: 0.88rem;">Vue<sup>3</sup></a><a href="/tags/pycharm/" style="font-size: 0.88rem;">pycharm<sup>1</sup></a><a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 0.88rem; font-weight: 500; color: var(--anzhiyu-lighttext)">人工智能<sup>21</sup></a><a href="/tags/%E4%BD%9C%E4%B8%9A/" style="font-size: 0.88rem;">作业<sup>8</sup></a><a href="/tags/%E5%8A%A0%E5%AF%86/" style="font-size: 0.88rem;">加密<sup>1</sup></a><a href="/tags/%E5%90%8E%E7%AB%AF/" style="font-size: 0.88rem;">后端<sup>2</sup></a><a href="/tags/%E5%9F%BA%E7%A1%80/" style="font-size: 0.88rem;">基础<sup>12</sup></a><a href="/tags/%E5%B7%A5%E5%85%B7/" style="font-size: 0.88rem;">工具<sup>3</sup></a><a href="/tags/%E5%BD%92%E7%BA%B3/" style="font-size: 0.88rem;">归纳<sup>3</sup></a><a href="/tags/%E6%95%99%E7%A8%8B/" style="font-size: 0.88rem;">教程<sup>10</sup></a><a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 0.88rem;">服务器<sup>1</sup></a><a href="/tags/%E6%9C%AA%E5%AE%8C%E6%88%90/" style="font-size: 0.88rem;">未完成<sup>1</sup></a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">机器学习<sup>21</sup></a><a href="/tags/%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0/" style="font-size: 0.88rem;">模型复现<sup>8</sup></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem; font-weight: 500; color: var(--anzhiyu-lighttext)">深度学习<sup>15</sup></a><a href="/tags/%E7%BD%91%E9%A1%B5%E5%89%8D%E7%AB%AF/" style="font-size: 0.88rem;">网页前端<sup>4</sup></a><a href="/tags/%E8%80%83%E7%A0%94/" style="font-size: 0.88rem;">考研<sup>1</sup></a><a href="/tags/%E8%A7%86%E8%A7%89%E5%85%B3%E7%B3%BB%E6%8E%A2%E6%B5%8B/" style="font-size: 0.88rem;">视觉关系探测<sup>1</sup></a><a href="/tags/%E8%A7%86%E8%A7%89%E9%97%AE%E7%AD%94/" style="font-size: 0.88rem;">视觉问答<sup>1</sup></a><a href="/tags/%E8%A7%86%E9%A2%91%E9%97%AE%E7%AD%94/" style="font-size: 0.88rem;">视频问答<sup>1</sup></a><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" style="font-size: 0.88rem;">计算机视觉<sup>12</sup></a><a href="/tags/%E8%AE%BA%E6%96%87/" style="font-size: 0.88rem;">论文<sup>4</sup></a></div></div><hr/></div></div><div id="keyboard-tips"><div class="keyboardTitle">博客快捷键</div><div class="keybordList"><div class="keybordItem"><div class="keyGroup"><div class="key">shift K</div></div><div class="keyContent"><div class="content">关闭快捷键功能</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift A</div></div><div class="keyContent"><div class="content">打开/关闭中控台</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift M</div></div><div class="keyContent"><div class="content">播放/暂停音乐</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift D</div></div><div class="keyContent"><div class="content">深色/浅色显示模式</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift S</div></div><div class="keyContent"><div class="content">站内搜索</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift R</div></div><div class="keyContent"><div class="content">随机访问</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift H</div></div><div class="keyContent"><div class="content">返回首页</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift F</div></div><div class="keyContent"><div class="content">友链鱼塘</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift L</div></div><div class="keyContent"><div class="content">友链页面</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift P</div></div><div class="keyContent"><div class="content">关于本站</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift I</div></div><div class="keyContent"><div class="content">原版/本站右键菜单</div></div></div></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="anzhiyufont anzhiyu-icon-comments"></i></a><a id="switch-commentBarrage" href="javascript:anzhiyu.switchCommentBarrage();" title="开关弹幕"><i class="anzhiyufont anzhiyu-icon-danmu"></i></a><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="60198" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random"></meting-js></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="anzhiyufont anzhiyu-icon-xmark"></i></button></nav><div class="is-center" id="loading-database"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-pulse-icon"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://music.163.com/#/playlist&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div id="he-plugin-simple"></div><script>var WIDGET = {
  "CONFIG": {
    "modules": "0124",
    "background": "2",
    "tmpColor": "FFFFFF",
    "tmpSize": "16",
    "cityColor": "FFFFFF",
    "citySize": "16",
    "aqiColor": "E8D87B",
    "aqiSize": "16",
    "weatherIconSize": "24",
    "alertIconSize": "18",
    "padding": "10px 10px 10px 10px",
    "shadow": "0",
    "language": "auto",
    "borderRadius": "20",
    "fixed": "true",
    "vertical": "top",
    "horizontal": "left",
    "left": "20",
    "top": "7.1",
    "key": "df245676fb434a0691ead1c63341cd94"
  }
}
</script><link rel="stylesheet" href="https://widget.qweather.net/simple/static/css/he-simple.css?v=1.4.0"/><script src="https://widget.qweather.net/simple/static/js/he-simple.js?v=1.4.0"></script><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.4/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script async src="/anzhiyu/random.js"></script><script async="async">(function () {
  var grt = new Date("04/01/2021 00:00:00"); //设置网站上线时间
  var now = new Date();
  var dnum;
  var hnum;
  var mnum;
  var snum;
  var nowHour;

  // 计算并更新天数、小时数、分钟数和秒数
  function updateTime() {
    now = new Date(); // 更新 now 的值
    nowHour = now.getHours(); // 更新 nowHour 的值
    var days = (now - grt) / 1000 / 60 / 60 / 24;
    dnum = Math.floor(days);
    var hours = (now - grt) / 1000 / 60 / 60 - 24 * dnum;
    hnum = Math.floor(hours);
    if (String(hnum).length == 1) {
      hnum = "0" + hnum;
    }
    var minutes = (now - grt) / 1000 / 60 - 24 * 60 * dnum - 60 * hnum;
    mnum = Math.floor(minutes);
    if (String(mnum).length == 1) {
      mnum = "0" + mnum;
    }
    var seconds = (now - grt) / 1000 - 24 * 60 * 60 * dnum - 60 * 60 * hnum - 60 * mnum;
    snum = Math.round(seconds);
    if (String(snum).length == 1) {
      snum = "0" + snum;
    }
  }

  // 更新网页中显示的网站运行时间
  function updateHtml() {
    const footer = document.getElementById("footer");
    if (!footer) return
    let currentTimeHtml = "";
    if (nowHour < 18 && nowHour >= 9) {
      // 如果是上班时间，默认就是"安知鱼-上班摸鱼中.svg"图片，不需要更改
      currentTimeHtml = `本站居然运行了 ${dnum} 天<span id='runtime'> ${hnum} 小时 ${mnum} 分 ${snum} 秒 </span><i class='anzhiyufont anzhiyu-icon-heartbeat' style='color:red'></i>`;
    } else {
      // 如果是下班时间，插入"安知鱼-下班啦.svg"图片
      let img = document.querySelector("#workboard .workSituationImg");
      img.src = "https://npm.elemecdn.com/anzhiyu-blog@2.0.4/img/badge/安知鱼-下班啦.svg";
      img.title = "下班了就该开开心心的玩耍，嘿嘿~";
      img.alt = "下班了就该开开心心的玩耍，嘿嘿~";

      currentTimeHtml = `本站居然运行了 ${dnum} 天<span id='runtime'> ${hnum} 小时 ${mnum} 分 ${snum} 秒 </span><i class='anzhiyufont anzhiyu-icon-heartbeat' style='color:red'></i>`;
    }

    if (document.getElementById("runtimeTextTip")) {
      document.getElementById("runtimeTextTip").innerHTML = currentTimeHtml;
    }
  }

  setInterval(() => {
    updateTime();
    updateHtml();
  }, 1000);
})();</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.cbd.int/katex@0.16.0/dist/katex.min.css"><script src="https://cdn.cbd.int/katex@0.16.0/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    anzhiyu.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://twikoo-api-pearl.vercel.app',
      region: '',
      onCommentLoaded: () => {
        anzhiyu.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') setTimeout(runFn,0)
    else getScript('https://cdn.cbd.int/twikoo@1.6.18/dist/twikoo.all.min.js').then(runFn)
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://twikoo-api-pearl.vercel.app',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const runFn = () => {
    init();
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) anzhiyu.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else {
      loadTwikoo()
    }
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script><input type="hidden" name="page-type" id="page-type" value="post"><script async src="/js/anzhiyu/comment_barrage.js"></script></div><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getComment = () => {
    const runTwikoo = () => {
      twikoo.getRecentComments({
        envId: 'https://twikoo-api-pearl.vercel.app',
        region: '',
        pageSize: 6,
        includeReply: true
      }).then(function (res) {
        const twikooArray = res.map(e => {
          return {
            'content': changeContent(e.comment),
            'avatar': e.avatar,
            'nick': e.nick,
            'url': e.url + '#' + e.id,
            'date': new Date(e.created).toISOString()
          }
        })

        saveToLocal.set('twikoo-newest-comments', JSON.stringify(twikooArray), 10/(60*24))
        generateHtml(twikooArray)
      }).catch(function (err) {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.textContent= "无法获取评论，请确认相关配置是否正确"
      })
    }

    if (typeof twikoo === 'object') {
      runTwikoo()
    } else {
      getScript('https://cdn.cbd.int/twikoo@1.6.18/dist/twikoo.all.min.js').then(runTwikoo)
    }
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'data-lazy-src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'><div class='name'><span>${array[i].nick} </span></div></a>`
        }
        
        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <time datetime="${array[i].date}">${anzhiyu.diffDate(array[i].date, true)}</time></div>
        </div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('twikoo-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script>var visitorMail = "visitor@anheyu.com";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><script src="/js/anzhiyu/right_click_menu.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["meta[property=\"og:image\"]","meta[property=\"og:title\"]","meta[property=\"og:url\"]","meta[property=\"og:type\"]","meta[property=\"og:site_name\"]","meta[property=\"og:description\"]","head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>
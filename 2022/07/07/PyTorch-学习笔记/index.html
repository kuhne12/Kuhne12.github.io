<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>PyTorch 学习笔记 | Kuhne</title><meta name="keywords" content="python,PyTorch,框架"><meta name="author" content="Kuhne"><meta name="copyright" content="Kuhne"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="记录 Pytorch 的入门使用">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch 学习笔记">
<meta property="og:url" content="https://kuhne.gitee.io/kuhne.gitee.io/2022/07/07/PyTorch-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Kuhne">
<meta property="og:description" content="记录 Pytorch 的入门使用">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/1cc2cc32c428a61ce3eb2f191668eb9c.jpeg">
<meta property="article:published_time" content="2022-07-07T06:31:48.000Z">
<meta property="article:modified_time" content="2022-07-07T07:09:48.000Z">
<meta property="article:author" content="Kuhne">
<meta property="article:tag" content="python">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="框架">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/1cc2cc32c428a61ce3eb2f191668eb9c.jpeg"><link rel="shortcut icon" href="/img/head2.png"><link rel="canonical" href="https://kuhne.gitee.io/kuhne.gitee.io/2022/07/07/PyTorch-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/6.0.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.staticfile.org/node-snackbar/0.1.9/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn1.tianli0.top/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://jsdelivr.pai233.top/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://jsdelivr.pai233.top/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'PyTorch 学习笔记',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2022-07-07 15:09:48'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/resource/JetBrainsMono-Medium.woff2"><link rel="stylesheet" href="/kuhne.gitee.io/css/index.min.css"><meta name="generator" content="Hexo 5.4.2"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "/img/lazyload.gif" data-lazy-src="/img/head.jpg" onerror="onerror=null;src='/img/head.jpg'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">74</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">44</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-archive"></i><span> Blog</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-list-ul"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/pictures/"><i class="fa-fw fa fa-camera"></i><span> 图片</span></a></li><li><a class="site-page child" href="/talk/"><i class="fa-fw fas fa-comment-dots"></i><span> 说说</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/1cc2cc32c428a61ce3eb2f191668eb9c.jpeg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Kuhne</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-archive"></i><span> Blog</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-list-ul"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/pictures/"><i class="fa-fw fa fa-camera"></i><span> 图片</span></a></li><li><a class="site-page child" href="/talk/"><i class="fa-fw fas fa-comment-dots"></i><span> 说说</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">PyTorch 学习笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-07-07T06:31:48.000Z" title="发表于 2022-07-07 14:31:48">2022-07-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-07-07T07:09:48.000Z" title="更新于 2022-07-07 15:09:48">2022-07-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Python/">Python</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">11.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>50分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="PyTorch 学习笔记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>PyTorch 学习笔记</h1>
<h2 id="一、PyTorch-介绍">一、PyTorch 介绍</h2>
<h3 id="1-1-Why-PyTorch">1.1 Why PyTorch ?</h3>
<p>PyTorch 是 Torch 在 Python 上的衍生，Torch 是一个使用 Lua 语言的神经网络库，但是 Lua 不是特别流行，所以 Touch 的开发团队就将其移植到了更流行的 Python 语言上。</p>
<p>下面是一些使用 PyTorch 的企业和机构：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/06/28/191019-1019b.png" alt=""></p>
<p>PyTorch 搭建的计算图是动态的，并不是首先搭建静态的图中，再将数据放入静态图中去。PyTorch 对比静态的 Tensorflow，他能更有效地处理一些问题， 比如说 RNN 变化时间长度的输出。</p>
<h3 id="1-2-Numpy-or-Torch">1.2 Numpy or Torch?</h3>
<p>Torch 自称是神经网络界的 Numpy，因为它能将 torch 生产的 tensor （张量），放在 GPU 中加速运算，就像 Numpy 会把 array 放入 CPU 中加速运算一样。</p>
<p>之前一直习惯于用 Numpy 进行编程也不必担心，PyTorch 和 Numpy 能很好地兼容，这样就能自由地转换 numpy array 和 torch tensor 了。</p>
<p>下面是一段示例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np_data = np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="comment"># numpy 数据转为 tensor</span></span><br><span class="line">torch_data = torch.from_numpy(np_data)</span><br><span class="line"><span class="comment"># tensor 数据转为 numpy 数组</span></span><br><span class="line">tensor2array = torch_data.numpy()</span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nnumpy&#x27;</span>, np_data,</span><br><span class="line">    <span class="string">&#x27;\ntorch&#x27;</span>, torch_data,</span><br><span class="line">    <span class="string">&#x27;\nnumpy&#x27;</span>, tensor2array</span><br><span class="line"></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">numpy [[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span> <span class="number">5</span>]] </span><br><span class="line">torch tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]], dtype=torch.int32) </span><br><span class="line">numpy [[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span> <span class="number">5</span>]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="二、PyTorch-基础">二、PyTorch 基础</h2>
<h3 id="2-1-加载数据">2.1 加载数据</h3>
<p>在 PyTorch 中，读取数据主要涉及 2 个类，分别是：</p>
<ul>
<li><strong>Dataset</strong>
<ul>
<li>用于获取数据及其对应 label
<ul>
<li>获取每一个数据及其 label</li>
<li>告知总共有多少数据</li>
</ul>
</li>
</ul>
</li>
<li><strong>Dataloader</strong>
<ul>
<li>为网络提供不同的数据形式</li>
</ul>
</li>
</ul>
<p>下面是一个创建数据读取对象的例子：</p>
<ul>
<li>
<p>代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyData</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="comment"># 初始化，创建实例时会运行该函数</span></span><br><span class="line">    <span class="comment"># 主要内容是为 class 提供全局变量</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, label_dir</span>):</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.label_dir = label_dir</span><br><span class="line">        self.path = os.path.join(self.root_dir, self.label_dir)</span><br><span class="line">        self.img_path = os.listdir(self.path)  <span class="comment"># 获得所有图片的路径</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 所有子类都必须重写 _getitem__() 方法，用于获取数据样本并给与对应的标签</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img_name = self.img_path[idx]</span><br><span class="line">        img_item_path = os.path.join(self.root_dir, self.label_dir, img_name)</span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_item_path)</span><br><span class="line">        label = self.label_dir</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 所有子类也必须重写 __len__() 方法，返回数据集长度</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root_dir = <span class="string">&quot;Dataset\\train&quot;</span></span><br><span class="line">ants_label = <span class="string">&quot;ants&quot;</span></span><br><span class="line">bees_label = <span class="string">&quot;bees&quot;</span></span><br><span class="line"><span class="comment"># 创建数据集实例</span></span><br><span class="line">ants_dataset = MyData(root_dir, ants_label)</span><br><span class="line">bees_dataset = MyData(root_dir, bees_label)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拼接数据集</span></span><br><span class="line">train_dataset = ants_dataset + bees_dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图片选择</span></span><br><span class="line">img, <span class="built_in">len</span> = bees_dataset[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 图片展示</span></span><br><span class="line">img.show()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>结果</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/02/203638-49a0b.png" alt=""></p>
</li>
</ul>
<p>本次使用的数据集共有两个 <code>label</code>，分别是 <code>ants</code> 和 <code>bees</code>，每一张图片都对应一个同名的 <code>txt</code> 文件，<code>txt</code> 文件内容是对应文件名图像的 <code>label </code>。</p>
<h3 id="2-2-TensorBoard-使用">2.2 TensorBoard 使用</h3>
<p>TensorBoard 是 PyTorch 中用于数据可视化的包，它的存在对于模型训练很有帮助，可以可视化模型在不同阶段的状态。</p>
<p>首先需要导入相应的包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br></pre></td></tr></table></figure>
<p>学习一个包的第一步一般是查看相应的 <code>help()</code> ，在 PyCharm 中可以直接进入类中寻找帮助信息：</p>
<blockquote>
<p>“”&quot;</p>
<p>Writes entries directly to event files in the log_dir to be consumed by TensorBoard.</p>
<p>The <code>SummaryWriter</code> class provides a high-level API to create an event file in a given directory and add summaries and events to it. The class updates the file contents asynchronously. This allows a training program to call methods to add data to the file directly from the training loop, without slowing down training.</p>
<p>“”&quot;</p>
</blockquote>
<p>了解到该类会向 <code>log_dir</code> 文件夹写入<strong>事件文件</strong>，该文件可以被 TensorBoard 解析。查看初始化方法中的介绍内容：</p>
<blockquote>
<p>“”&quot;Creates a <code>SummaryWriter</code> that will write out events and summaries<br>
to the event file.</p>
<p>Args:<br>
<strong>log_dir (string):</strong> Save directory location. Default is<br>
runs/CURRENT_DATETIME_HOSTNAME, which changes after each run.<br>
Use hierarchical folder structure to compare<br>
between runs easily. e.g. pass in ‘runs/exp1’, ‘runs/exp2’, etc.<br>
for each new experiment to compare across them.<br>
<strong>comment (string):</strong> Comment log_dir suffix appended to the default<br>
<code>log_dir</code>. If <code>log_dir</code> is assigned, this argument has no effect.<br>
······</p>
<p>“”&quot;</p>
</blockquote>
<p>这里面介绍了初始化该类的参数信息，<code>log_dir</code> 参数是一个 String ，内容是用于保存事件文件夹的路径，其他的参数用的不多。</p>
<p>下面是一个具体的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化实例，传入事件文件夹路径</span></span><br><span class="line">writter = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加标量</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="comment"># 参数1：指定图像名称</span></span><br><span class="line">    <span class="comment"># 参数2：指定 x</span></span><br><span class="line">    <span class="comment"># 参数3：指定 y</span></span><br><span class="line">    <span class="comment"># 这里相当于绘制了一个 y=x 的图像</span></span><br><span class="line">    writter.add_scalar(<span class="string">&quot;y=x&quot;</span>, i, i)</span><br><span class="line"></span><br><span class="line">writter.close()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里报错了需要下载 tensorboard，代码 <code>pip install tensorboard</code></p>
<p>version 报错需要下载 <code>pip install setuptools==59.5.0</code></p>
</blockquote>
<p>运行成功后，目录中会新增一个 <code>log</code> 文件夹，可以在 PyCharm 中的 Terminal 中打开生成的文件，输入代码：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=logs	# logdir=事件文件夹名</span><br></pre></td></tr></table></figure>
<p>随后打开生成的<a target="_blank" rel="noopener" href="http://localhost:6006/#scalars">网址</a>，就可以成功启动 TensorBoard 了</p>
<p>为了避免端口冲突，可以在命令后面加上端口信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=logs --port=6007</span><br></pre></td></tr></table></figure>
<p>现在需要打开这个网址：<a target="_blank" rel="noopener" href="http://localhost:6007/%EF%BC%8C%E6%89%93%E5%BC%80%E5%90%8E%E5%B0%B1%E5%BE%97%E5%88%B0%E4%BA%86%E5%AF%B9%E5%BA%94%E7%9A%84%E5%9B%BE%E5%83%8F">http://localhost:6007/，打开后就得到了对应的图像</a></p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/111650-c2031.png" alt=""></p>
<p>在 TensorBoard 中，可以对数据图像进行缩放、坐标轴变换，鼠标移动到图像上会显示坐标位置。</p>
<p>同理，也可以绘制 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mn>2</mn><mi>x</mi></mrow><annotation encoding="application/x-tex">y=2x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span><span class="mord mathnormal">x</span></span></span></span> 的函数图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化实例，传入事件文件夹路径</span></span><br><span class="line">writter = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加标量</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writter.add_scalar(<span class="string">&quot;y=2x&quot;</span>, <span class="number">2</span>*i, i)</span><br><span class="line"></span><br><span class="line">writter.close()</span><br></pre></td></tr></table></figure>
<p>运行成功后刷新网页，得到这样的结果</p>
<img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/114128-ddf85.png" style="zoom:67%;" />
<p>如果没有修改图像名称，只修改了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span> 的信息，就会出现错误</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化实例，传入事件文件夹路径</span></span><br><span class="line">writter = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加标量</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="comment"># 注意这里是 y=x</span></span><br><span class="line">    writter.add_scalar(<span class="string">&quot;y=x&quot;</span>, <span class="number">2</span>*i, i)</span><br><span class="line"></span><br><span class="line">writter.close()</span><br></pre></td></tr></table></figure>
<img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/114850-ba1fa.png" style="zoom:67%;" />
<p>出现这种情况就需要将事件文件夹进行删除，随后重新绘制图像</p>
<h3 id="2-3-TensorBoard-add-image-用法">2.3 TensorBoard add_image() 用法</h3>
<p>与 <code>writter.add_scalar()</code> 相同，使用 <code>writter.add_image()</code> 之前，我们需要进入该方法查看它的使用介绍：</p>
<blockquote>
<p>“”&quot;Add image data to summary.</p>
<p>Note that this requires the <code>pillow</code> package.</p>
<p>Args:<br>
<strong>tag (string):</strong> Data identifier<br>
<strong>img_tensor (torch.Tensor, numpy.array, or string/blobname):</strong> Image data<br>
<strong>global_step (int):</strong> Global step value to record<br>
<strong>walltime (float):</strong> Optional override default walltime (time.time())<br>
seconds after epoch of event<br>
Shape:<br>
img_tensor: Default is :math:<code>(3, H, W)</code>. You can use <code>torchvision.utils.make_grid()</code> to<br>
convert a batch of tensor into 3xHxW format or call <code>add_images</code> and let us do the job.<br>
Tensor with :math:<code>(1, H, W)</code>, :math:<code>(H, W)</code>, :math:<code>(H, W, 3)</code> is also suitable as long as<br>
corresponding <code>dataformats</code> argument is passed, e.g. <code>CHW</code>, <code>HWC</code>, <code>HW</code>.</p>
<p>“”&quot;</p>
</blockquote>
<p>这里指出了使用该方法需要的参数列表，之前使用的 <code>Image </code> 类生成的图像类型并不满足要求（必须是<code>torch.Tensor</code>, <code>numpy.array</code>, or <code>string/blobname</code> 类型），所以必须换一种图像格式，比如使用 <code>numpy.array</code>类型，同时也指定了输入数据的 shape，只有满足了指定的 shape 才能成功运行。-</p>
<ul>
<li>
<p>代码编写：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化实例，传入事件文件夹路径</span></span><br><span class="line">writter = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"><span class="comment"># 图像路径</span></span><br><span class="line">image_path = <span class="string">&#x27;./Dataset/train/ants_image/0013035.jpg&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取图像</span></span><br><span class="line">img_PIL = Image.<span class="built_in">open</span>(image_path)</span><br><span class="line"><span class="comment"># 转化为 numpy.array 格式</span></span><br><span class="line">img_array = numpy.array(img_PIL)</span><br><span class="line"><span class="comment"># 添加图像,参数1: 名称，参数2: 图像，参数3: global_step</span></span><br><span class="line">writter.add_image(<span class="string">&#x27;test&#x27;</span>, img_array, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">writter.close()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>运行后会发现抱错，原因是 shape 不匹配，numpy 格式的图片的 shape 是（H, W, 3），但是 <code>writter.add_image()</code> 默认的 shape 是 （3, H, W），如果需要指定成（H, W, 3），需要修改 <code>dataformats</code> 属性为 <code>HWC</code></p>
<ul>
<li>
<p>修改后代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">writter.add_image(<span class="string">&#x27;test&#x27;</span>, img_array, <span class="number">1</span>, ataformats=<span class="string">&#x27;HWC&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>修改后就能成功运行程序。</p>
</li>
<li>
<p>运行结果</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/133740-62958.png" alt=""></p>
</li>
</ul>
<p>如果需要添加图片，仅需修改图片路径和对应代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">image_path = <span class="string">&#x27;./Dataset/train/bees_image/16838648_415acd9e3f.jpg&#x27;</span></span><br><span class="line"><span class="comment"># 这里改成了 2 </span></span><br><span class="line">writter.add_image(<span class="string">&#x27;test&#x27;</span>, img_array, <span class="number">2</span>, dataformats=<span class="string">&#x27;HWC&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>输出结果</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/134155-0e5ca.png" alt=""></p>
</li>
</ul>
<p>由于未修改图像名称，是在同一片区域下现实的，所以需要拖动上方的滑块，如果希望在不同区域显示，则需要修改标题名称：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">image_path = <span class="string">&#x27;./Dataset/train/bees_image/95238259_98470c5b10.jpg&#x27;</span></span><br><span class="line">writter.add_image(<span class="string">&#x27;train&#x27;</span>, img_array, <span class="number">1</span>, dataformats=<span class="string">&#x27;HWC&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>输出结果</p>
<img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/134437-5782b.png" style="zoom:67%;" />
</li>
</ul>
<p>通过这种方式，可以很直观地了解到我们给模型的数据。</p>
<h3 id="2-3-transform-的使用">2.3 transform 的使用</h3>
<p>PyTorch 中的 transform 包主要是用来预处理图片，导入方式法为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br></pre></td></tr></table></figure>
<p>我们可以进入该包，可以看到包内定义了很多类</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/135408-6e618.png" alt=""></p>
<p>比如 <code>Compose</code>类，作用是对图片做<strong>中心裁剪</strong>，随后返回一个对应的 tensor。</p>
<p>但是最常用的类是 <code>ToTensor</code> 类，该类可以将 <code>PIL Image</code> 或 <code>numpy.ndarray</code> 转化为 tensor。</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&quot;Dataset/train/ants_image/0013035.jpg&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># img 类型：&lt;class &#x27;PIL.JpegImagePlugin.JpegImageFile&#x27;&gt;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化对象</span></span><br><span class="line">tensor_trans = transforms.ToTensor()</span><br><span class="line"><span class="comment"># 相当于调用了 ToTensor 的 __call__ 方法</span></span><br><span class="line"><span class="comment"># tensor_img 类型：&lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br><span class="line">tensor_img = tensor_trans(img)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>查看 <code>tensor_img</code> 的内容，里面封装了一些反向传播所需要的数据：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/142552-4bc19.png" alt=""></p>
<p>有了 tensor 格式的数据，就可以直接使用 <code>writter.add_image()</code> 方法绘制图像了</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&quot;Dataset/train/ants_image/0013035.jpg&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># img 类型：&lt;class &#x27;PIL.JpegImagePlugin.JpegImageFile&#x27;&gt;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化对象</span></span><br><span class="line">tensor_trans = transforms.ToTensor()</span><br><span class="line">tensor_img = tensor_trans(img)</span><br><span class="line"></span><br><span class="line">writer.add_image(<span class="string">&quot;Tensor_img&quot;</span>, tensor_img)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>输出结果</p>
<img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/143724-bf6ae.png" style="zoom:67%;" />
</li>
</ul>
<h3 id="2-4-常见的-Transform">2.4 常见的 Transform</h3>
<ul>
<li>
<p><strong>ToTensor</strong></p>
<p>ToTensor 的功能已经在上方解释过了，不再赘述</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;image/4f8ac1e08099aa4c3d8601b5a6c3d604.jpeg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">trans_toTensor = transforms.ToTensor()</span><br><span class="line">img_tensor = trans_toTensor(img)</span><br><span class="line"></span><br><span class="line">writer.add_image(<span class="string">&quot;ToTensor&quot;</span>, img_tensor)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p><strong>Normalize</strong></p>
<p>Normalize 的作用是归一化 tensor ，具体过程是将 Tensor 减去均值再除以标准差</p>
<p>输入：means（均值）、std（标准差），传入的是数组，有几个通道就传几个元素</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;image/4f8ac1e08099aa4c3d8601b5a6c3d604.jpeg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ToTensor</span></span><br><span class="line">trans_toTensor = transforms.ToTensor()</span><br><span class="line">img_tensor = trans_toTensor(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize</span></span><br><span class="line"><span class="comment"># 传入的两个数组分别是均值和标准差</span></span><br><span class="line">trans_norm = transforms.Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line"><span class="comment"># 传入 tensor</span></span><br><span class="line">img_norm = trans_norm(img_tensor)</span><br><span class="line">writer.add_image(<span class="string">&#x27;Normalize&#x27;</span>, img_norm)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>运行结果</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/153225-321b0.png" alt=""></p>
</li>
</ul>
</li>
<li>
<p><strong>Resize</strong></p>
<p>Resize 的作用是将输入的  PIL 图像转化为指定的 size ，参数为 (W, H) 时，会将图像转为改大小，若参数为一个整数，则会让图片的最小边匹配该数值，进行等比缩放</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;image/4f8ac1e08099aa4c3d8601b5a6c3d604.jpeg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ToTensor</span></span><br><span class="line">trans_toTensor = transforms.ToTensor()</span><br><span class="line">img_tensor = trans_toTensor(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Resize 成 512 * 512 的图像</span></span><br><span class="line">trans_resize = transforms.Resize((<span class="number">512</span>, <span class="number">512</span>))</span><br><span class="line">img_resize = trans_resize(img_tensor)</span><br><span class="line">writer.add_image(<span class="string">&#x27;Resize&#x27;</span>, img_resize)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>运行结果</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/154703-7e64a.png" alt=""></p>
</li>
</ul>
</li>
</ul>
<p>以上是使用 <code>PIL</code> 的 <code>Image</code> 模块进行图片读取，但常用的方式是使用 Opencv，安装代码：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install opencv-python</span><br></pre></td></tr></table></figure>
<p>使用 Opencv 读取图片会转换为 <code>numpy.array</code> 格式，</p>
<ul>
<li>
<p><strong>Compose</strong></p>
<p>Compose 的作用是将不同的 transform 组合在一起，比如先将图片进行 Resize，随后将图片进行 ToTensor</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;image/4f8ac1e08099aa4c3d8601b5a6c3d604.jpeg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ToTensor</span></span><br><span class="line">trans_toTensor = transforms.ToTensor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Resize</span></span><br><span class="line">trans_resize_2 = transforms.Resize(<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compose</span></span><br><span class="line">trans_compose = transforms.Compose([trans_resize_2, trans_toTensor])</span><br><span class="line">img_compose = trans_compose(img)</span><br><span class="line">writer.add_image(<span class="string">&quot;Compose&quot;</span>, img_compose)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>数据的变化过程是：PIL -&gt; PIL -&gt; Tensor</p>
<p>新版的 PyTorch 的 Resize 可以接受 Tensor 输入了，所以可以直接用 Tensor 进行 Resize</p>
</blockquote>
</li>
<li>
<p>运行结果</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/155923-1da7c.png" alt=""></p>
</li>
</ul>
</li>
<li>
<p><strong>RandomCrop</strong></p>
<p>RandomCrop 可以对图片进行随机裁剪，可以指定裁剪的尺寸，利用 for 循环可以进行多次裁剪。</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;image/4f8ac1e08099aa4c3d8601b5a6c3d604.jpeg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ToTensor</span></span><br><span class="line">trans_toTensor = transforms.ToTensor()</span><br><span class="line">img_tensor = trans_toTensor(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># RandomCrop()</span></span><br><span class="line">trans_random = transforms.RandomCrop(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    img_random = trans_random(img_tensor)</span><br><span class="line">    writer.add_image(<span class="string">&quot;RandomCrop&quot;</span>, img_random, i)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>该代码对目标图像进行了 10 次随机裁剪，每次裁剪的尺寸是 512 * 512.</p>
</li>
<li>
<p>运行结果</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/161544-e886a.png" alt=""><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/161554-c064d.png" alt=""></p>
</li>
</ul>
</li>
</ul>
<h3 id="2-4-torchvision-数据集使用">2.4 torchvision 数据集使用</h3>
<p>在 PyTorch 的官方网站中，提供了很多包括关于文本、图像、音频等等的文档，在这些文档里都能找到标准的数据集，供模型训练。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/164221-12a79.png" alt=""></p>
<p>这一节将结合 dataset 和 transform，学习如何对数据集进行处理。</p>
<p>torchvision 的数据集参数都比较相近，首先来了解一下本次使用的 <code>CIFAR-10</code> 数据集：</p>
<p><code>CIFAR-10</code> 数据集包括了 60000 张 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>32</mn><mo>×</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">32\times32</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">32</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">32</span></span></span></span> 的图像，总共有 10 个类别，其中 50000 张是训练图片， 10000 张是验证图片。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/165203-d27da.png" alt=""></p>
<ul>
<li><strong>root:</strong> 必须设置， 指定数据集位置</li>
<li><strong>train:</strong> 为 True 表示该数据集是训练集，为 False 表示该数据集为测试集</li>
<li><strong>transform:</strong> 数据集的预处理</li>
<li><strong>target_transform:</strong> 对指定的 target 进行 transform</li>
<li><strong>download:</strong> 为 True 则会自动下载数据集</li>
</ul>
<p>接下来将使用该数据集进行一系列的数据处理。</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 CIFAR-10 训练集，download=True 自动下载</span></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 CIFAR-10 测试集</span></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(test_set[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(&lt;PIL.Image.Image image mode=RGB size=32x32 at <span class="number">0x10CE0532080</span>&gt;, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>输出包含了两部分，分别是 <code>img</code> 和 <code>target</code>：</p>
<ul>
<li><code>img</code>：图片文件</li>
<li><code>target</code>：图片对应 label 的序号</li>
</ul>
<p>我们也可以通过 <code>class</code> 属性输出对应的 label 名称</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">img, target = test_set[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(test_set.classes[target])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出 cat</span></span><br></pre></td></tr></table></figure>
<p>可以看到，我们得到的图片数据都是 PIL 类型的，但是在 PyTorch 中，最好还是使用 Tensor 类型的数据，所以就需要做一些变换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对数据进行 ToTensor</span></span><br><span class="line">dataset_transform = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 dataset_transform 应用于训练集</span></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, transform=dataset_transform, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 dataset_transform 应用于测试集</span></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=dataset_transform, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">img, target = test_set[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(img))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出 &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<p>转化为 Torch 格式后，就能将图像放入 Tensorboard 进行可视化了，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    img, target = train_set[i]</span><br><span class="line">    writer.add_image(<span class="string">&#x27;train_set&#x27;</span>, img, i)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/180802-96105.png" style="zoom:67%;" />
<h3 id="2-5-DataLoader-的使用">2.5 DataLoader 的使用</h3>
<p>Dataset 是将数据读入，而 DataLoader 的作用是将 Dataset 的数据加载到神经网络中。Dataset 就相当于是一副牌堆，而 DataLoader 就相当于是一张张的手牌，每次都需要从牌堆中取牌，而取牌的方法就是由 DataLoader 控制的。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/182209-85f20.png" alt=""></p>
<p>DataLoader 中有很多参数，但是其中有很多参数是已经有默认值的。</p>
<ul>
<li><strong>dataset</strong> ：Dataset ，需要加载的数据集，相当于一整副扑克</li>
<li><strong>batch_size</strong> ：int，每次取数据的数量，相当于次摸得牌数</li>
<li><strong>shuffle</strong>：bool，是否打乱数据，相当于洗牌</li>
<li><strong>num_workers</strong> ：int，设置多进程</li>
<li><strong>drop_last</strong> ：bool，取不尽时是否舍去剩余数据</li>
</ul>
<p>接下来看看具体代码。</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取测试集</span></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>,</span><br><span class="line">                                         transform=torchvision.transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 DataLoader</span></span><br><span class="line">test_loader = DataLoader(dataset=test_data, batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>,</span><br><span class="line">                         drop_last=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">img, target = test_data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(img.shape)		<span class="comment"># torch.Size([3, 32, 32])</span></span><br><span class="line"><span class="built_in">print</span>(target)			<span class="comment"># 3</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里设置了 <code>batch_size=4</code>，表示一次会取 4 个数据；<code>shuffle=True</code> 表示每次取完数据后都会打乱数据。</p>
<p>接下来使用 DataLoader 取出数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)</span><br><span class="line">    <span class="built_in">print</span>(targets)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>]) 	<span class="comment"># 4张图片 3通道 尺寸为 32 * 32</span></span><br><span class="line">tensor([<span class="number">5</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">2</span>])		<span class="comment"># 四张图片对应的 label </span></span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">tensor([<span class="number">2</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">4</span>])</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">tensor([<span class="number">7</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">tensor([<span class="number">7</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">9</span>])</span><br><span class="line">···· </span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>有了数据后就可以通过 TensorBoard 进行可视化了，但是这次不是单独的数据，而是多个数据打包在一起，所以代码和之前有些许区别：</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取测试集</span></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>,</span><br><span class="line">                                         transform=torchvision.transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每次取64张</span></span><br><span class="line">test_loader = DataLoader(dataset=test_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>,</span><br><span class="line">                         drop_last=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">img, target = test_data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">writter = SummaryWriter(<span class="string">&#x27;DataLoader&#x27;</span>)</span><br><span class="line"><span class="comment"># 设置全局路径</span></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    writter.add_images(<span class="string">&quot;test_DataLoader&quot;</span>, imgs, step)</span><br><span class="line">    <span class="comment"># 每次添加完图片后增加 step</span></span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">writter.close()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>输出结果</p>
<img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/194542-abfb2.png" style="zoom:67%;" />
</li>
</ul>
<p>可以看到每个 step 有 64 张图片，但是最后一步并没有 64 张，这是因为数据集不能被 64 整除，由于设置了 <code>drop_last=False</code> ，剩下的图像被合并成最后一步。</p>
<p>由于设置了 <code>shuffle=True</code> ，所以每个 epoch 取得图片顺序都不会相同：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        writer.add_images(<span class="string">&quot;test_DataLoader_epoch_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch), imgs, step)</span><br><span class="line">        step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/03/200730-8b4ce.png" alt=""></p>
<h2 id="三、神经网络结构">三、神经网络结构</h2>
<h3 id="3-1-PyTorch-搭建神经网络介绍">3.1 PyTorch 搭建神经网络介绍</h3>
<p>PyTorch 中关于神经网络的文档都可以在 PyTorch 官网的 <code>torch.nn</code> 模块中找到，包括<strong>骨架（Containers）</strong>、<strong>池化层（Pooling Layers）</strong>、 **非线性激活函数（Non-linear-Activation）**等等，都可以在 PyTorch 官网中找到。</p>
<p>其中 Containers  中包含六个类，最常用的是 <code>Module</code> 类 ，它为所有神经网络提供了最基本的骨架，神经网络必须继承这个类。</p>
<p>官网给出的案例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每次都必须重写该方法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        <span class="keyword">return</span> F.relu(self.conv2(x))</span><br></pre></td></tr></table></figure>
<p>这个 Demo 首先是继承了 nn.Module 类，进行了初始化，随后进行前向传播，前向传播的过程是：输入 -&gt; 卷积 -&gt;  ReLU -&gt; 卷积 -&gt; ReLU -&gt; 输出。</p>
<h3 id="3-2-搭建自己的神经网络">3.2 搭建自己的神经网络</h3>
<p>了解了 PyTorch 搭建神经网络的方法，接下来就开始搭建一个最简单的神经网络：</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建自己的网络类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net01</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># 前向传播让输入加 1 </span></span><br><span class="line">        output = <span class="built_in">input</span> + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化对象</span></span><br><span class="line">net01 = Net01()</span><br><span class="line">x = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line"><span class="comment"># 前向传播，因为 __call__() 方法的实现 _call_impl() 中调用了 forward() ，所以会直接执行 forward()</span></span><br><span class="line">output = net01(x)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="comment"># 输出 2.</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="3-3-PyTorch-卷积操作">3.3 PyTorch 卷积操作</h3>
<p>在 PyTorch 官网中可以找到有关卷积层操作的文档：[地址](<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#convolution-layers">torch.nn — PyTorch 1.12 documentation</a>)。其中 <code>nn.Conv1d</code> 表示数据是一维数据，<code>nn.Conv2d</code> 表示数据是二维的（比如图像就是二维数据）。</p>
<p>PyTorch有关神经网络的 API 有两个，分别是 <code>torch.nn</code> 和 <code>torch.nn.function</code>，其中 <code>torch.nn</code> 是对 <code>torch.nn.function</code> 的封装，是更底层的代码，在使用卷积层操作之前，先来了解一下 <code>torch.nn.function</code> 这个 API 。</p>
<p><code>torch.nn.function</code>  中的模块和 <code>torch.nn</code> 的模块是相同的，首先来看看 <code>nn.Conv1d</code> 模块，它包含了以下参数：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/04/191536-95e57.png" alt=""></p>
<p>其中 <code>weight</code> 就是卷积核，<code>bias</code> 是偏置项，<code>stride</code> 是卷积核移动的步长，<code>padding</code> 是为卷积结果进行零填充的层数。</p>
<p>下面是一个具体例子：</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建二维矩阵</span></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                      [<span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建卷积核</span></span><br><span class="line">kernel = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                       [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">kernel = torch.reshape(kernel, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出1：卷积核步长为 1 </span></span><br><span class="line">output1 = F.conv2d(<span class="built_in">input</span>, kernel, stride=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(output1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出2：卷积核步长为 2 ，</span></span><br><span class="line">output2 = F.conv2d(<span class="built_in">input</span>, kernel, stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(output2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出2：卷积核步长为 1 ，零填充 1 层</span></span><br><span class="line">output2 = F.conv2d(<span class="built_in">input</span>, kernel, stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(output2)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>运行结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出 1 </span></span><br><span class="line">tensor([[[[<span class="number">10</span>, <span class="number">12</span>, <span class="number">12</span>],</span><br><span class="line">          [<span class="number">18</span>, <span class="number">16</span>, <span class="number">16</span>],</span><br><span class="line">          [<span class="number">13</span>,  <span class="number">9</span>,  <span class="number">3</span>]]]])</span><br><span class="line"><span class="comment"># 输出 2</span></span><br><span class="line">tensor([[[[<span class="number">10</span>, <span class="number">12</span>],</span><br><span class="line">          [<span class="number">13</span>,  <span class="number">3</span>]]]])</span><br><span class="line"><span class="comment"># 输出 3</span></span><br><span class="line">tensor([[[[ <span class="number">1</span>,  <span class="number">3</span>,  <span class="number">4</span>, <span class="number">10</span>,  <span class="number">8</span>],</span><br><span class="line">          [ <span class="number">5</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">12</span>,  <span class="number">6</span>],</span><br><span class="line">          [ <span class="number">7</span>, <span class="number">18</span>, <span class="number">16</span>, <span class="number">16</span>,  <span class="number">8</span>],</span><br><span class="line">          [<span class="number">11</span>, <span class="number">13</span>,  <span class="number">9</span>,  <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">          [<span class="number">14</span>, <span class="number">13</span>,  <span class="number">9</span>,  <span class="number">7</span>,  <span class="number">4</span>]]]])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="3-4-卷积层">3.4 卷积层</h3>
<p>PyTorch 中的卷积层主要使用 <code>torch.nn</code> 的 <code>Conv2d</code> 类，首先来看看官方文档：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/04/195436-39e11.png" alt=""></p>
<p>文档指出了使用该类所需要的参数，包括：</p>
<ul>
<li><em><strong>in_channels</strong></em>：输入通道数，彩色图片一般为 3 通道，必须指定</li>
<li><em><strong>out_channels</strong></em>：输出通道数，设置为几就有几个卷积核，必须指定</li>
<li><em><strong>kernel_size</strong></em>：卷积核大小，整型或者元组，必须指定</li>
<li><em>stride</em>：卷积核步长，一般需要自己指定</li>
<li><em>padding</em>：填充层数，一般需要自己指定</li>
<li><em>dilation</em>：卷积核元素间隔，一般不常用</li>
<li><em>groups</em>：一般设置为 1 ，分组卷积时进行修改，基本遇不到</li>
<li><em>bias</em>：是否启用偏置项，一般为 True</li>
<li><em>padding_mode</em>：填充模式（比如 0 填充）</li>
<li><em>device</em>：使用设备</li>
<li><em>dtype</em>：数据类型</li>
</ul>
<p>下面是一个实例：</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取数据集</span></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 dataLoader</span></span><br><span class="line">dataLoader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建自己的卷积神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 创建卷积层</span></span><br><span class="line">        self.conv1 = Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重写前向传播</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化网络</span></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了方便理解，在 tensorboard 中进行可视化</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 遍历 dataLoader，进行卷积</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataLoader:</span><br><span class="line">    <span class="comment"># torch.Size([64, 3, 30, 30])</span></span><br><span class="line">    imgs, targets = data</span><br><span class="line">    <span class="comment"># torch.Size([64, 6, 30, 30])</span></span><br><span class="line">    output = myNet(imgs)</span><br><span class="line">    </span><br><span class="line">    writer.add_images(<span class="string">&#x27;input&#x27;</span>, imgs, step)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 由于卷积后尺寸并不匹配 add_images()，需要进行 reshape,</span></span><br><span class="line">    <span class="comment"># 设置 -1 会让框架进行自动计算</span></span><br><span class="line">    output = torch.reshape(output, (-<span class="number">1</span>, <span class="number">3</span>, <span class="number">30</span>, <span class="number">30</span>))</span><br><span class="line">    writer.add_images(<span class="string">&#x27;output&#x27;</span>, output, step)</span><br><span class="line"></span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>输出结果</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/04/210712-28c62.png" alt=""></p>
</li>
</ul>
<h3 id="3-5-池化层">3.5 池化层</h3>
<p>[池化层文档](<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#pooling-layers">torch.nn — PyTorch 1.12 documentation</a>)，PyTorch 中的池化层包含很多类，比如 <code>nn.MaxPool2d</code>（下采样）、<code>nn.MaxUnpool2d</code>（上采样）、<code>nn.AvgPool2d</code>（平均池化）但是我们最常用的还是 <code>nn.MaxPool2d</code>。接下来看看对应的文档：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/093527-c6fd5.png" alt=""></p>
<p>它的参数有：</p>
<ul>
<li>
<p><em><strong>kernel_size</strong></em>：池化核大小，同卷积核大小，必须设置</p>
</li>
<li>
<p><em>stride</em>：池化核移动步长，默认值是 kernel_size</p>
</li>
<li>
<p><em>padding</em>：填充层数</p>
</li>
<li>
<p><em>dilation</em>：卷积核元素间隔，一般不设置</p>
</li>
<li>
<p><em>return_indices</em>：用的很少，不做了解</p>
</li>
<li>
<p><em>ceil_mode</em>：bool，设置为 False 时，使用 floor 模式，丢弃多余数据；设置为 True 时，使用 ceil 模式，保留多余数据，如下图所示。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/100158-53e0f.png" alt=""></p>
</li>
</ul>
<p>下面是一个具体例子：</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建输入</span></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                      [<span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把输入 reshape 成指定 shape</span></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (-<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 网络初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        self.pooling = nn.MaxPool2d(kernel_size=<span class="number">3</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重写前向传播</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        output = self.pooling(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络实例化</span></span><br><span class="line">myNet = MyNet()</span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">output = myNet(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出1</span></span><br><span class="line">tensor([[[[<span class="number">2.</span>]]]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果设置 ceil_mode=True,输出为</span></span><br><span class="line">tensor([[[[<span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">          [<span class="number">5.</span>, <span class="number">1.</span>]]]])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>为了清楚池化层的作用，我们可以通过 tensorboard 进行可视化：</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                      [<span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (-<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line">dataLoader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        self.pooling = nn.MaxPool2d(kernel_size=<span class="number">3</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        output = self.pooling(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line">myNet = MyNet()</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataLoader:</span><br><span class="line">    img, target = data</span><br><span class="line">    output = myNet(img)</span><br><span class="line">    writer.add_images(<span class="string">&#x27;input&#x27;</span>, img, step)</span><br><span class="line">    writer.add_images(<span class="string">&#x27;output&#x27;</span>, output, step)</span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>运行结果</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/103334-4073f.png" alt=""></p>
</li>
</ul>
<p>可以看到，经过池化层后，图片仍然保存了本身的特征，而且体积被大大缩小了。</p>
<h3 id="3-6-非线性激活函数">3.6 非线性激活函数</h3>
<p>Pytorch [官方文档](<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">torch.nn — PyTorch 1.12 documentation</a>)。在神经网络中，非线性激活函数的作用是为网络引入非线性的特质，其中最常用的是 <code>nn.ReLU</code> 。</p>
<img src= "/img/lazyload.gif" data-lazy-src="C:/Users/Fkhsns/AppData/Roaming/Typora/typora-user-images/image-20220705105339920.png" style="zoom:67%;" />
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1.1</span>, -<span class="number">0.5</span>],</span><br><span class="line">                      [-<span class="number">1</span>, <span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, [-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        <span class="comment"># 若 inplace 为 False 会返回一个新的变量，</span></span><br><span class="line">        <span class="comment"># 若 inplace 为 True 则会修改原来的变量</span></span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.relu()</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line">output = myNet(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">tensor([[[[<span class="number">1.1000</span>, <span class="number">0.0000</span>],</span><br><span class="line">          [<span class="number">0.0000</span>, <span class="number">3.0000</span>]]]])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>同样的，为了显示非线性函数的效果，可以将处理过的数据进行可视化</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line">dataLoader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">False</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># 这里使用 sigmoid 函数，</span></span><br><span class="line">        <span class="comment"># 因为用 sigmoid 函数处理的图像区别看起来更明显</span></span><br><span class="line">        output = self.sigmoid(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">myNet = MyNet()</span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataLoader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    writer.add_images(<span class="string">&#x27;input&#x27;</span>, imgs, step)</span><br><span class="line">    output = myNet(imgs)</span><br><span class="line">    writer.add_images(<span class="string">&#x27;output2&#x27;</span>, output, step)</span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>输出结果</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/113131-16f07.png" alt=""></p>
</li>
</ul>
<h3 id="3-7-批量归一化">3.7 批量归一化</h3>
<p>PyTorch <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#normalization-layers">官方文档</a>。使用批量归一化可以解决梯度消失和梯度爆炸的问题，常用的是 <code>nn.BatchNorm2d</code> 类，官方文档内容如下：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/123427-5a137.png" alt=""></p>
<p>参数：</p>
<ul>
<li><em><strong>num_features</strong></em>：输入的通道数， $(N, C, H, W) $ 中的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span></li>
<li>其他参数一半使用默认值</li>
</ul>
<h3 id="3-8-线性层">3.8 线性层</h3>
<p>PyTorch <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#linear-layers">官方文档</a>。线性层即神经网络中的全连接层，包含大量的参数，一般放在网络的最后方，如下图所示：</p>
<img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/124830-13cd8.png" style="zoom:67%;" />
<p>除了 Input layer ，每一层的元素都是上层元素的加权和，在 PyTorch 中计算线性层非常方便，首先来看看官方文档，确认这一层的参数：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/125034-97440.png" alt=""></p>
<p>参数：</p>
<ul>
<li><em><strong>in_features</strong></em>：输入的特征数</li>
<li><em><strong>out_features</strong></em>：输出的特征数</li>
<li><em>bias</em>：是否添加偏置项</li>
<li><em>device</em>： 使用设备</li>
<li><em>dtype</em>：指定数据类型</li>
</ul>
<p>下面是一个例子：</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line">dataLoader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        <span class="comment"># 线性层，输入维度时 196608 ,输出维度是 10 </span></span><br><span class="line">        self.linear = nn.Linear(<span class="number">196608</span>, <span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.linear(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataLoader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    <span class="comment"># 将图像展平</span></span><br><span class="line">    imgs = torch.flatten(imgs)</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)</span><br><span class="line">    <span class="comment"># 全连接层前向传播</span></span><br><span class="line">    output = myNet(imgs)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="3-9-Dropout-层">3.9 Dropout 层</h3>
<p>PyTorch <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#dropout-layers">官方文档</a>。Dropout 层的主要作用是在模型训练阶段，随机地将输入张量的某些元素以概率 p 归零，是卷积神经网络中防止过拟合的一种方式。</p>
<h3 id="3-10-Sequential">3.10 Sequential</h3>
<p>PyTorch <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential">官方文档</a>。使用 Squential 可以将网络结构组合起来，按照我们设定的顺序一步步进行前向传播，使代码更加简洁易懂，更加方便管理。</p>
<p>下面是一个官方示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Using Sequential to create a small model. When `model` is run,</span></span><br><span class="line"><span class="comment"># input will first be passed to `Conv2d(1,20,5)`. The output of</span></span><br><span class="line"><span class="comment"># `Conv2d(1,20,5)` will be used as the input to the first</span></span><br><span class="line"><span class="comment"># `ReLU`; the output of the first `ReLU` will become the input</span></span><br><span class="line"><span class="comment"># for `Conv2d(20,64,5)`. Finally, the output of</span></span><br><span class="line"><span class="comment"># `Conv2d(20,64,5)` will be used as input to the second `ReLU`</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">          nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU(),</span><br><span class="line">          nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using Sequential with OrderedDict. This is functionally the</span></span><br><span class="line"><span class="comment"># same as the above code</span></span><br><span class="line">model = nn.Sequential(OrderedDict([</span><br><span class="line">          (<span class="string">&#x27;conv1&#x27;</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU()),</span><br><span class="line">          (<span class="string">&#x27;conv2&#x27;</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">&#x27;relu2&#x27;</span>, nn.ReLU())</span><br><span class="line">        ]))</span><br></pre></td></tr></table></figure>
<h2 id="四、神经网络简单搭建">四、神经网络简单搭建</h2>
<blockquote>
<p>了解了 PyTorch 中神经网络各层的代码编写规则，接下来就可以搭建自己的卷积神经网络了，本节将实现使用卷积神经网络和 <code>CIFAR-10</code> 数据集，完成 图像分类的功能</p>
</blockquote>
<h3 id="4-1-CIFAR-10-网络结构">4.1 CIFAR-10 网络结构</h3>
<p>想要实现一个卷积神经网络，首先需要设计网络的结构，这里我们不自己设计网络结构，而是使用别人已经设计好的结构来实现分类功能，具体的网络结构如下图所示：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/141555-e209f.png" alt=""></p>
<p>网络的结构是：输入 - &gt; <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mo>×</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">5\times5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">5</span></span></span></span> 卷积层 - &gt; <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span></span></span></span> 最大池化层 - &gt; <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mo>×</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">5\times5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">5</span></span></span></span> 卷积层 - &gt; <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span></span></span></span> 最大池化层 - &gt; <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mo>×</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">5\times5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">5</span></span></span></span>  卷积层 -&gt; <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span></span></span></span> 池化 - &gt; 展开成一维 - &gt; 全连接层 - &gt; 输出。</p>
<p>可以发现网络结构十分简单，也没有引入非线性函数，接下来我们就根据这个结构来训练一个图像分类器。</p>
<h3 id="4-2-网络搭建">4.2 网络搭建</h3>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dataLoader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        <span class="comment"># 为了使输出尺寸保持不变，需要计算 padding 和 stride</span></span><br><span class="line">        <span class="comment"># 计算结果为 padding = 2, stride = 1</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        self.pooling1 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, padding=<span class="number">2</span>)</span><br><span class="line">        self.pooling2 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        self.pooling3 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">1024</span>, <span class="number">64</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">	<span class="comment"># 重写前向传播</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        x = self.conv1(<span class="built_in">input</span>)</span><br><span class="line">        x = self.pooling1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.pooling2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.pooling3(x)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.linear1(x)</span><br><span class="line">        output = self.linear2(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络结构检验</span></span><br><span class="line"><span class="built_in">input</span> = torch.ones((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">output = myNet(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape) <span class="comment"># 输出 torch.Size([64, 10])</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>卷积层计算公式：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/142813-be9d0.png" alt="卷积层 shape 计算公式"></p>
<p>可以看到这样编写代码，会让前向传播显得十分冗余，我们可以使用 <code>nn.Sequential</code> 来简化代码。</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dataLoader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        <span class="comment"># 使用 nn.Sequential 整合网络结构</span></span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播代码三行就搞定了</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.model(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络结构检验</span></span><br><span class="line"><span class="built_in">input</span> = torch.ones((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">output = myNet(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape) <span class="comment"># 输出 torch.Size([64, 10])</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>通过 tensorboard ，也可以将我们的卷积神经网络结构进行可视化。</p>
<ul>
<li>
<p>代码编写（接上方代码）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化网络结构（计算图)</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./logs_seq&#x27;</span>)</span><br><span class="line">writer.add_graph(myNet, <span class="built_in">input</span>)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>运行结果</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/150805-27392.png" alt=""></p>
</li>
</ul>
<p>可以看到 tensorboard 可视化后的网络结构十分清晰，我们还可以进一步细化，进入每一层的内部观察数据的 shape。</p>
<img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/151023-dcc2c.png" alt="第一层卷积层" style="zoom:67%;" />
<h2 id="五、损失函数、反向传播及优化">五、损失函数、反向传播及优化</h2>
<h3 id="5-1-损失函数">5.1 损失函数</h3>
<p>PyTorch <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#loss-functions">官方文档</a>。损失函数是衡量模型预测结果与实际数据差距的数值，包括 <code>L1loss</code>、<code>MSE  loss</code>、<code>nn.CrossEntropyLoss</code>  等等，不同的损失函数在官方文档都有很详细的解释，下面来看一个例子。</p>
<ul>
<li>
<p>代码编写  ( L1loss )</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出 [1, 2, 3]</span></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.float32)</span><br><span class="line"><span class="comment"># 目标 [1, 2, 5]</span></span><br><span class="line">targets = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>], dtype=torch.float32)</span><br><span class="line"><span class="comment"># 损失值： |(1 - 1 + 2 - 2 + 3 - 5)| / 3 = 0.66667</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># reshape 输出和目标</span></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line">targets = torch.reshape(targets, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        self.loss = nn.L1Loss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.loss(<span class="built_in">input</span>, targets)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line">loss = myNet(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出 tensor(0.6667)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>上一节我们已经编写了一个用于分类 <code>CIFAR-10</code> 数据集的神经网络模型，现在我们就可以来看看这个模型在数据集上的损失函数。</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dataLoader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        self.loss = nn.L1Loss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.model(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化损失计算</span></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataLoader:</span><br><span class="line">    imgs, target = data</span><br><span class="line">    output = myNet(imgs)</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    result_loss = loss(output, target)</span><br><span class="line">    <span class="built_in">print</span>(result_loss)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>运行结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">2.3061</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br><span class="line">tensor(<span class="number">2.3143</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br><span class="line">tensor(<span class="number">2.2893</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br><span class="line">tensor(<span class="number">2.2944</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br><span class="line">tensor(<span class="number">2.3121</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br><span class="line">tensor(<span class="number">2.3048</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br><span class="line">tensor(<span class="number">2.3090</span>, grad_fn=&lt;NllLossBackward0&gt;)</span><br><span class="line">···</span><br><span class="line">···</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>运行的结果就是<strong>网络输出</strong>与<strong>目标</strong>之间的差距，有了这些差距，就为我们提供了参数更新的依据（反向传播）。</p>
<h3 id="5-2-反向传播">5.2 反向传播</h3>
<p>PyTorch 为计算图中的每一个节点提供了一个 <code>grad</code> 参数，用于保存该节点的梯度，有了 <code>grad</code> 就有了反向传播的依据，我们只需要一行代码：<code>result_loss.backward()</code> ，就能实现梯度的自动计算。</p>
<p>在运行这行代码之前，节点的梯度值 <code>grad</code> 都是 None</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/165938-523bf.png" alt=""></p>
<p>运行完该段代码后，<code>grad</code> 会自动计算出来：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/05/170027-6b153.png" alt=""></p>
<h3 id="5-3-优化器">5.3 优化器</h3>
<p>Pytorch <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">官方文档</a>。为了实现参数更新，我们需要使用到 PyTorch 中的优化器，在官方文档中有详细的介绍，且提供了构造优化器的案例。在上节中我们已经介绍了如何计算节点的梯度，有了梯度后就可以调用<code>optimizer.step()</code> 来进行参数更新，下面来看一个例子。</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                       transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dataLoader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        self.loss = nn.L1Loss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.model(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">myNet = MyNet()</span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 构建优化器</span></span><br><span class="line">optmi = torch.optim.SGD(myNet.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    <span class="comment"># 初始化一个 epoch 的 loss</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataLoader:</span><br><span class="line">        imgs, target = data</span><br><span class="line">        output = myNet(imgs)</span><br><span class="line">        result_loss = loss(output, target)</span><br><span class="line">        <span class="comment"># 先将梯度归零</span></span><br><span class="line">        optmi.zero_grad()</span><br><span class="line">        <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">        result_loss.backward()</span><br><span class="line">        <span class="comment"># 参数更新</span></span><br><span class="line">        optmi.step()</span><br><span class="line">        <span class="comment"># 将一批数据的 loss 加到整个 epoch 的 loss 上</span></span><br><span class="line">        running_loss += result_loss</span><br><span class="line">    <span class="built_in">print</span>(running_loss)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>输出结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">360.9923</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">tensor(<span class="number">358.1781</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">tensor(<span class="number">345.0446</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">···</span><br><span class="line">tensor(<span class="number">237.9353</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">tensor(<span class="number">234.4855</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">tensor(<span class="number">231.1977</span>, grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>可以看到每一轮 epoch 的整体 loss 都在减小，模型正在不断拟合数据。</p>
<h2 id="六、模型使用">六、模型使用</h2>
<h3 id="6-1-现有模型使用方法">6.1 现有模型使用方法</h3>
<p>PyTorch 中已经提供了许多已经训练好的模型，本节将介绍如何使用这些模型和如何对模型做一些自定义的修改，<a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/models.html#">官方文档</a>。</p>
<p>模型的类型包括：</p>
<ul>
<li>分类</li>
<li>语义分割</li>
<li>目标检测、实例分割和人物关键点检测</li>
<li>视频分类</li>
</ul>
<p>本节将使用 PyTorch 提供的 <code>VGG16</code> 分类模型来实现 <code>CIFAR-10</code> 图片数据集分类的功能。</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 140G 的训练集</span></span><br><span class="line"><span class="comment"># train_data = torchvision.datasets.ImageNet(&#x27;./data_image_net&#x27;, split=&#x27;train&#x27;, download=True,</span></span><br><span class="line"><span class="comment">#                                            transform=torchvision.transforms.ToTensor())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 未训练好的 Vgg16</span></span><br><span class="line">vgg16_false = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 已训练好的 Vgg16</span></span><br><span class="line">vgg16_true = torchvision.models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vgg16_true)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>运行结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">VGG(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">3</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">4</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">5</span>): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">6</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">7</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">8</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">9</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">10</span>): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">11</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">12</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">13</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">14</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">15</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">16</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">17</span>): Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">18</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">19</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">20</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">21</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">22</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">23</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">24</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">25</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">26</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">27</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">28</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">29</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">30</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(<span class="number">7</span>, <span class="number">7</span>))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">25088</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">2</span>): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">3</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">4</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">5</span>): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">1000</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">Process finished <span class="keyword">with</span> exit code <span class="number">0</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>输出结果很清晰的展示了 Vgg16 的网络结构，大体上分为三个部分：</p>
<ul>
<li>特征处理  (features)</li>
<li>平均池化  (avgpool)</li>
<li>分类器      (classifier)</li>
</ul>
<p>观察最后一层，可以看到输出是 1000 维的向量，而 <code>CIFAR-10</code> 数据库只有 10 类数据，我们可以想办法修改这个已有的模型，方法有：</p>
<ol>
<li>
<p>将最后的输出层元素数量由 1000 改为 10</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                          download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">vgg16_false = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">vgg16_true = torchvision.models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># vgg16_true.classifier.add_module(&#x27;add_linear&#x27;, nn.Linear(1000, 10))</span></span><br><span class="line"></span><br><span class="line">vgg16_false.classifier[<span class="number">6</span>] = nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vgg16_false)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>输出结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">VGG(</span><br><span class="line">	···</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">	···</span><br><span class="line">    (<span class="number">5</span>): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p>在最后一层的后方在添加一层网络</p>
<ul>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                          download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">vgg16_false = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">vgg16_true = torchvision.models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 classifier 后添加一层网络</span></span><br><span class="line"><span class="comment"># 去掉 .classifier 也可也在后方加上一层，只不过是在 classifier 之外</span></span><br><span class="line">vgg16_true.classifier.add_module(<span class="string">&#x27;add_linear&#x27;</span>, nn.Linear(<span class="number">1000</span>, <span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(vgg16_true)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>输出结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">VGG(</span><br><span class="line">	······</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">	······</span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">1000</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (add_linear): Linear(in_features=<span class="number">1000</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ol>
<h3 id="6-2-模型的保存与读取">6.2 模型的保存与读取</h3>
<ul>
<li>
<p>模型保存</p>
<ul>
<li>
<p>保存方式 1：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式1，这种方式不仅保存了网络，还保存了网络中的参数</span></span><br><span class="line"><span class="comment"># 这种方式存在陷阱，保存后，在使用之前必须引入对应的包</span></span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">torch.save(vgg16, <span class="string">&#x27;./vgg16_method1.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>保存方式2：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式2，仅保存网络参数，不保存模型，官方推荐</span></span><br><span class="line">torch.save(vgg16.state_dict(), <span class="string">&#x27;./vgg16_method2.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p>模型读取</p>
<ul>
<li>
<p>读取方式 1：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式1 -&gt; 保存方式 1 对应的加载模型，不仅读取了模型，还读取了网络参数 </span></span><br><span class="line">model = torch.load(<span class="string">&#x27;./vgg16_method1.pth&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出网络结构</span></span><br><span class="line">VGG(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      ·······</span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">1000</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>读取方式2：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式2 -&gt; 保存方式 2 对应的模型加载</span></span><br><span class="line">model2 = torch.load(<span class="string">&#x27;./vgg16_method2.pth&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(model2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为模型载入参数</span></span><br><span class="line">vgg16 = torchvision.models.vgg16()</span><br><span class="line">vgg16.load_state_dict(model2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出字典，</span></span><br><span class="line">OrderedDict([(<span class="string">&#x27;features.0.weight&#x27;</span>, tensor([[[[-<span class="number">0.0072</span>, -<span class="number">0.0889</span>, -<span class="number">0.0209</span>],</span><br><span class="line">          [ <span class="number">0.0320</span>,  <span class="number">0.0086</span>, -<span class="number">0.0210</span>],</span><br><span class="line">          [-<span class="number">0.1112</span>, -<span class="number">0.1005</span>,  <span class="number">0.0080</span>]],</span><br><span class="line">······</span><br><span class="line"><span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,</span><br><span class="line">        <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]))])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h2 id="七、-完整的模型训练">七、 完整的模型训练</h2>
<h3 id="7-1-训练模型完整步骤">7.1 训练模型完整步骤</h3>
<ol>
<li>
<p>准备数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model_train.py</span></span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                          download=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                         download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line"><span class="comment"># 测试集长度</span></span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dataLoader 加载数据集</span></span><br><span class="line">train_dataLoader = DataLoader(dataset=train_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataLoader = DataLoader(dataset=test_data, batch_size=<span class="number">64</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>编写网络模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新起一个 Model.py 文件，专门保存模型代码</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet, self).__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">64</span> * <span class="number">4</span> * <span class="number">4</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.model(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试网络正确性，通过测试后就可以将该模型引入其他文件</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    myNet = MyNet()</span><br><span class="line">    <span class="built_in">input</span> = torch.ones((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">    output = myNet(<span class="built_in">input</span>)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>实例化网络模型、损失函数、优化器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model_train.py</span></span><br><span class="line"><span class="comment"># 创建网络模型</span></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = torch.optim.SGD(myNet.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 writer</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./test_logs&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>模型训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练次数</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-----第 &#123;&#125; 次训练开始-----&quot;</span>.<span class="built_in">format</span>(i + <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataLoader:</span><br><span class="line">        imgs, target = data</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        output = myNet(imgs)</span><br><span class="line">        <span class="comment"># 损失计算</span></span><br><span class="line">        loss = loss_fn(output, target)</span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_train_step += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;训练次数：&#123;&#125;， loss:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_train_step, loss.item()))</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>模型及保存测试</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment"># 测试步骤</span></span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 仅需要进行测试，不需要调整梯度，最好设置为 torch.no_grad()</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataLoader:</span><br><span class="line">            imgs, targets = data</span><br><span class="line">            output = myNet(imgs)</span><br><span class="line">            test_loss = loss_fn(output, targets)</span><br><span class="line">            total_test_loss += test_loss.item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;整体 test loos:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;整体 accuracy:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;test_loss&#x27;</span>, total_test_loss, total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;test_accuracy&#x27;</span>, total_accuracy, total_test_step)</span><br><span class="line">    total_test_step += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 保存模型</span></span><br><span class="line">    torch.save(myNet, <span class="string">&#x27;mynet_&#123;&#125;.pth&#x27;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    <span class="comment"># 方式2：torch.save(myNet.state_dict(), &#x27;mynet_&#123;&#125;.pth&#x27;.format(i))  </span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;模型已保存&#x27;</span>) </span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ul>
<li>
<p>输出结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">-----第 <span class="number">1</span> 次训练开始-----</span><br><span class="line">训练次数：<span class="number">1</span>， loss:<span class="number">2.3277838230133057</span></span><br><span class="line">训练次数：<span class="number">2</span>， loss:<span class="number">2.3247644901275635</span></span><br><span class="line">训练次数：<span class="number">3</span>， loss:<span class="number">2.2977490425109863</span></span><br><span class="line">······</span><br><span class="line">训练次数：<span class="number">7818</span>， loss:<span class="number">0.9464911222457886</span></span><br><span class="line">训练次数：<span class="number">7819</span>， loss:<span class="number">1.2017658948898315</span></span><br><span class="line">训练次数：<span class="number">7820</span>， loss:<span class="number">1.4486428499221802</span></span><br><span class="line">整体 test loos:<span class="number">206.69312530755997</span></span><br><span class="line">整体 accuracy:<span class="number">0.534500002861023</span></span><br><span class="line">模型已保存</span><br></pre></td></tr></table></figure>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/07/121631-39c5e.png" alt="损失函数值"></p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/07/124144-78a33.png" alt="测试集准确率"></p>
</li>
</ul>
<blockquote>
<p>在读别人的代码的时候，在训练之前有时候会加上 <code>model.train()</code>，让模型进入训练状态；在验证之前会加上 <code>model.eval()</code>，让模型进入验证状态。这并不代表加上这些代码才可以运行程序，而是说如果模型中有些一些特殊的层（BN、Dropout）时，需要添加上，如果没有，即便不添加上这两个代码也可以运行。</p>
</blockquote>
<h3 id="7-2-使用-GPU-训练模型">7.2 使用 GPU 训练模型</h3>
<p>PyTorch 可以使用 NVIDIA 的 GPU 进行模型训练，分为两种方式：</p>
<ul>
<li>
<p>方式1：</p>
<p>找到模型以下三个部分：</p>
<ul>
<li>网络模型</li>
<li>数据（imgs，targets）</li>
<li>损失函数</li>
</ul>
<p>调用对应对象的 <code>.cuda()</code> 方法，即可实现在 GPU 上进行训练</p>
<ul>
<li>
<p>完整代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> Model <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                          download=<span class="literal">True</span>)</span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                         download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line"><span class="comment"># 测试集长度</span></span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dataLoader 加载数据集</span></span><br><span class="line">train_dataLoader = DataLoader(dataset=train_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataLoader = DataLoader(dataset=test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建网络模型</span></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络模型使用 GPU 计算</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    myNet.cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;cuda 无法使用&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 损失函数使用 GPU 计算</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    loss_fn = loss_fn.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = torch.optim.SGD(myNet.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 writer</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./test_logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-----第 &#123;&#125; 次训练开始-----&quot;</span>.<span class="built_in">format</span>(i + <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataLoader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练数据使用 GPU 计算</span></span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            imgs = imgs.cuda()</span><br><span class="line">            targets = targets.cuda()</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        output = myNet(imgs)</span><br><span class="line">        <span class="comment"># 损失计算</span></span><br><span class="line">        loss = loss_fn(output, targets)</span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_train_step += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            end_time = time.time()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;运行时间：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(end_time-start_time))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;训练次数：&#123;&#125;， loss:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_train_step, loss.item()))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试步骤</span></span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataLoader:</span><br><span class="line">            imgs, targets = data</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 测试数据使用 GPU 计算</span></span><br><span class="line">            <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">                imgs = imgs.cuda()</span><br><span class="line">                targets = targets.cuda()</span><br><span class="line"></span><br><span class="line">            output = myNet(imgs)</span><br><span class="line">            test_loss = loss_fn(output, targets)</span><br><span class="line">            total_test_loss += test_loss.item()</span><br><span class="line">            total_accuracy += (output.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;整体 test loos:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;整体 accuracy:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;test_loss&#x27;</span>, total_test_loss, total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;test_accuracy&#x27;</span>, total_accuracy, total_test_step)</span><br><span class="line">    total_test_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存模型</span></span><br><span class="line">    torch.save(myNet, <span class="string">&#x27;mynet_&#123;&#125;.pth&#x27;</span>.<span class="built_in">format</span>=(i))</span><br><span class="line">    <span class="comment"># 方式2：torch.save(myNet.state_dict(), &#x27;mynet_&#123;&#125;.pth&#x27;.format(i))</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;模型已保存&#x27;</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p>方式2：</p>
<p>在程序的一开始用 <code>torch.device()</code> 方法定义设备，随后在需要 GPU 加速的地方调用 <code>xx.to(device)</code> 即可实现使用 GPU 加速。</p>
<p>定义设备处可以使用语法糖方式编写，比如：</p>
<p><code>device = torch.device('cuda') if torch.cuda.is_available() else 'cpu' </code></p>
<ul>
<li>
<p>完整代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> Model <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练的设备</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line"></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                          download=<span class="literal">True</span>)</span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                         download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line"><span class="comment"># 测试集长度</span></span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dataLoader 加载数据集</span></span><br><span class="line">train_dataLoader = DataLoader(dataset=train_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataLoader = DataLoader(dataset=test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建网络模型</span></span><br><span class="line">myNet = MyNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络模型使用 GPU 计算</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    myNet = myNet.to(device)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;cuda 无法使用&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 损失函数使用 GPU 计算</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    loss_fn = loss_fn.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = torch.optim.SGD(myNet.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 writer</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./test_logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-----第 &#123;&#125; 次训练开始-----&quot;</span>.<span class="built_in">format</span>(i + <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataLoader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练数据使用 GPU 计算</span></span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            imgs = imgs.to(device)</span><br><span class="line">            targets = targets.to(device)</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        output = myNet(imgs)</span><br><span class="line">        <span class="comment"># 损失计算</span></span><br><span class="line">        loss = loss_fn(output, targets)</span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_train_step += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            end_time = time.time()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;运行时间：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(end_time-start_time))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;训练次数：&#123;&#125;， loss:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_train_step, loss.item()))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试步骤</span></span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataLoader:</span><br><span class="line">            imgs, targets = data</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 测试数据使用 GPU 计算</span></span><br><span class="line">            <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">                imgs = imgs.to(device)</span><br><span class="line">                targets = targets.to(device)</span><br><span class="line"></span><br><span class="line">            output = myNet(imgs)</span><br><span class="line">            test_loss = loss_fn(output, targets)</span><br><span class="line">            total_test_loss += test_loss.item()</span><br><span class="line"></span><br><span class="line">            total_accuracy += (output.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;整体 test loos:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;整体 accuracy:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;test_loss&#x27;</span>, total_test_loss, total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;test_accuracy&#x27;</span>, total_accuracy, total_test_step)</span><br><span class="line">    total_test_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存模型</span></span><br><span class="line">    torch.save(myNet, <span class="string">&#x27;mynet_&#123;&#125;.pth&#x27;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    <span class="comment"># 方式2：torch.save(myNet.state_dict(), &#x27;mynet_&#123;&#125;.pth&#x27;.format(i))</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;模型已保存&#x27;</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h2 id="八、模型验证">八、模型验证</h2>
<p>模型训练并保存完成后，接下来就需要进行进一步的验证了，我们可以载入保存好的模型，并在网络上查找图片，让模型对图片进行分类，具体代码如下。</p>
<ul>
<li>
<p>图片 <code>img.png</code></p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/07/141828-90162.png" alt=""></p>
</li>
<li>
<p>代码编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> Model <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor,</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">imgPath = <span class="string">&#x27;./imgs/img.png&#x27;</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(imgPath)</span><br><span class="line"></span><br><span class="line">transform = torchvision.transforms.Compose([torchvision.transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">                                            torchvision.transforms.ToTensor()])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入模型, 从 GPU 映射到 CPU 上</span></span><br><span class="line">model = torch.load(<span class="string">&#x27;mynet_19.pth&#x27;</span>, map_location=torch.device(<span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图片处理</span></span><br><span class="line">img = transform(image)</span><br><span class="line"><span class="comment"># 输入图片是需要一个 batch_size 的，所以需要 reshape 一下</span></span><br><span class="line">img = torch.reshape(img, (<span class="number">1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># 这一步可以提高性能</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output = model(img)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;预测类别：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(dataset.classes[output.argmax(<span class="number">1</span>)]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行结果：  预测类别：dog</span></span><br></pre></td></tr></table></figure></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Kuhne</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://kuhne.gitee.io/kuhne.gitee.io/2022/07/07/PyTorch-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">https://kuhne.gitee.io/kuhne.gitee.io/2022/07/07/PyTorch-学习笔记/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kuhne.gitee.io/kuhne.gitee.io" target="_blank">Kuhne</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/PyTorch/">PyTorch</a><a class="post-meta__tags" href="/tags/%E6%A1%86%E6%9E%B6/">框架</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/07/10/cs231n-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%889%EF%BC%89%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><img class="prev-cover" src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">cs231n 学习笔记（9）循环神经网络</div></div></a></div><div class="next-post pull-right"><a href="/2022/07/03/cs231n-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%888%EF%BC%89%E7%BB%8F%E5%85%B8%E7%9A%84-CNN-%E6%9E%B6%E6%9E%84/"><img class="next-cover" src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">cs231n 学习笔记（8）经典的 CNN 架构</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/05/03/Matplotlib-%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/" title="Matplotlib 基础教程"><img class="cover" src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/1cc2cc32c428a61ce3eb2f191668eb9c.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-03</div><div class="title">Matplotlib 基础教程</div></div></a></div><div><a href="/2022/05/02/Pandas%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/" title="Pandas基础教程"><img class="cover" src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/Pandas_Logo.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-02</div><div class="title">Pandas基础教程</div></div></a></div><div><a href="/2022/05/02/Numpy%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/" title="Numpy基础教程"><img class="cover" src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/Numpy_Logo2.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-02</div><div class="title">Numpy基础教程</div></div></a></div><div><a href="/2022/04/11/Python%E5%9F%BA%E7%A1%80/" title="Python基础"><img class="cover" src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/c59ba07eefc154fd81aac8f18b4b5513.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-11</div><div class="title">Python基础</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "/img/lazyload.gif" data-lazy-src="/img/head.jpg" onerror="this.onerror=null;this.src='/img/head.jpg'" alt="avatar"/></div><div class="author-info__name">Kuhne</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">74</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">44</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://gitee.com/kuhne/kuhne.gitee.io"><i class="fab fa-github"></i><span>来瞅瞅？</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="http://wpa.qq.com/msgrd?v=3&amp;uin=1064675347&amp;site=qq&amp;menu=yes" target="_blank" title="腾讯QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1064675347@qq.com" target="_blank" title="微信"><i class="fab fa-weixin"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">PyTorch 学习笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81PyTorch-%E4%BB%8B%E7%BB%8D"><span class="toc-text">一、PyTorch 介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Why-PyTorch"><span class="toc-text">1.1 Why PyTorch ?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Numpy-or-Torch"><span class="toc-text">1.2 Numpy or Torch?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81PyTorch-%E5%9F%BA%E7%A1%80"><span class="toc-text">二、PyTorch 基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-text">2.1 加载数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-TensorBoard-%E4%BD%BF%E7%94%A8"><span class="toc-text">2.2 TensorBoard 使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-TensorBoard-add-image-%E7%94%A8%E6%B3%95"><span class="toc-text">2.3 TensorBoard add_image() 用法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-transform-%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-text">2.3 transform 的使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E5%B8%B8%E8%A7%81%E7%9A%84-Transform"><span class="toc-text">2.4 常见的 Transform</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-torchvision-%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BD%BF%E7%94%A8"><span class="toc-text">2.4 torchvision 数据集使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-DataLoader-%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-text">2.5 DataLoader 的使用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-text">三、神经网络结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-PyTorch-%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D"><span class="toc-text">3.1 PyTorch 搭建神经网络介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">3.2 搭建自己的神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-PyTorch-%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C"><span class="toc-text">3.3 PyTorch 卷积操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-text">3.4 卷积层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-text">3.5 池化层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">3.6 非线性激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-7-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-text">3.7 批量归一化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-8-%E7%BA%BF%E6%80%A7%E5%B1%82"><span class="toc-text">3.8 线性层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-9-Dropout-%E5%B1%82"><span class="toc-text">3.9 Dropout 层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-10-Sequential"><span class="toc-text">3.10 Sequential</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E5%8D%95%E6%90%AD%E5%BB%BA"><span class="toc-text">四、神经网络简单搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-CIFAR-10-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-text">4.1 CIFAR-10 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA"><span class="toc-text">4.2 网络搭建</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%8F%8A%E4%BC%98%E5%8C%96"><span class="toc-text">五、损失函数、反向传播及优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">5.1 损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">5.2 反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">5.3 优化器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8"><span class="toc-text">六、模型使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E7%8E%B0%E6%9C%89%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-text">6.1 现有模型使用方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96"><span class="toc-text">6.2 模型的保存与读取</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81-%E5%AE%8C%E6%95%B4%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-text">七、 完整的模型训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%AE%8C%E6%95%B4%E6%AD%A5%E9%AA%A4"><span class="toc-text">7.1 训练模型完整步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-%E4%BD%BF%E7%94%A8-GPU-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-text">7.2 使用 GPU 训练模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81"><span class="toc-text">八、模型验证</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/02/27/Git-%E5%8F%8A-GitHub-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95/" title="Git 及 GitHub 常用命令记录"><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2023/02/27/20230227141226-160f64.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Git 及 GitHub 常用命令记录"/></a><div class="content"><a class="title" href="/2023/02/27/Git-%E5%8F%8A-GitHub-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95/" title="Git 及 GitHub 常用命令记录">Git 及 GitHub 常用命令记录</a><time datetime="2023-02-27T06:08:31.000Z" title="发表于 2023-02-27 14:08:31">2023-02-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/02/26/%E4%B8%AA%E4%BA%BA%E5%B8%B8%E7%94%A8-Linux-%E5%91%BD%E4%BB%A4/" title="个人常用 Linux 命令"><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2023/02/26/20230226105141-94929b.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="个人常用 Linux 命令"/></a><div class="content"><a class="title" href="/2023/02/26/%E4%B8%AA%E4%BA%BA%E5%B8%B8%E7%94%A8-Linux-%E5%91%BD%E4%BB%A4/" title="个人常用 Linux 命令">个人常用 Linux 命令</a><time datetime="2023-02-26T02:47:48.000Z" title="发表于 2023-02-26 10:47:48">2023-02-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/14/Linux-%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" title="Linux 服务器环境配置"><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/4c417afa91736428def17dcc96aa64b8.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Linux 服务器环境配置"/></a><div class="content"><a class="title" href="/2022/09/14/Linux-%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" title="Linux 服务器环境配置">Linux 服务器环境配置</a><time datetime="2022-09-14T08:01:29.000Z" title="发表于 2022-09-14 16:01:29">2022-09-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/03/%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89Measuring-Compositional-Consistency-for-Video-Question-Answering/" title="论文整理（3）Measuring Compositional Consistency for Video Question Answering"><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/top2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文整理（3）Measuring Compositional Consistency for Video Question Answering"/></a><div class="content"><a class="title" href="/2022/09/03/%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89Measuring-Compositional-Consistency-for-Video-Question-Answering/" title="论文整理（3）Measuring Compositional Consistency for Video Question Answering">论文整理（3）Measuring Compositional Consistency for Video Question Answering</a><time datetime="2022-09-03T07:28:11.000Z" title="发表于 2022-09-03 15:28:11">2022-09-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/24/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/" title="论文学习笔记整理"><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/dknjyLgQcEXhsuK.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文学习笔记整理"/></a><div class="content"><a class="title" href="/2022/08/24/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/" title="论文学习笔记整理">论文学习笔记整理</a><time datetime="2022-08-24T05:50:52.000Z" title="发表于 2022-08-24 13:50:52">2022-08-24</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Kuhne</div><div class="footer_custom_text">欢迎来到<a href="https://kuhne.gitee.io/kuhne.gitee.io/">我的博客!</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn1.tianli0.top/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.staticfile.org/instant.page/5.1.0/instantpage.min.js" type="module"></script><script src="https://cdn.staticfile.org/vanilla-lazyload/17.3.1/lazyload.iife.min.js"></script><script src="https://cdn.staticfile.org/node-snackbar/0.1.9/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.staticfile.org/KaTeX/0.15.3/katex.min.css"><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/KaTeX/0.15.2/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/KaTeX/0.15.2/contrib/copy-tex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'S47HOBqpgUCutu7Azmdww7o3-gzGzoHsz',
      appKey: 'CGS1ktSYn9hr3dpj8vWHdHFz',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.staticfile.org/valine/1.4.18/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script src="https://jsdelivr.pai233.top/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script src="https://cdn1.tianli0.top/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>
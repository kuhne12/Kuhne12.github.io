<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>cs231n 学习笔记（11）可视化与理解 | Kuhne</title><meta name="keywords" content="计算机视觉,深度学习"><meta name="author" content="Kuhne"><meta name="copyright" content="Kuhne"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="可视化卷积神经网络 &#x2F; 梯度上升 &#x2F; 风格迁移">
<meta property="og:type" content="article">
<meta property="og:title" content="cs231n 学习笔记（11）可视化与理解">
<meta property="og:url" content="https://kuhne.gitee.io/kuhne.gitee.io/2022/07/15/cs231n-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B8%8E%E7%90%86%E8%A7%A3/index.html">
<meta property="og:site_name" content="Kuhne">
<meta property="og:description" content="可视化卷积神经网络 &#x2F; 梯度上升 &#x2F; 风格迁移">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg">
<meta property="article:published_time" content="2022-07-15T10:06:28.000Z">
<meta property="article:modified_time" content="2022-07-15T10:12:30.000Z">
<meta property="article:author" content="Kuhne">
<meta property="article:tag" content="计算机视觉">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg"><link rel="shortcut icon" href="/img/head2.png"><link rel="canonical" href="https://kuhne.gitee.io/kuhne.gitee.io/2022/07/15/cs231n-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B8%8E%E7%90%86%E8%A7%A3/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/6.0.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.staticfile.org/node-snackbar/0.1.9/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn1.tianli0.top/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://jsdelivr.pai233.top/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://jsdelivr.pai233.top/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'cs231n 学习笔记（11）可视化与理解',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-07-15 18:12:30'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/resource/JetBrainsMono-Medium.woff2"><link rel="stylesheet" href="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/resource/iconfont.min.css"><link rel="stylesheet" href="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/resource/index.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "/img/lazyload.gif" data-lazy-src="/img/head.jpg" onerror="onerror=null;src='/img/head.jpg'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">74</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">44</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-archive"></i><span> Blog</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-list-ul"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/pictures/"><i class="fa-fw fa fa-camera"></i><span> 图片</span></a></li><li><a class="site-page child" href="/talk/"><i class="fa-fw fas fa-comment-dots"></i><span> 说说</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Kuhne</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-archive"></i><span> Blog</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-list-ul"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/pictures/"><i class="fa-fw fa fa-camera"></i><span> 图片</span></a></li><li><a class="site-page child" href="/talk/"><i class="fa-fw fas fa-comment-dots"></i><span> 说说</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">cs231n 学习笔记（11）可视化与理解</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-07-15T10:06:28.000Z" title="发表于 2022-07-15 18:06:28">2022-07-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-07-15T10:12:30.000Z" title="更新于 2022-07-15 18:12:30">2022-07-15</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="cs231n 学习笔记（11）可视化与理解"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>cs231n 学习笔记（11）可视化与理解</h1>
<p><em>2022 / 7 / 12</em></p>
<blockquote>
<p>本节将讨论如何可视化理解卷积神经网络</p>
</blockquote>
<h2 id="一、问题描述">一、问题描述</h2>
<p>现在有这样一个问题：CNN 的内部真正的原理是什么？</p>
<p>我们已经知道了如何训练卷积神经网络，以及如何构建不同类型架构的网络来解决不同的问题，但现在还有一个疑问，这些<strong>神经网络的内部到底是怎么运行的</strong>？<strong>它们是如何让完成各自特定的工作的</strong>？<strong>它们寻找的特征是什么</strong>？</p>
<p>到目前为止，我们一直将 CNN 看作一个<strong>黑箱</strong>，包含原始像素的输入<strong>图像</strong>，从黑箱的一端进入，经过了很多 CNN 层，且进行了许多不同的转换，在 CNN 的输出端会得到一些<strong>类的分值</strong>，或是别的可以被理解的输出（比如边框位置、标记像素等），但问题是，中间层的作用是什么？</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/12/141015-042a0.png" alt=""></p>
<h2 id="二、理解卷积神经网络">二、理解卷积神经网络</h2>
<h3 id="2-1-CNN-第一层">2.1 CNN 第一层</h3>
<p>相对来说 CNN 的第一层会比较简单，一般来说第一层卷积层由卷积核组成，例如在 AlexNet 中，第一个卷积层由许多卷积核（64 个）组成，每个卷积核的尺寸是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>11</mn><mo>×</mo><mn>11</mn></mrow><annotation encoding="application/x-tex">3\times11\times11</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">11</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">11</span></span></span></span> ，这些卷积核会在输入图像上不断滑动，获取图像块与卷积核的数量积，得到第一层的输出结果。</p>
<p>我们可以简单地<strong>将学习到的卷积核可视化，来得知卷积核正在“寻找”什么</strong>：把 AlexNet 中的每个卷积核看作是一个 3 通道 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>11</mn><mo>×</mo><mn>11</mn></mrow><annotation encoding="application/x-tex">11\times11</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">11</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">11</span></span></span></span> 的图像，将 64 个卷积核进行可视化，如下图所示：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/12/144342-5cf4d.png" alt=""></p>
<p>可以看到，这些层中的卷积核正在寻找<strong>有向边</strong>（比如明暗线条） ，从不同的角度和位置来观察输入图像，也可以看到一个卷积核中存在完全不同的颜色（比如绿色和粉色、橙色和蓝色），相当于第一个卷积层在尝试去做一些<strong>类似于人类视觉系统早期阶段的工作</strong>。有趣的是，无论链接的体系结构或者训练数据类型是什么，在第一层卷积层总是能得到类似的结果。</p>
<h3 id="2-2-CNN-中间层">2.2 CNN 中间层</h3>
<blockquote>
<p>这部分不明白的地方比较多。。。。。。。。。。。。。。。。。。。。。写的几乎是逐字稿了。。。。</p>
</blockquote>
<ul>
<li>
<p><strong>可视化权重</strong></p>
<p>如果对中间的卷积层做相同的可视化，会发现对应图像的<strong>可解释性会差很多</strong>， 第二层的权重会经过 <code>ReLU</code> 激活函数或其他的非线性激活函数，接收了 16 个通道的输入，并且使用 20 个卷积核进行 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>7</mn><mo>×</mo><mn>7</mn></mrow><annotation encoding="application/x-tex">7\times7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">7</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">7</span></span></span></span> 的卷积，我们不能直接将这些权重进行可视化，因为输入图像的深度是 16 维的，且每个卷积核的尺寸为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn><mo>×</mo><mn>7</mn><mo>×</mo><mn>7</mn></mrow><annotation encoding="application/x-tex">16\times7\times7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">16</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">7</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">7</span></span></span></span> ，共有 20 个卷积核。这里的做法是：对于单一的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn><mo>×</mo><mn>7</mn><mo>×</mo><mn>7</mn></mrow><annotation encoding="application/x-tex">16\times7\times7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">16</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">7</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">7</span></span></span></span> 的卷积核，画出 1 6个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>7</mn><mo>×</mo><mn>7</mn></mrow><annotation encoding="application/x-tex">7\times7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">7</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">7</span></span></span></span>的灰度图，一共画20组，下图的灰度图像显示了第二层某个卷积核的权重，因为有 20 个输出，所以第二层卷积层有 20 个形状为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn><mo>×</mo><mn>7</mn><mo>×</mo><mn>7</mn></mrow><annotation encoding="application/x-tex">16\times7\times7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">16</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">7</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">7</span></span></span></span> 的卷积核。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/12/150413-757ba.png" alt=""></p>
<p>将卷积核权重进行可视化后，可以发现他们具有一些空间结构，但并不能直接观察到很有价值的信息，因为这些卷积核并<strong>没有直接连接到输入图像</strong>，所以我们需要想出一些更奇特的方法，用来理解中间卷积层究竟在做什么。</p>
</li>
<li>
<p><strong>可视化激活图（feature map）</strong></p>
<p>中间层<strong>可视化权重</strong>的解释性并不是很强，而可视化中间层的 <strong>feature map</strong> 在某些情况下更具可解释性。再次以 AlexNet 为例，AlexNet 的第 5 个卷积层提供了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>128</mn><mo>×</mo><mn>13</mn><mo>×</mo><mn>13</mn></mrow><annotation encoding="application/x-tex">128\times13\times13</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">128</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">13</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">13</span></span></span></span> 的张量，我们可以将其转化为 128 个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>13</mn><mo>×</mo><mn>13</mn></mrow><annotation encoding="application/x-tex">13\times13</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">13</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">13</span></span></span></span>  的灰度图像，这能让我们意识到 CNN 需要寻找的特征在输入中是什么样的，下面是可视化的一个具体例子：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/12/204010-1900b.png" alt=""></p>
<p>可以看到中间高亮的图像，似乎在匹配输入的图像，寻找类似于人脸的东西，但其他的大部分图像都是噪声。</p>
</li>
<li>
<h5 id="最大激活块（Maximally-Activating-Patches）">最大激活块（Maximally Activating Patches）</h5>
<p>对于可视化中间层可以做的另一件事情是：可视化输入图像中不同类型的图像块，可以最大程度地激活不同的特征、不同的神经元。比如选取 AlexNet 的第 5 个卷积层的第 17 个激活图（共 128 个），随后输入很多图像通过卷积神经网络，分别记录 Conv5 第 17 个激活图的值，这个特征图上部分值会被输入图片集<strong>最大激活</strong>，由于每个神经元的感受野有限，我们可以画出<strong>这些被最大激活的神经元对应在原始输入图片的小块</strong>，通过这些小块观察不同的神经元在寻找哪些信息。下面是一些来自特定神经元输入图像块的例子：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/12/210305-cbafd.png" alt=""></p>
<p>其中，每一行都是某个神经元被最大激活对应的图片块，可以看到有的神经元在寻找类似眼睛的东西，有的在寻找弯曲的曲线等。如果不使用Conv5的激活图，而是更后面的卷积层，由于<strong>卷积核视野扩大</strong>，寻找的特征也会更加复杂，比如人脸、相机等，对应下图的下面部分。</p>
<blockquote>
<p>一个单层的卷积核对应一行，多个就形成了上图，意味着这层卷积核关注的是原图的什么（可能是眼睛、脸）</p>
</blockquote>
</li>
</ul>
<h3 id="2-3-CNN-最后层">2.3 CNN 最后层</h3>
<p>现在来讨论模型的最后一层，CNN 的最后一层可能会得到 1000 个类的得分情况，来告知数据集中每个类的得分，在最后一层之前一般会存在全连接层，AlexNet 用 4096 维的特征向量来表示图像，然后将其输入到最后一层，来预测最终类的得分。</p>
<p>另一种用于可视化和理解 CNN 的方法是<strong>尝试理解神经网络的最后一层</strong>，可以通过 CNN 来检测一些图像，并为每一幅图像<strong>标记对应的 4096 维向量</strong>，试着可视化最后一层隐层。</p>
<ul>
<li>
<p>最近邻</p>
<p>我们可能会看到尝试使用<strong>最近邻法</strong>，最近邻法可以在像素空间中寻找 “近邻”，比如之前在 <code>CIFAR-10</code> 数据集上使用 NN 法，会发现找的到图像和查询的图像看起来非常相似，但不一定是一类事物。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/12/153807-04d12.png" alt=""></p>
<p><strong>与其根据像素空间来计算近邻，不如在由卷积得到的 4096 维的特征空间中计算近邻</strong>，如下图所示，有些图像在像素空间中并不类似（比如大象的第一张和第四张），但由于是在由 CNN 计算得到的 4096 维特征空间上进行 NN 计算，最终会认为两张图象直接非常相似。</p>
<blockquote>
<p>大概意思就是，比起在原始像素上使用 NN，不如在卷积得到的向量上使用 NN，说明了 4096 维的特征空间能捕捉到图像中的语义内容。</p>
</blockquote>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/12/153349-b0d2f.png" alt=""></p>
</li>
<li>
<p><strong>降维</strong></p>
<p>另一种用于观察 CNN 最后一层的方法是<strong>降维</strong>，<strong>通过 PCA 将 4096 维的特征向量压缩到二维空间</strong>，以便更加直观的可视化特征空间。除了 PCA ，还有一种更强大的降维算法 —— <code>t-SNE</code> ，也就是 <code>t-distributed Stochastic Neighbor Embeddings</code> 的缩写，它是以一种在深度学习领域常被使用的，<strong>用于可视化特征的非线性降维方法</strong>。下图是 <code>t-SNE</code> 的一个应用实例，展示了在 <code>mnist</code> 数据集上的降维。</p>
<blockquote>
<p><code>mnist</code> 也就是手写数字数据集（0 - 9）</p>
</blockquote>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/12/155155-369b5.png" alt=""></p>
<p>可以看到降维后的数据集可以更清晰的展现出集群的特点，这些集群对应了 <code>mnist</code> 数据集中的数据。</p>
<p>所以我们现在就可以做类似的可视化工作，也就是在图像分类器网络的最后一层上应用 <code>t-SNE</code> 降维。具体来说就是提取大量图像，并让他们在 CNN 上运行，记录每个图像在网络最后一层产生的 4096 维特征向量（数量很大），通过 <code>t-SNE</code> 把 4096 维的特征空间压缩至 2 维，随后在 2 维的特征空间中布局网格，并观察 2 维特征空间网格每个位置会出现什么类型的图像。通过这种做法可以粗略地感受到学习到的特征空间的几何结构。</p>
<ul>
<li>
<p>粗略版本</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/12/160330-cc87d.png" alt=""></p>
<p>通过粗略版本可以观察到，左下角都是一些绿色的东西，而右上角偏蓝。</p>
</li>
<li>
<p>高清版本</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/12/160244-37ef8.jpeg" alt=""></p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_4k.jpg" alt="img"></p>
</li>
</ul>
</li>
</ul>
<h2 id="三、-理解像素">三、 理解像素</h2>
<h3 id="3-1-遮挡实验">3.1 遮挡实验</h3>
<p>遮挡实验是为了弄清楚<strong>究竟是输入图像的哪部分导致神经网络做出了最终的分类决策</strong>。以包含了一头大象的图片为例，实验的具体方法是遮挡住图像的某个部分（替换为数据集的平均像素值），然后将图像输入网络，记录遮挡图像的预测概率。随后将此遮挡块滑过所有位置，重复所有步骤，最后绘制图像的热力图，如下图所示。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/12/214937-8a968.png" alt=""></p>
<p>热力图显示了在该位置设定遮挡块时，对应的预测概率为多少。如果遮挡了图像的某部分，导致神经网络分值急剧变化，那么该遮挡区域可能对模型分类决策起到非常重要的作用。</p>
<blockquote>
<p>图中颜色越深的地方代表预测概率越低，以大象为例，如果遮住大象的脸部，预测概率会急剧下降，说明大象的脸部像素对神经网络做出决策有很重要的作用。</p>
</blockquote>
<h3 id="3-2-显著图">3.2 显著图</h3>
<p>给出一只狗的输入图像，以及狗的预测类标签，我们想知道输入图像中的哪部分像素对于分类是最重要的，除了遮挡实验，显著图是从另一个角度解决这个问题的方法，具体做法是：计算分类得分相对于图像像素的梯度，这将告诉我们，在一阶近似意义上对于输入图片的每个像素<strong>进行小小的扰动</strong>，那么相应分类的分值会有多大的变化。当通过显著图的方法运行小狗的图像，能很方便地看出图中小狗的轮廓，这表示网络正在寻找的特征可能在这些像素之间。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/140108-1b3be.png" alt=""></p>
<p>如果对不同的图像做相同地操作，就能发现神经网络在寻找合适的区域。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/140159-0385b.png" alt=""></p>
<blockquote>
<p>进行语义分割的时候也可以运用显著图的方法，可以<strong>在没有任何标签的情况下</strong>可以运用显著图进行语义分割。</p>
</blockquote>
<h3 id="3-3引导式反向传播-（guided-backprop）">3.3引导式反向传播 （guided backprop）</h3>
<p>显著图是使用<strong>分类得分</strong>对图片上的<strong>像素</strong>求导得出对分类决策影响较大的像素，而引导式反向传播是使用 CNN <strong>某一层的特定神经元的值</strong>对像素求导，这样就能观察<strong>图像的像素</strong>对<strong>特定神经元</strong>的影响。但是这里的反向传播是<strong>引导式的</strong>，即 <code>ReLU</code> 函数反向传播时，只回传大于0的梯度，如下图所示。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/141238-cd1b0.png" alt=""></p>
<blockquote>
<p>虽然这样做很奇怪，但是效果很好，图像很清晰。</p>
</blockquote>
<p>当使用引导式反向传播，而不是用定期反向传播时，更容易获得更清晰的图像，来告知图像中的哪部分像素影响了特定的神经元。</p>
<p>我们把<strong>引导式反向传播计算的梯度</strong>可视化和<strong>最大激活块</strong>进行对比，发现这两者的表现很相似。下图左边是最大激活块，每一行代表一个神经元，右侧是该神经元计算得到的对原始像素的引导式反向传播梯度。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/141905-d573a.png" alt="image-20220713141905105"></p>
<p>以第一行为例，最大激活该神经元的图像块都是一些<strong>圆形的区域</strong>，这表明该神经元可能在寻找圆形状物体，然后观察右边，发现<strong>的确是圆形区域的像素</strong>会影响的神经元的值。</p>
<h3 id="3-4-梯度上升（-Gradient-Ascent）">3.4 梯度上升（ Gradient Ascent）</h3>
<p>从以上讲述的几个方法中，我们了解到如何查看<strong>对神经元影响最大的像素</strong>，如果我们在一些输入图像中移除这种依赖性，那什么类型的输入才会激活该神经元？我们可以通过<strong>梯度上升</strong>算法来解决这个问题。</p>
<p>我们总在训练神经网络时使用梯度下降来使 loss 最小化，而现在我们想要在图像上执行梯度上升，来合成图像，以尝试<strong>最大化</strong>某些<strong>中间神经元</strong>和<strong>类别</strong>的分值，来改变像素值。在执行梯度上升的过程中，我们不再优化权重，而是让权重保持不变，尝试改变一些图像的像素，使该<strong>神经元的值</strong>或对应<strong>类的分值最大化</strong>。除此之外，我们还需要用到正则项，以阻止生成的图像过拟合，而生成很奇怪的东西。</p>
<p>生成图像具备两个属性：</p>
<ol>
<li>使最大程度地激活分类得分或神经元的值；</li>
<li>使我们希望这个生成的图像看起来是自然的（正则项强制生成的图像看起来是自然的图像）。</li>
</ol>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/144447-ab485.png" alt=""></p>
<p>具体方法是：把初始图象初始化为 0 或者添加高斯噪声，然后通过神经网络<strong>重复传入图像</strong>并计算 1. 神经元分值，2. 反向传播计算神经元分值对图像像素的梯度，3. 更新图像以使神经元分值最大化。不断重复上述步骤直到图像可以理解为止。</p>
<p>上面讨论到为<strong>图像进行正则化</strong>，一个比较常见的正则化方法是通过 <code>L2</code> 正则化来约束像素，针对分类得分生成的图片如下所示：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/145834-28024.png" alt=""></p>
<p>用肉眼很难看清楚，但是能看出个大概。</p>
<p>除了 <code>L2</code> 正则化外，还能使用一些方法来优化正则，比如对生成的图像进行<strong>高斯模糊处理</strong>、将<strong>像素值特别小或梯度值特别小的值去掉</strong>。这些方法会使生成的图像更清晰。也可以针对某个神经元进行梯度上升。层数越高，生成的结构越复杂，如下图所示。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/150704-0723d.png" alt=""></p>
<blockquote>
<p>添加多模态（multi-faceted）可视化可以提供更好的结果(加上更仔细的正则化，中心偏差）。通过优化 FC6 的特征而不是原始像素，会得到更加自然的图像。</p>
</blockquote>
<p>通过图像像素的梯度来合成图像的想法其实非常强大，其中一个有趣的想法就是<strong>愚弄网络</strong>，即选取一些任意的图像，然后告诉网络我们希望最大化另一类事物的分值，然后修改原图的像素，最后网络会将该图像分类成零一类事物，而图像看上去却没多大变化</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/152036-e84e1.png" alt=""></p>
<h2 id="四、DeepDream">四、DeepDream</h2>
<p>我们可以做的另一件<strong>基于梯度的图像优化</strong>是 <code>DeepDream</code>，这谷歌发布的一篇博客文章的内容。仍然利用<strong>梯度上升</strong>的原理，不再是通过最大化神经元激活来合成图片，而是<strong>直接放大某些层的神经元激活特征</strong>。</p>
<p>想法是输入图像，通过神经网络运行到某一层，接着进行反向传播，并<strong>设置该层的梯度等于激活值</strong>，然后反向传播到图像，并不断地更新图像。</p>
<p>上述步骤试图<strong>放大神经网络在图像中检测到的特征</strong>，无论那层存在什么特征，都将其梯度设为特征值，<strong>使神经网络放大它在图像中检测到的特征</strong>，这种方法同样可以用于最大化图像在该层中的 <code>L2</code> 范数。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/154804-14a33.png" alt=""></p>
<p>做完一切后，输入一张天空的图像，可以把网络中学到的特征在原图像上生成：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/155849-c513c.png" alt="image-20220713155848896"></p>
<p>现在特征已经被放置在了天空的图像上，通过这个过程，特征得到了放大，可以看到很多“突变物体”。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/160143-45334.png" alt=""></p>
<p>对神经网络不同层做同样的操作也会得到差不多的结果，下图选择的是神经网络一个较低层，似乎正在计算边缘或者漩涡之类的东西。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/160400-86378.png" alt=""></p>
<p>如果长时间运行 DeepDream ，并且进行多尺度处理，可能得到很 “诡异” 的图像：<br>
<img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/160454-ca19e.png" alt="image-20220713160454373"></p>
<p>下面是在 MIT Places Dataset 上运行 DeepDream 后的结果（MIT 数据集包含了 200 种不同的场景图像）：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/160659-7c59a.png" alt=""></p>
<h2 id="五、风格迁移">五、风格迁移</h2>
<h3 id="5-1-特征反演（Feature-Inversion）">5.1 特征反演（Feature Inversion）</h3>
<p>特征反演是为了查看<strong>不同层的特征向量能保留多少原始的图片信息</strong>。</p>
<p>具体过程是：向 CNN  种喂入一张图像，选取其在 CNN 某一层产生的特征向量，随后根据该特征向量重构图像，重构后的图像会提供一些<strong>神经网络捕获到的图像类型信息</strong>（可以通过梯度上升和正则化做到）。</p>
<blockquote>
<p>与其最大化某些分值，不如最小化特征向量之间的距离，并且在生成图像的特征之间尝试合成一个新的相匹配的图像。</p>
</blockquote>
<p>全变差正则化将左右相邻像素之间的差异拼凑成上下相邻，尝试增加生成图像中特殊的平滑度。</p>
<p>用这种方法，我们可以看到不同层的特征向量所包含的信息完整度：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/163127-62307.png" alt=""></p>
<p>最左侧是原始图像，右侧是根据 <code>VGG-16</code> 不同层记录的特征生成的图像，这能让我们了解到这张图像在神经网络不同层的信息存储量。</p>
<p>在relu2_2层，可以根据特征向量几乎无损地恢复出原图片；但是尝试从ReLU4_3 ReLU5_1重构图像时，可以看到图像的<strong>一般空间结构</strong>被保留了下来，仍可以分辨出大象，苹果和香蕉。但是许多低<strong>层次的细节</strong>并比如纹理、颜色在神经网路的较高层<strong>更容易损失</strong>。</p>
<h3 id="5-2-纹理生成">5.2 纹理生成</h3>
<p>纹理合成在计算机图形学里是一个老问题，比如给定一些纹理的输入图像，想要生成更大块的相同纹理的图像，如下图所示。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/163654-aef2e.png" alt=""></p>
<p>NN 算法在纹理生成问题的表现非常好，但是没有神经网络，这里采用的是非常简单的方法，即按照扫描线一次一个像素地遍历生成图像，然后<strong>根据已经生成的像素查看当前像素周围的邻域，并在输入图像地图像块中计算近邻，然后从输入图像中复制像素</strong>。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/164340-746a6.png" alt=""></p>
<p>如果使用的是复杂纹理，直接从输入图像的图像块复制像素的方法可能会行不通。 有一篇论文试图<strong>将神经网络的特征应用到纹理合成问题上</strong>，最终将这个问题转化为梯度上升的过程，类似于 feature map，其中各种各样的 feature 指的是物体上的特征。</p>
<ul>
<li>
<p><strong>格莱姆矩阵</strong>（Gram Matrix）</p>
<p>为了在神经网络上进行纹理合成，使用了格莱姆矩阵 。将输入图像传递给 CNN ，抽取他们在某层的卷积特征，假设卷积特征大小为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">C\times H\times W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span> ，可以将其看作是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>×</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">H\times W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>  个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span> 维的特征向量，用于描述图像在这点的外观，做矩阵乘法可以得到一个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>×</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">C\times C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span> 的矩阵。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/170350-30b25.png" alt=""></p>
<p>然后对激活图中任意两个C维向量的组合，都可以求出这样一个矩阵。把这些矩阵求和并平均，就是格莱姆矩阵。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/170423-2182e.png" alt=""></p>
<p>格莱姆矩阵告诉我们<strong>两个点代表的不同特征的同现关系</strong>，矩阵中位置索引为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mi>j</mi></mrow><annotation encoding="application/x-tex">ij</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">ij</span></span></span></span> 的元素值非常大，这意味着这两个输入向量的位置索引为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span> 的元素值非常大。这以某种方式捕获了一些二阶统计量，即映射特征图中的<strong>哪些特征倾向于在空间的不同位置一起激活</strong>。</p>
<blockquote>
<p>Q：为什么不使用协方差矩阵或类似的方法，而是使用格莱姆矩阵？</p>
<p>A：格莱姆矩阵其实是 feature 之间的<strong>偏心协方差矩阵</strong>（即没有减去均值的协方差矩阵）。其计算了每个通道特征之间的相关性，考察哪些特征是此消彼长的，哪些特征是同时出现的。</p>
<p>我们认为格莱姆矩阵度量了图片中的纹理特性，并且不包含图像的结构信息，因为我们对图像中的每一点所对应的特征向量取平均值，它只是捕获特征间的二阶同现统计量，这最终是一个很好的纹理描述符。</p>
<p>事实上，使用协方差矩阵代替格莱姆矩阵也能取得很好的效果，但是格莱姆矩阵有更高效的计算方法：将激活图张量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">C\times H\times W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>展开成 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>×</mo><mi>H</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">C\times HW</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>的形式，然后将其乘以其转置，即得到gram矩阵。</p>
</blockquote>
</li>
<li>
<h5 id="神经纹理生成（Neural-Texture-Synthesis）">神经纹理生成（Neural Texture Synthesis）</h5>
<p>当我们有了格莱姆矩阵这一度量图像纹理特性的工具后，就可以使用类似于梯度上升算法来产生特定纹理的图像。算法流程为：</p>
<ol>
<li>使用 ImageNet 数据集<strong>预训练</strong>一个 VGG 网络</li>
<li>把含有纹理的图像输入到VGG网络，进行前向传播，<strong>记录其每一层的激活图</strong></li>
<li>计算每一层的格<strong>莱姆矩阵</strong>，给出特征的外积**</li>
<li>在随机噪声中<strong>初始化</strong>生成图像</li>
<li>通过 CNN <strong>生成图像</strong>，计算每一层的格拉姆矩阵</li>
<li><strong>计算损失</strong>:  格莱姆矩阵之间 L2 距离的加权和</li>
<li>反向传播得到图片的梯度</li>
<li>根据<strong>梯度上升</strong>一点点更新图像的像素</li>
<li>重复步骤 5</li>
</ol>
<p>最终会生成与纹理图像相匹配的纹理图像。效果如下：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/174323-7e6f5.png" alt=""></p>
<p>上图说明，如果以<strong>更高层格莱姆矩阵</strong>的 <code>L2</code> 距离作为损失函数，那么生成图像就会更好地重建图像的纹理结构。这是由于<strong>更高层的神经元具有更大的感受野</strong>导致的。</p>
</li>
</ul>
<h3 id="5-3-风格迁移（Style-Transfer）">5.3 风格迁移（Style Transfer）</h3>
<p>结合<strong>特征反演</strong>和<strong>纹理生成</strong>，就可以实现所谓的<strong>风格迁移</strong>。</p>
<p>在风格迁移中，我们把两张图像作为输入图像，第一步将其中一张图像做为<strong>内容图像</strong>，他将引导生成<strong>图像的主体</strong>；将零一幅图像作为<strong>风格图像</strong>，负责生成图像的<strong>纹理或风格</strong>，然后<strong>共同做特征识别，最小化内容图像的特征重构损失，以及风格图像的格莱姆矩阵损失</strong>。完成以上步骤，就能获得非常不错的图像：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/175044-876f3.png" alt=""></p>
<p>基本架构如下：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/175440-cfb28.png" alt=""></p>
<p>在这个框架中，使用<strong>随机噪声初始化</strong>生成图像，同时优化<strong>特征反演</strong>和<strong>纹理生成</strong>的<strong>损失函数</strong>（生成图像与内容图像激活特征向量的 <code>L2</code> 距离以及与风格图像格莱姆矩阵的 <code>L2</code> 距离的加权和），计算图像上的<strong>像素梯度</strong>，重复这些步骤，在生成图像上执行<strong>梯度上升</strong>。经过一些迭代后会生成风格迁移后的图像。这样生成图像既具有内容图像的空间结构，又具有风格图像的纹理结构。</p>
<p>由于网络使用的<strong>总损失</strong>是<strong>特征反演</strong>和<strong>纹理生成</strong>的两部分损失的<strong>加权和</strong>，所以<strong>调整这个加权值</strong>可以得到不同风格的输出：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/175729-7311d.png" alt=""></p>
<p>也可以改变风格图像的尺寸：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/175809-20b58.png" alt="image-20220713175809098"></p>
<p>此外，也可以使用<strong>不同风格的格莱姆矩阵</strong>的加权和，来生成多风格图：</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/175840-a482e.png" alt=""></p>
<p>实现代码：<a target="_blank" rel="noopener" href="https://github.com/jcjohnson/neural-style">点击这里</a></p>
<h3 id="5-4-快速风格迁移（Fast-style-Transfer）">5.4 快速风格迁移（Fast style Transfer）</h3>
<p>但是风格迁移算法有一个问题：<strong>算法的效率特别低</strong>。为了生成新图像，需要在预训练网络中计算很多前向传播和反向传播，特别是为了生成高分辨率的图像，每个 4K 图像的前向传播和反向传播都需要大量的计算和内存。</p>
<p>解决方法是训练另一种神经网络，来进行风格迁移的工作。</p>
<p>有一篇研究提出了这么一个想法：在一开始就<strong>训练好想要迁移的风格</strong>，并不是为每个图象运行一个单独的优化程序，而是训练一个可以输入内容图像的前馈网络，直接输出风格迁移后的结果。</p>
<p><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2022/07/13/181137-a80cf.png" alt=""></p>
<p>训练前馈神将网络的方法是：在训练期间计算相同内容图像和风格图像的损失，然后使用<strong>相同梯度</strong>来更新前馈神经网络的权重，一旦训练完成，只需在训练好的网络上进行单一的前向传播。训练过程可能需要耗费几个小时的时间，但是一旦训练完成，生成风格迁移图像的速度会很快。</p>
<p>实现代码：<a target="_blank" rel="noopener" href="https://github.com/jcjohnson/fast-neural-style">点击这里</a></p>
<h2 id="总结">总结</h2>
<ol>
<li><strong>理解CNN：</strong>
<ul>
<li>激活值：在激活值的基础上理解这些神经元在<strong>寻找什么特征</strong>，方法有最邻近、降维、最大化图像块、遮挡；</li>
<li>梯度：使用<strong>梯度上升</strong>合成新图像来 理解特征的意义，比如显著图、类可视化、愚弄图像、特征反演。</li>
</ul>
</li>
<li>**风格迁移：**特征反演 + 纹理生成。</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Kuhne</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://kuhne.gitee.io/kuhne.gitee.io/2022/07/15/cs231n-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B8%8E%E7%90%86%E8%A7%A3/">https://kuhne.gitee.io/kuhne.gitee.io/2022/07/15/cs231n-学习笔记（11）可视化与理解/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kuhne.gitee.io/kuhne.gitee.io" target="_blank">Kuhne</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/07/24/cs231n-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%8812%EF%BC%89%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"><img class="prev-cover" src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">cs231n 学习笔记（12）生成模型</div></div></a></div><div class="next-post pull-right"><a href="/2022/07/15/cs231n-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%8810%EF%BC%89%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"><img class="next-cover" src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">cs231n 学习笔记（10）目标检测与图像分割</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/07/24/cs231n-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%8812%EF%BC%89%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/" title="cs231n 学习笔记（12）生成模型"><img class="cover" src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-24</div><div class="title">cs231n 学习笔记（12）生成模型</div></div></a></div><div><a href="/2022/07/15/cs231n-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%8810%EF%BC%89%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/" title="cs231n 学习笔记（10）目标检测与图像分割"><img class="cover" src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-15</div><div class="title">cs231n 学习笔记（10）目标检测与图像分割</div></div></a></div><div><a href="/2022/07/10/cs231n-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%889%EF%BC%89%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="cs231n 学习笔记（9）循环神经网络"><img class="cover" src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-10</div><div class="title">cs231n 学习笔记（9）循环神经网络</div></div></a></div><div><a href="/2022/07/03/cs231n-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%888%EF%BC%89%E7%BB%8F%E5%85%B8%E7%9A%84-CNN-%E6%9E%B6%E6%9E%84/" title="cs231n 学习笔记（8）经典的 CNN 架构"><img class="cover" src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-03</div><div class="title">cs231n 学习笔记（8）经典的 CNN 架构</div></div></a></div><div><a href="/2022/06/26/cs231n-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%887%EF%BC%89%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BD%AF%E4%BB%B6/" title="cs231n 学习笔记（7）深度学习软件"><img class="cover" src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-26</div><div class="title">cs231n 学习笔记（7）深度学习软件</div></div></a></div><div><a href="/2022/06/22/cs231n-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%886%EF%BC%89%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E4%BA%8C%EF%BC%89/" title="cs231n 学习笔记（6）训练神经网络（二）"><img class="cover" src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/2901e2f51d643b321c61e158805fe248.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-22</div><div class="title">cs231n 学习笔记（6）训练神经网络（二）</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "/img/lazyload.gif" data-lazy-src="/img/head.jpg" onerror="this.onerror=null;this.src='/img/head.jpg'" alt="avatar"/></div><div class="author-info__name">Kuhne</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">74</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">44</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://gitee.com/kuhne/kuhne.gitee.io"><i class="fab fa-github"></i><span>来瞅瞅？</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="http://wpa.qq.com/msgrd?v=3&amp;uin=1064675347&amp;site=qq&amp;menu=yes" target="_blank" title="腾讯QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:1064675347@qq.com" target="_blank" title="微信"><i class="fab fa-weixin"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客</div></div><div class="card-widget latestBB"><div class="item-headline"><i class="fas fa-bolt"></i><span>最新吐槽</span></div><div class="item-content"><div class="swiper-container swiper-container-aside">
  <div class="swiper-wrapper swiper-weapper-aside">成功将本博客部署到 xxxx</div>
</div>
<script>
  window.kkBBConfig = {
    limit: 9,
    blog:'/bb/',
    api_url:
      'https://636f-comment-5gj5t55m7efcd73d-1251136071.tcb.qcloud.la/json/bber.json'
  }
</script>
<script src="https://cdn.jsdelivr.net/npm/butterfly-bber-swiper/dist/index.min.js"></script>
</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">cs231n 学习笔记（11）可视化与理解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0"><span class="toc-text">一、问题描述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E7%90%86%E8%A7%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">二、理解卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-CNN-%E7%AC%AC%E4%B8%80%E5%B1%82"><span class="toc-text">2.1 CNN 第一层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-CNN-%E4%B8%AD%E9%97%B4%E5%B1%82"><span class="toc-text">2.2 CNN 中间层</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E6%BF%80%E6%B4%BB%E5%9D%97%EF%BC%88Maximally-Activating-Patches%EF%BC%89"><span class="toc-text">最大激活块（Maximally Activating Patches）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-CNN-%E6%9C%80%E5%90%8E%E5%B1%82"><span class="toc-text">2.3 CNN 最后层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81-%E7%90%86%E8%A7%A3%E5%83%8F%E7%B4%A0"><span class="toc-text">三、 理解像素</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E9%81%AE%E6%8C%A1%E5%AE%9E%E9%AA%8C"><span class="toc-text">3.1 遮挡实验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%98%BE%E8%91%97%E5%9B%BE"><span class="toc-text">3.2 显著图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3%E5%BC%95%E5%AF%BC%E5%BC%8F%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-%EF%BC%88guided-backprop%EF%BC%89"><span class="toc-text">3.3引导式反向传播 （guided backprop）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87%EF%BC%88-Gradient-Ascent%EF%BC%89"><span class="toc-text">3.4 梯度上升（ Gradient Ascent）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81DeepDream"><span class="toc-text">四、DeepDream</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB"><span class="toc-text">五、风格迁移</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E7%89%B9%E5%BE%81%E5%8F%8D%E6%BC%94%EF%BC%88Feature-Inversion%EF%BC%89"><span class="toc-text">5.1 特征反演（Feature Inversion）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E7%BA%B9%E7%90%86%E7%94%9F%E6%88%90"><span class="toc-text">5.2 纹理生成</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BA%B9%E7%90%86%E7%94%9F%E6%88%90%EF%BC%88Neural-Texture-Synthesis%EF%BC%89"><span class="toc-text">神经纹理生成（Neural Texture Synthesis）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%EF%BC%88Style-Transfer%EF%BC%89"><span class="toc-text">5.3 风格迁移（Style Transfer）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-%E5%BF%AB%E9%80%9F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%EF%BC%88Fast-style-Transfer%EF%BC%89"><span class="toc-text">5.4 快速风格迁移（Fast style Transfer）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/02/27/Git-%E5%8F%8A-GitHub-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95/" title="Git 及 GitHub 常用命令记录"><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2023/02/27/20230227141226-160f64.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Git 及 GitHub 常用命令记录"/></a><div class="content"><a class="title" href="/2023/02/27/Git-%E5%8F%8A-GitHub-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95/" title="Git 及 GitHub 常用命令记录">Git 及 GitHub 常用命令记录</a><time datetime="2023-02-27T06:08:31.000Z" title="发表于 2023-02-27 14:08:31">2023-02-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/02/26/%E4%B8%AA%E4%BA%BA%E5%B8%B8%E7%94%A8-Linux-%E5%91%BD%E4%BB%A4/" title="个人常用 Linux 命令"><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/article-img/2023/02/26/20230226105141-94929b.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="个人常用 Linux 命令"/></a><div class="content"><a class="title" href="/2023/02/26/%E4%B8%AA%E4%BA%BA%E5%B8%B8%E7%94%A8-Linux-%E5%91%BD%E4%BB%A4/" title="个人常用 Linux 命令">个人常用 Linux 命令</a><time datetime="2023-02-26T02:47:48.000Z" title="发表于 2023-02-26 10:47:48">2023-02-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/14/Linux-%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" title="Linux 服务器环境配置"><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/4c417afa91736428def17dcc96aa64b8.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Linux 服务器环境配置"/></a><div class="content"><a class="title" href="/2022/09/14/Linux-%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" title="Linux 服务器环境配置">Linux 服务器环境配置</a><time datetime="2022-09-14T08:01:29.000Z" title="发表于 2022-09-14 16:01:29">2022-09-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/03/%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89Measuring-Compositional-Consistency-for-Video-Question-Answering/" title="论文整理（3）Measuring Compositional Consistency for Video Question Answering"><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/top2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文整理（3）Measuring Compositional Consistency for Video Question Answering"/></a><div class="content"><a class="title" href="/2022/09/03/%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89Measuring-Compositional-Consistency-for-Video-Question-Answering/" title="论文整理（3）Measuring Compositional Consistency for Video Question Answering">论文整理（3）Measuring Compositional Consistency for Video Question Answering</a><time datetime="2022-09-03T07:28:11.000Z" title="发表于 2022-09-03 15:28:11">2022-09-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/24/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/" title="论文学习笔记整理"><img src= "/img/lazyload.gif" data-lazy-src="https://kuhne-blog-img.oss-cn-hangzhou.aliyuncs.com/blog/img/dknjyLgQcEXhsuK.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文学习笔记整理"/></a><div class="content"><a class="title" href="/2022/08/24/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/" title="论文学习笔记整理">论文学习笔记整理</a><time datetime="2022-08-24T05:50:52.000Z" title="发表于 2022-08-24 13:50:52">2022-08-24</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Kuhne</div><div class="footer_custom_text">欢迎来到<a href="https://kuhne.gitee.io/kuhne.gitee.io/">我的博客!</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn1.tianli0.top/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.staticfile.org/instant.page/5.1.0/instantpage.min.js" type="module"></script><script src="https://cdn.staticfile.org/vanilla-lazyload/17.3.1/lazyload.iife.min.js"></script><script src="https://cdn.staticfile.org/node-snackbar/0.1.9/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.staticfile.org/KaTeX/0.15.3/katex.min.css"><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/KaTeX/0.15.2/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/KaTeX/0.15.2/contrib/copy-tex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'S47HOBqpgUCutu7Azmdww7o3-gzGzoHsz',
      appKey: 'CGS1ktSYn9hr3dpj8vWHdHFz',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.staticfile.org/valine/1.4.18/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script src="https://jsdelivr.pai233.top/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script src="https://cdn1.tianli0.top/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>